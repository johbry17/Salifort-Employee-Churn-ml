{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n",
    "<link href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css\" rel=\"stylesheet\">\n",
    "<link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css\">\n",
    "<link rel=\"stylesheet\" href=\"../static/css/styles.css\">\n",
    "\n",
    "\n",
    "        \n",
    "<!-- <body> -->\n",
    "<!-- Navigation-->\n",
    "<nav class=\"navbar navbar-expand-lg navbar-light fixed-top\" id=\"mainNav\">\n",
    "    <div class=\"container px-4 px-lg-5\">\n",
    "        <a class=\"navbar-brand\" href=\"../index.html\">Home</a>\n",
    "        <button class=\"navbar-toggler\" type=\"button\" data-bs-toggle=\"collapse\" data-bs-target=\"#navbarResponsive\" aria-controls=\"navbarResponsive\" aria-expanded=\"false\" aria-label=\"Toggle navigation\">\n",
    "            Menu\n",
    "            <i class=\"fas fa-bars\"></i>\n",
    "        </button>\n",
    "        <div class=\"collapse navbar-collapse\" id=\"navbarResponsive\">\n",
    "            <ul class=\"navbar-nav ms-auto py-4 py-lg-0\">\n",
    "                <li class=\"nav-item\"><a class=\"nav-link px-lg-3 py-3 py-lg-4\" href=\"../index.html\">Executive Summary</a></li>\n",
    "                <li class=\"nav-item\"><a class=\"nav-link px-lg-3 py-3 py-lg-4\" href=\"eda.html\">Exploratory Data Analysis</a></li>\n",
    "                <!-- <li class=\"nav-item\"><a class=\"nav-link px-lg-3 py-3 py-lg-4\" href=\"models.html\">Model Construction & Validation</a></li> -->\n",
    "                <li class=\"nav-item\"><a class=\"nav-link px-lg-3 py-3 py-lg-4\" href=\"initial_work.html\">Reference: Model Development</a></li>\n",
    "            </ul>\n",
    "        </div>\n",
    "    </div>\n",
    "</nav>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ysS5rgTMWpwL"
   },
   "source": [
    "<h2 id=\"title\" style=\"text-align: center; width: 80%;\">Model Construction and Validation</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "- [Description and Deliverables](#description-and-deliverables)\n",
    "- [Exploratory Data Analysis Insights](#insights)\n",
    "- [Modeling Strategies Recap](#modeling-strategies-recap)\n",
    "  - [Cross-Validation Results](#cross-validation-results)\n",
    "- [Model Building](#model-building)\n",
    "  - [Logistic Regression](#logistic-regression)\n",
    "  - [Decision Tree](#decision-tree)\n",
    "  - [Random Forest](#random-forest)\n",
    "  - [XGBoost](#xgboost)\n",
    "- [Model Results](#model-results)\n",
    "- [Appendix: Data Dictionary](#appendix-data-dictionary)\n",
    "- [Appendix: Feature Engineering](#appendix-feature-engineering)\n",
    "- [Appendix: Hyperparameter Tuning Grids](#appendix-hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"button\">\n",
    "    <a href=\"eda.html\">Back to Part 1<br>Exploratory Data Analysis</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZNZpq0EotHt"
   },
   "source": [
    "<a id=\"description-and-deliverables\"></a>\n",
    "\n",
    "# Description and Deliverables\n",
    "---\n",
    "\n",
    "[Back to top](#)\n",
    "\n",
    "This project builds on insights from exploratory data analysis (EDA) to **develop predictive models for employee attrition** at Salifort Motors. The primary objective was to identify employees at risk of leaving, enabling targeted retention strategies. The modeling process was designed to be rigorous, transparent, and accessible to both technical and non-technical stakeholders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stakeholders:**  \n",
    "The primary stakeholder is the Human Resources (HR) department, as they will use the results to inform retention strategies. Secondary stakeholders include C-suite executives who oversee company direction, managers implementing day-to-day retention efforts, employees (whose experiences and outcomes are directly affected), and, indirectly, customers—since employee satisfaction can impact customer satisfaction.\n",
    "\n",
    "**Ethical Considerations:**  \n",
    "- Ensure employee data privacy and confidentiality throughout the analysis.\n",
    "- Avoid introducing or perpetuating bias in model predictions (e.g., not unfairly targeting specific groups).\n",
    "- Maintain transparency in how predictions are generated and how they will be used in HR decision-making.\n",
    "\n",
    "### This page summarizes the second part of the project: predictive model construction and validation, with interpretation of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import time\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    StratifiedKFold,\n",
    "    cross_val_predict,\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    fbeta_score,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset into a dataframe\n",
    "df0 = pd.read_csv(\"../resources/HR_capstone_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns as needed\n",
    "df0.rename(\n",
    "    columns={\n",
    "        \"Department\": \"department\",\n",
    "        \"Work_accident\": \"work_accident\",\n",
    "        \"average_montly_hours\": \"average_monthly_hours\",\n",
    "        \"time_spend_company\": \"tenure\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates and save resulting dataframe in a new variable as needed\n",
    "df = df0.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the number of rows containing tenure outliers\n",
    "q1 = df.tenure.quantile(0.25)\n",
    "q3 = df.tenure.quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "upper_bound = q3 + 1.5 * iqr\n",
    "\n",
    "# # Filter the dataframe to find outliers\n",
    "# outliers = df[df.tenure > upper_bound]\n",
    "\n",
    "# # Display the number of outliers\n",
    "# print(f\"Number of tenure outliers: {len(outliers)}\")\n",
    "# print(f\"Outliers percentage of total: {len(outliers) / len(df) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DeTmNVlAANLd"
   },
   "source": [
    "<a id=\"insights\"></a>\n",
    "\n",
    "# Exploratory Data Analysis Insights\n",
    "---\n",
    "\n",
    "[Back to Top](#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQT8YqymD-yL"
   },
   "source": [
    "#### A quick recap of the [exploratory data analysis](./eda.html):\n",
    "\n",
    "The data suggests significant challenges with employee retention at this company. Two main groups of leavers emerge:\n",
    "\n",
    "- **Underutilized and Dissatisfied:** Employees in this category worked on fewer projects and logged fewer hours than a typical full-time schedule, and reported lower satisfaction. These individuals may have been disengaged, assigned less work as they prepared to leave, or potentially subject to layoffs or terminations.\n",
    "- **Overworked and Burned Out:** The second group managed a high number of projects (up to 7) and worked exceptionally long hours—sometimes nearing 80 hours per week. These employees exhibited very low satisfaction and rarely received promotions, suggesting that high demands without recognition or advancement led to burnout and resignation.\n",
    "\n",
    "A majority of the workforce greatly exceeds the typical 40-hour work week (160–184 hours per month), pointing to a workplace culture that expects long hours. The combination of high workload and limited opportunities for advancement likely fuels dissatisfaction and increases the risk of turnover.\n",
    "\n",
    "Performance evaluations show only a weak link to attrition; both those who left and those who stayed received similar review scores. This indicates that strong performance alone does not guarantee retention, especially if employees are overworked or lack opportunities for growth.\n",
    "\n",
    "Other variables—such as department, salary, and work accidents—do not show strong predictive value for employee churn compared to satisfaction and workload. Overall, **the data points to issues with workload management** and limited career progression as the main factors driving employee turnover at this company."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"modeling-strategies-recap\"></a>\n",
    "\n",
    "# Modeling Strategies Recap\n",
    "---\n",
    "\n",
    "[Back to top](#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach\n",
    "\n",
    "Given the business context, the modeling strategy **prioritized recall**—the ability to identify as many at-risk employees as possible—while also considering precision and model simplicity. Four model types were evaluated: Logistic Regression, Decision Tree, Random Forest, and XGBoost. Each model was developed using a systematic pipeline approach to ensure reproducibility and prevent data leakage.\n",
    "\n",
    "Key steps included:\n",
    "- **Data Preparation**: Outliers were removed for logistic regression, and all preprocessing was handled within scikit-learn `Pipelines`.\n",
    "- **Cross-Validation**: Stratified K-Fold cross-validation was used throughout to ensure robust performance estimates, especially given class imbalance.\n",
    "- **Hyperparameter Tuning**: `GridSearchCV` was used for simpler models, while `RandomizedSearchCV` was adopted for more complex models to balance thoroughness and computational efficiency. Details [here](#appendix-hyperparameters).\n",
    "- **Feature Engineering**: Multiple rounds of feature engineering and selection were conducted, focusing on satisfaction, workload, tenure, and interaction terms identified as important in EDA. Details [here](#appendix-feature-engineering).\n",
    "- **Model Evaluation**: In addition to standard metrics (recall, precision, F₁, ROC AUC), a custom F₂ score was introduced to explicitly weight recall four times higher than precision, reflecting the business priority of minimizing false negatives.\n",
    "\n",
    "#### Lessons Learned\n",
    "\n",
    "The modeling process was iterative and exploratory, reflecting both best practices and lessons learned:\n",
    "- **Metric Selection Matters**: Initial experiments optimizing for AUC led to suboptimal recall, especially for logistic regression. Refocusing on recall as the primary metric improved model alignment with business goals.\n",
    "- **Model Complexity vs. Interpretability**: Tree-based models (Random Forest, XGBoost) consistently outperformed logistic regression in overall metrics, but targeted feature engineering allowed logistic regression to become competitive while remaining highly interpretable.\n",
    "- **Feature Engineering**: Additional features and interaction terms provided modest improvements, but the most significant gains came from careful feature selection and focusing on core predictors.\n",
    "- **Efficiency and Automation**: Automating pipeline construction and model evaluation streamlined experimentation and reduced the risk of errors or data leakage.\n",
    "\n",
    "\n",
    "#### Final Model Selection\n",
    "\n",
    "With several high-performing models, explicit selection criteria were established:\n",
    "\n",
    "1. Minimum recall threshold (≥ 0.935)\n",
    "2. Minimum F₂ score (≥ 0.85)\n",
    "3. Fewest features (for simplicity and interpretability)\n",
    "4. Highest F₂ score\n",
    "5. Highest precision\n",
    "\n",
    "This approach ensured that the final models were not only accurate but also practical for deployment and stakeholder communication.\n",
    "\n",
    "#### Next Steps\n",
    "\n",
    "The selected models will be retrained on the full training set and evaluated on the holdout test set (`X_test`). Results—including confusion matrices, feature importances, and actionable insights—will be communicated to HR and leadership to inform retention strategies. The process and findings will be documented to support transparency, ethical use, and ongoing model improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following a focused exploratory data analysis, I entered model development with a clear sense of which features and relationships to explore. However, as this was only my second machine learning project, the learning curve was steep. I approached the modeling process ambitiously—iterating frequently, experimenting extensively, and making my share of mistakes along the way.\n",
    "\n",
    "One of the key challenges was a lack of explicit early-stage success criteria. While I had always intended to compare four model types (Logistic Regression, Decision Tree, Random Forest, and XGBoost), I initially aimed for a vague goal of “best recall,” loosely tempered by precision and simplicity. This unfocused objective led to prolonged tuning efforts far beyond the point of diminishing returns.\n",
    "\n",
    "I began with AUC as my optimization metric but realized that prioritizing recall better aligned with the business objective: minimizing false negatives (i.e., not missing employees likely to leave). This pivot dramatically improved performance, especially for Logistic Regression, which initially performed poorly on recall despite acceptable accuracy.\n",
    "\n",
    "From a modeling standpoint, I built parallel pipelines for logistic and tree-based models. For Logistic Regression, I removed extreme tenure outliers, which introduced a slight divergence between training sets. Going forward, I standardized all data preparation using `Pipeline` objects to streamline preprocessing, reduce errors, and prevent data leakage.\n",
    "\n",
    "I implemented stratified splits throughout training and validation to maintain class distribution. All models used cross-validation with Stratified K-Folds, and I progressively incorporated class weighting to account for imbalance.\n",
    "\n",
    "Hyperparameter tuning began with `GridSearchCV` but shifted to `RandomizedSearchCV` for computationally intensive models like Random Forest and XGBoost. This allowed me to iterate more efficiently while maintaining model quality. Ultimately, I tuned logistic and decision tree models via grid search and the ensemble models via random search.\n",
    "\n",
    "Feature engineering and selection were iterative, but gains eventually plateaued. To investigate further, I analyzed misclassifications from baseline models. The ambiguous nature of many false positives and negatives indicated that most misclassified employees fell into genuine gray zones—not easily distinguishable as stayers or leavers.\n",
    "\n",
    "At this point, I finalized model selection criteria to align with both business and performance goals:\n",
    "\n",
    "- A **minimum recall threshold** of 0.935\n",
    "\n",
    "- A **minimum F₂ score** of 0.85 (prioritizing recall at a 4:1 weight over precision)\n",
    "\n",
    "- Preference for models with **fewer features**\n",
    "\n",
    "- Tie-breaking by **highest F₂**, followed by **precision**\n",
    "\n",
    "This framework allowed me to choose the strongest models with a well-balanced approach to performance and interpretability. With model pipelines finalized and selection criteria defined, I was ready for the final evaluation on the test set (`X_test` and `X_test_lr`)—to plot the results and translate the findings into actionable insights for stakeholders.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cross-validation-results)\"></a>\n",
    "\n",
    "## Cross-Validation Results\n",
    "\n",
    "\n",
    "[Back to top](#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section presents the cross-validated results of all modeling strategies to date. Here, we summarize model performance prior to retraining the final selected models on the full training set and evaluating them on the holdout test set (`X_test`). The results include a comprehensive table of evaluation metrics for each model, as well as visualizations of confusion matrices for both the baseline and top-performing models. These outputs provide a clear comparison of model effectiveness and support transparent, data-driven model selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<img src=\"../resources/images/confusion_matrix_exemplar.png\" style=\"max-width:100%; height:auto; display:block; margin:auto;\">\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show confusion matrix exemplar\n",
    "# display(Image(filename=\"../resources/images/confusion_matrix_exemplar.png\", width=800))\n",
    "display(\n",
    "    HTML(\n",
    "        \"\"\"\n",
    "<img src=\"../resources/images/confusion_matrix_exemplar.png\" style=\"max-width:100%; height:auto; display:block; margin:auto;\">\n",
    "\"\"\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<img src=\"../results/images/base_model_confusion_matrices_confusion_grid.png\" style=\"max-width:100%; height:auto; display:block; margin:auto;\">\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display confusion matrix for base model\n",
    "# display(Image(filename=\"../results/images/base_model_confusion_matrices_confusion_grid.png\", width=800))\n",
    "display(\n",
    "    HTML(\n",
    "        \"\"\"\n",
    "<img src=\"../results/images/base_model_confusion_matrices_confusion_grid.png\" style=\"max-width:100%; height:auto; display:block; margin:auto;\">\n",
    "\"\"\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F2 Score</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>Num Features</th>\n",
       "      <th>Confusion Matrix</th>\n",
       "      <th>Search Time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression (Core + Interactions)</td>\n",
       "      <td>0.962126</td>\n",
       "      <td>0.817433</td>\n",
       "      <td>0.666974</td>\n",
       "      <td>0.510398</td>\n",
       "      <td>0.838128</td>\n",
       "      <td>0.888335</td>\n",
       "      <td>6</td>\n",
       "      <td>[[6039, 1389], [57, 1448]]</td>\n",
       "      <td>0.801151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression with Interaction (feature ...</td>\n",
       "      <td>0.960133</td>\n",
       "      <td>0.819161</td>\n",
       "      <td>0.671312</td>\n",
       "      <td>0.516071</td>\n",
       "      <td>0.841599</td>\n",
       "      <td>0.891666</td>\n",
       "      <td>10</td>\n",
       "      <td>[[6073, 1355], [60, 1445]]</td>\n",
       "      <td>2.056098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic Regression (Core + Interactions + Bur...</td>\n",
       "      <td>0.951495</td>\n",
       "      <td>0.825836</td>\n",
       "      <td>0.689290</td>\n",
       "      <td>0.540377</td>\n",
       "      <td>0.855480</td>\n",
       "      <td>0.903177</td>\n",
       "      <td>6</td>\n",
       "      <td>[[6210, 1218], [73, 1432]]</td>\n",
       "      <td>0.982459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logistic Regression (base)</td>\n",
       "      <td>0.947508</td>\n",
       "      <td>0.796203</td>\n",
       "      <td>0.642342</td>\n",
       "      <td>0.485860</td>\n",
       "      <td>0.822232</td>\n",
       "      <td>0.891388</td>\n",
       "      <td>18</td>\n",
       "      <td>[[5919, 1509], [79, 1426]]</td>\n",
       "      <td>5.170930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Decision Tree (Core + Burnout)</td>\n",
       "      <td>0.943503</td>\n",
       "      <td>0.887040</td>\n",
       "      <td>0.813972</td>\n",
       "      <td>0.715714</td>\n",
       "      <td>0.928378</td>\n",
       "      <td>0.956257</td>\n",
       "      <td>7</td>\n",
       "      <td>[[7402, 597], [90, 1503]]</td>\n",
       "      <td>2.831705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Decision Tree (base)</td>\n",
       "      <td>0.942247</td>\n",
       "      <td>0.881179</td>\n",
       "      <td>0.803103</td>\n",
       "      <td>0.699767</td>\n",
       "      <td>0.923269</td>\n",
       "      <td>0.945551</td>\n",
       "      <td>18</td>\n",
       "      <td>[[7355, 644], [92, 1501]]</td>\n",
       "      <td>2.358475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Random Forest (Core + Burnout)</td>\n",
       "      <td>0.940992</td>\n",
       "      <td>0.896317</td>\n",
       "      <td>0.836729</td>\n",
       "      <td>0.753266</td>\n",
       "      <td>0.939012</td>\n",
       "      <td>0.976981</td>\n",
       "      <td>7</td>\n",
       "      <td>[[7508, 491], [94, 1499]]</td>\n",
       "      <td>78.417716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Random Forest (base)</td>\n",
       "      <td>0.940364</td>\n",
       "      <td>0.863799</td>\n",
       "      <td>0.769784</td>\n",
       "      <td>0.651588</td>\n",
       "      <td>0.906589</td>\n",
       "      <td>0.964169</td>\n",
       "      <td>18</td>\n",
       "      <td>[[7198, 801], [95, 1498]]</td>\n",
       "      <td>178.203500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Logistic Regression with Binning (feature sele...</td>\n",
       "      <td>0.939535</td>\n",
       "      <td>0.855104</td>\n",
       "      <td>0.753531</td>\n",
       "      <td>0.629004</td>\n",
       "      <td>0.896451</td>\n",
       "      <td>0.947481</td>\n",
       "      <td>14</td>\n",
       "      <td>[[6594, 834], [91, 1414]]</td>\n",
       "      <td>1.312480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Logistic Regression with Binning</td>\n",
       "      <td>0.937542</td>\n",
       "      <td>0.854944</td>\n",
       "      <td>0.755151</td>\n",
       "      <td>0.632168</td>\n",
       "      <td>0.897571</td>\n",
       "      <td>0.952159</td>\n",
       "      <td>26</td>\n",
       "      <td>[[6607, 821], [94, 1411]]</td>\n",
       "      <td>3.109411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>XGBoost (base)</td>\n",
       "      <td>0.936598</td>\n",
       "      <td>0.926018</td>\n",
       "      <td>0.910589</td>\n",
       "      <td>0.885986</td>\n",
       "      <td>0.969454</td>\n",
       "      <td>0.986294</td>\n",
       "      <td>18</td>\n",
       "      <td>[[7807, 192], [101, 1492]]</td>\n",
       "      <td>18.722251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>XGBoost (Core + Burnout)</td>\n",
       "      <td>0.936598</td>\n",
       "      <td>0.923953</td>\n",
       "      <td>0.905615</td>\n",
       "      <td>0.876616</td>\n",
       "      <td>0.967577</td>\n",
       "      <td>0.983741</td>\n",
       "      <td>7</td>\n",
       "      <td>[[7789, 210], [101, 1492]]</td>\n",
       "      <td>16.961480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>XGBoost with Binning</td>\n",
       "      <td>0.936598</td>\n",
       "      <td>0.924182</td>\n",
       "      <td>0.906165</td>\n",
       "      <td>0.877647</td>\n",
       "      <td>0.967786</td>\n",
       "      <td>0.985069</td>\n",
       "      <td>26</td>\n",
       "      <td>[[7791, 208], [101, 1492]]</td>\n",
       "      <td>25.402422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>XGBoost with Binning (feature selection)</td>\n",
       "      <td>0.935970</td>\n",
       "      <td>0.926087</td>\n",
       "      <td>0.911648</td>\n",
       "      <td>0.888558</td>\n",
       "      <td>0.969871</td>\n",
       "      <td>0.984534</td>\n",
       "      <td>14</td>\n",
       "      <td>[[7812, 187], [102, 1491]]</td>\n",
       "      <td>19.591144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>XGBoost with Flags (feature selection)</td>\n",
       "      <td>0.935342</td>\n",
       "      <td>0.926847</td>\n",
       "      <td>0.914391</td>\n",
       "      <td>0.894358</td>\n",
       "      <td>0.970913</td>\n",
       "      <td>0.983656</td>\n",
       "      <td>9</td>\n",
       "      <td>[[7823, 176], [103, 1490]]</td>\n",
       "      <td>16.068023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Decision Tree with Binning (feature selection)</td>\n",
       "      <td>0.935342</td>\n",
       "      <td>0.920776</td>\n",
       "      <td>0.899758</td>\n",
       "      <td>0.866783</td>\n",
       "      <td>0.965388</td>\n",
       "      <td>0.959523</td>\n",
       "      <td>14</td>\n",
       "      <td>[[7770, 229], [103, 1490]]</td>\n",
       "      <td>4.507073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Logistic Regression (Core + Burnout)</td>\n",
       "      <td>0.934884</td>\n",
       "      <td>0.798796</td>\n",
       "      <td>0.655638</td>\n",
       "      <td>0.504844</td>\n",
       "      <td>0.834546</td>\n",
       "      <td>0.886564</td>\n",
       "      <td>7</td>\n",
       "      <td>[[6048, 1380], [98, 1407]]</td>\n",
       "      <td>0.779778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>XGBoost with Flags</td>\n",
       "      <td>0.934714</td>\n",
       "      <td>0.925420</td>\n",
       "      <td>0.911819</td>\n",
       "      <td>0.890018</td>\n",
       "      <td>0.969975</td>\n",
       "      <td>0.985902</td>\n",
       "      <td>21</td>\n",
       "      <td>[[7815, 184], [104, 1489]]</td>\n",
       "      <td>23.907882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Decision Tree with Binning</td>\n",
       "      <td>0.934087</td>\n",
       "      <td>0.918632</td>\n",
       "      <td>0.896386</td>\n",
       "      <td>0.861610</td>\n",
       "      <td>0.964137</td>\n",
       "      <td>0.956223</td>\n",
       "      <td>26</td>\n",
       "      <td>[[7760, 239], [105, 1488]]</td>\n",
       "      <td>5.613861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>XGBoost with Interaction</td>\n",
       "      <td>0.934087</td>\n",
       "      <td>0.927565</td>\n",
       "      <td>0.917952</td>\n",
       "      <td>0.902365</td>\n",
       "      <td>0.972269</td>\n",
       "      <td>0.983251</td>\n",
       "      <td>22</td>\n",
       "      <td>[[7838, 161], [105, 1488]]</td>\n",
       "      <td>24.394828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Logistic Regression with Interaction</td>\n",
       "      <td>0.933555</td>\n",
       "      <td>0.808121</td>\n",
       "      <td>0.672571</td>\n",
       "      <td>0.525627</td>\n",
       "      <td>0.846860</td>\n",
       "      <td>0.901327</td>\n",
       "      <td>22</td>\n",
       "      <td>[[6160, 1268], [100, 1405]]</td>\n",
       "      <td>4.670596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Decision Tree with Interaction</td>\n",
       "      <td>0.933459</td>\n",
       "      <td>0.907150</td>\n",
       "      <td>0.870354</td>\n",
       "      <td>0.815241</td>\n",
       "      <td>0.953816</td>\n",
       "      <td>0.960404</td>\n",
       "      <td>22</td>\n",
       "      <td>[[7662, 337], [106, 1487]]</td>\n",
       "      <td>5.739857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Decision Tree with Interaction (feature select...</td>\n",
       "      <td>0.932831</td>\n",
       "      <td>0.902795</td>\n",
       "      <td>0.861200</td>\n",
       "      <td>0.799785</td>\n",
       "      <td>0.950063</td>\n",
       "      <td>0.957669</td>\n",
       "      <td>10</td>\n",
       "      <td>[[7627, 372], [107, 1486]]</td>\n",
       "      <td>4.947870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>XGBoost (Core + Interactions + Burnout)</td>\n",
       "      <td>0.932203</td>\n",
       "      <td>0.917346</td>\n",
       "      <td>0.895928</td>\n",
       "      <td>0.862369</td>\n",
       "      <td>0.964033</td>\n",
       "      <td>0.980458</td>\n",
       "      <td>6</td>\n",
       "      <td>[[7762, 237], [108, 1485]]</td>\n",
       "      <td>16.792476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>XGBoost with Interaction (feature selection)</td>\n",
       "      <td>0.931576</td>\n",
       "      <td>0.925880</td>\n",
       "      <td>0.917465</td>\n",
       "      <td>0.903776</td>\n",
       "      <td>0.972164</td>\n",
       "      <td>0.983235</td>\n",
       "      <td>10</td>\n",
       "      <td>[[7841, 158], [109, 1484]]</td>\n",
       "      <td>20.269477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Decision Tree (Core + Interactions)</td>\n",
       "      <td>0.931576</td>\n",
       "      <td>0.909202</td>\n",
       "      <td>0.877587</td>\n",
       "      <td>0.829514</td>\n",
       "      <td>0.956839</td>\n",
       "      <td>0.960284</td>\n",
       "      <td>6</td>\n",
       "      <td>[[7694, 305], [109, 1484]]</td>\n",
       "      <td>3.563765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>XGBoost (Core + Interactions)</td>\n",
       "      <td>0.930948</td>\n",
       "      <td>0.919747</td>\n",
       "      <td>0.903442</td>\n",
       "      <td>0.877515</td>\n",
       "      <td>0.966952</td>\n",
       "      <td>0.980297</td>\n",
       "      <td>6</td>\n",
       "      <td>[[7792, 207], [110, 1483]]</td>\n",
       "      <td>17.936487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Decision Tree with Flags</td>\n",
       "      <td>0.930948</td>\n",
       "      <td>0.909035</td>\n",
       "      <td>0.878034</td>\n",
       "      <td>0.830812</td>\n",
       "      <td>0.957048</td>\n",
       "      <td>0.966076</td>\n",
       "      <td>21</td>\n",
       "      <td>[[7697, 302], [110, 1483]]</td>\n",
       "      <td>3.652912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Random Forest with Interaction (feature select...</td>\n",
       "      <td>0.930320</td>\n",
       "      <td>0.904210</td>\n",
       "      <td>0.867681</td>\n",
       "      <td>0.812946</td>\n",
       "      <td>0.952877</td>\n",
       "      <td>0.975869</td>\n",
       "      <td>10</td>\n",
       "      <td>[[7658, 341], [111, 1482]]</td>\n",
       "      <td>189.970039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Decision Tree (Core + Interactions + Burnout)</td>\n",
       "      <td>0.930320</td>\n",
       "      <td>0.897420</td>\n",
       "      <td>0.852214</td>\n",
       "      <td>0.786207</td>\n",
       "      <td>0.946414</td>\n",
       "      <td>0.955136</td>\n",
       "      <td>6</td>\n",
       "      <td>[[7596, 403], [111, 1482]]</td>\n",
       "      <td>3.630563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Random Forest with Interaction</td>\n",
       "      <td>0.929692</td>\n",
       "      <td>0.903710</td>\n",
       "      <td>0.867350</td>\n",
       "      <td>0.812843</td>\n",
       "      <td>0.952773</td>\n",
       "      <td>0.975924</td>\n",
       "      <td>22</td>\n",
       "      <td>[[7658, 341], [112, 1481]]</td>\n",
       "      <td>179.275656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Decision Tree with Flags (feature selection)</td>\n",
       "      <td>0.929692</td>\n",
       "      <td>0.909705</td>\n",
       "      <td>0.881285</td>\n",
       "      <td>0.837670</td>\n",
       "      <td>0.958403</td>\n",
       "      <td>0.965284</td>\n",
       "      <td>9</td>\n",
       "      <td>[[7712, 287], [112, 1481]]</td>\n",
       "      <td>3.198139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Random Forest (Core + Interactions + Burnout)</td>\n",
       "      <td>0.929692</td>\n",
       "      <td>0.900852</td>\n",
       "      <td>0.860796</td>\n",
       "      <td>0.801407</td>\n",
       "      <td>0.950063</td>\n",
       "      <td>0.973889</td>\n",
       "      <td>6</td>\n",
       "      <td>[[7632, 367], [112, 1481]]</td>\n",
       "      <td>97.320380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Random Forest with Flags (feature selection)</td>\n",
       "      <td>0.929065</td>\n",
       "      <td>0.873878</td>\n",
       "      <td>0.802385</td>\n",
       "      <td>0.706107</td>\n",
       "      <td>0.923999</td>\n",
       "      <td>0.973045</td>\n",
       "      <td>9</td>\n",
       "      <td>[[7383, 616], [113, 1480]]</td>\n",
       "      <td>79.539320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Random Forest with Flags</td>\n",
       "      <td>0.929065</td>\n",
       "      <td>0.873878</td>\n",
       "      <td>0.802385</td>\n",
       "      <td>0.706107</td>\n",
       "      <td>0.923999</td>\n",
       "      <td>0.973169</td>\n",
       "      <td>21</td>\n",
       "      <td>[[7383, 616], [113, 1480]]</td>\n",
       "      <td>84.636412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Random Forest with Binning (feature selection)</td>\n",
       "      <td>0.928437</td>\n",
       "      <td>0.923682</td>\n",
       "      <td>0.916641</td>\n",
       "      <td>0.905141</td>\n",
       "      <td>0.971956</td>\n",
       "      <td>0.974975</td>\n",
       "      <td>14</td>\n",
       "      <td>[[7844, 155], [114, 1479]]</td>\n",
       "      <td>85.079001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Random Forest with Binning</td>\n",
       "      <td>0.927809</td>\n",
       "      <td>0.923173</td>\n",
       "      <td>0.916305</td>\n",
       "      <td>0.905083</td>\n",
       "      <td>0.971852</td>\n",
       "      <td>0.974756</td>\n",
       "      <td>26</td>\n",
       "      <td>[[7844, 155], [115, 1478]]</td>\n",
       "      <td>86.953273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Random Forest (Core + Interactions)</td>\n",
       "      <td>0.927809</td>\n",
       "      <td>0.861707</td>\n",
       "      <td>0.778509</td>\n",
       "      <td>0.670599</td>\n",
       "      <td>0.912323</td>\n",
       "      <td>0.960897</td>\n",
       "      <td>6</td>\n",
       "      <td>[[7273, 726], [115, 1478]]</td>\n",
       "      <td>134.612169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Logistic Regression with Flags (feature select...</td>\n",
       "      <td>0.917608</td>\n",
       "      <td>0.870524</td>\n",
       "      <td>0.808311</td>\n",
       "      <td>0.722280</td>\n",
       "      <td>0.926676</td>\n",
       "      <td>0.953360</td>\n",
       "      <td>9</td>\n",
       "      <td>[[6897, 531], [124, 1381]]</td>\n",
       "      <td>0.687529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Logistic Regression with Flags</td>\n",
       "      <td>0.917608</td>\n",
       "      <td>0.868335</td>\n",
       "      <td>0.803608</td>\n",
       "      <td>0.714803</td>\n",
       "      <td>0.924437</td>\n",
       "      <td>0.957952</td>\n",
       "      <td>21</td>\n",
       "      <td>[[6877, 551], [124, 1381]]</td>\n",
       "      <td>1.940526</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Model    Recall  F2 Score  \\\n",
       "0           Logistic Regression (Core + Interactions)  0.962126  0.817433   \n",
       "1   Logistic Regression with Interaction (feature ...  0.960133  0.819161   \n",
       "2   Logistic Regression (Core + Interactions + Bur...  0.951495  0.825836   \n",
       "3                          Logistic Regression (base)  0.947508  0.796203   \n",
       "4                      Decision Tree (Core + Burnout)  0.943503  0.887040   \n",
       "5                                Decision Tree (base)  0.942247  0.881179   \n",
       "6                      Random Forest (Core + Burnout)  0.940992  0.896317   \n",
       "7                                Random Forest (base)  0.940364  0.863799   \n",
       "8   Logistic Regression with Binning (feature sele...  0.939535  0.855104   \n",
       "9                    Logistic Regression with Binning  0.937542  0.854944   \n",
       "10                                     XGBoost (base)  0.936598  0.926018   \n",
       "11                           XGBoost (Core + Burnout)  0.936598  0.923953   \n",
       "12                               XGBoost with Binning  0.936598  0.924182   \n",
       "13           XGBoost with Binning (feature selection)  0.935970  0.926087   \n",
       "14             XGBoost with Flags (feature selection)  0.935342  0.926847   \n",
       "15     Decision Tree with Binning (feature selection)  0.935342  0.920776   \n",
       "16               Logistic Regression (Core + Burnout)  0.934884  0.798796   \n",
       "17                                 XGBoost with Flags  0.934714  0.925420   \n",
       "18                         Decision Tree with Binning  0.934087  0.918632   \n",
       "19                           XGBoost with Interaction  0.934087  0.927565   \n",
       "20               Logistic Regression with Interaction  0.933555  0.808121   \n",
       "21                     Decision Tree with Interaction  0.933459  0.907150   \n",
       "22  Decision Tree with Interaction (feature select...  0.932831  0.902795   \n",
       "23            XGBoost (Core + Interactions + Burnout)  0.932203  0.917346   \n",
       "24       XGBoost with Interaction (feature selection)  0.931576  0.925880   \n",
       "25                Decision Tree (Core + Interactions)  0.931576  0.909202   \n",
       "26                      XGBoost (Core + Interactions)  0.930948  0.919747   \n",
       "27                           Decision Tree with Flags  0.930948  0.909035   \n",
       "28  Random Forest with Interaction (feature select...  0.930320  0.904210   \n",
       "29      Decision Tree (Core + Interactions + Burnout)  0.930320  0.897420   \n",
       "30                     Random Forest with Interaction  0.929692  0.903710   \n",
       "31       Decision Tree with Flags (feature selection)  0.929692  0.909705   \n",
       "32      Random Forest (Core + Interactions + Burnout)  0.929692  0.900852   \n",
       "33       Random Forest with Flags (feature selection)  0.929065  0.873878   \n",
       "34                           Random Forest with Flags  0.929065  0.873878   \n",
       "35     Random Forest with Binning (feature selection)  0.928437  0.923682   \n",
       "36                         Random Forest with Binning  0.927809  0.923173   \n",
       "37                Random Forest (Core + Interactions)  0.927809  0.861707   \n",
       "38  Logistic Regression with Flags (feature select...  0.917608  0.870524   \n",
       "39                     Logistic Regression with Flags  0.917608  0.868335   \n",
       "\n",
       "    F1 Score  Precision  Accuracy   ROC AUC  Num Features  \\\n",
       "0   0.666974   0.510398  0.838128  0.888335             6   \n",
       "1   0.671312   0.516071  0.841599  0.891666            10   \n",
       "2   0.689290   0.540377  0.855480  0.903177             6   \n",
       "3   0.642342   0.485860  0.822232  0.891388            18   \n",
       "4   0.813972   0.715714  0.928378  0.956257             7   \n",
       "5   0.803103   0.699767  0.923269  0.945551            18   \n",
       "6   0.836729   0.753266  0.939012  0.976981             7   \n",
       "7   0.769784   0.651588  0.906589  0.964169            18   \n",
       "8   0.753531   0.629004  0.896451  0.947481            14   \n",
       "9   0.755151   0.632168  0.897571  0.952159            26   \n",
       "10  0.910589   0.885986  0.969454  0.986294            18   \n",
       "11  0.905615   0.876616  0.967577  0.983741             7   \n",
       "12  0.906165   0.877647  0.967786  0.985069            26   \n",
       "13  0.911648   0.888558  0.969871  0.984534            14   \n",
       "14  0.914391   0.894358  0.970913  0.983656             9   \n",
       "15  0.899758   0.866783  0.965388  0.959523            14   \n",
       "16  0.655638   0.504844  0.834546  0.886564             7   \n",
       "17  0.911819   0.890018  0.969975  0.985902            21   \n",
       "18  0.896386   0.861610  0.964137  0.956223            26   \n",
       "19  0.917952   0.902365  0.972269  0.983251            22   \n",
       "20  0.672571   0.525627  0.846860  0.901327            22   \n",
       "21  0.870354   0.815241  0.953816  0.960404            22   \n",
       "22  0.861200   0.799785  0.950063  0.957669            10   \n",
       "23  0.895928   0.862369  0.964033  0.980458             6   \n",
       "24  0.917465   0.903776  0.972164  0.983235            10   \n",
       "25  0.877587   0.829514  0.956839  0.960284             6   \n",
       "26  0.903442   0.877515  0.966952  0.980297             6   \n",
       "27  0.878034   0.830812  0.957048  0.966076            21   \n",
       "28  0.867681   0.812946  0.952877  0.975869            10   \n",
       "29  0.852214   0.786207  0.946414  0.955136             6   \n",
       "30  0.867350   0.812843  0.952773  0.975924            22   \n",
       "31  0.881285   0.837670  0.958403  0.965284             9   \n",
       "32  0.860796   0.801407  0.950063  0.973889             6   \n",
       "33  0.802385   0.706107  0.923999  0.973045             9   \n",
       "34  0.802385   0.706107  0.923999  0.973169            21   \n",
       "35  0.916641   0.905141  0.971956  0.974975            14   \n",
       "36  0.916305   0.905083  0.971852  0.974756            26   \n",
       "37  0.778509   0.670599  0.912323  0.960897             6   \n",
       "38  0.808311   0.722280  0.926676  0.953360             9   \n",
       "39  0.803608   0.714803  0.924437  0.957952            21   \n",
       "\n",
       "               Confusion Matrix  Search Time (s)  \n",
       "0    [[6039, 1389], [57, 1448]]         0.801151  \n",
       "1    [[6073, 1355], [60, 1445]]         2.056098  \n",
       "2    [[6210, 1218], [73, 1432]]         0.982459  \n",
       "3    [[5919, 1509], [79, 1426]]         5.170930  \n",
       "4     [[7402, 597], [90, 1503]]         2.831705  \n",
       "5     [[7355, 644], [92, 1501]]         2.358475  \n",
       "6     [[7508, 491], [94, 1499]]        78.417716  \n",
       "7     [[7198, 801], [95, 1498]]       178.203500  \n",
       "8     [[6594, 834], [91, 1414]]         1.312480  \n",
       "9     [[6607, 821], [94, 1411]]         3.109411  \n",
       "10   [[7807, 192], [101, 1492]]        18.722251  \n",
       "11   [[7789, 210], [101, 1492]]        16.961480  \n",
       "12   [[7791, 208], [101, 1492]]        25.402422  \n",
       "13   [[7812, 187], [102, 1491]]        19.591144  \n",
       "14   [[7823, 176], [103, 1490]]        16.068023  \n",
       "15   [[7770, 229], [103, 1490]]         4.507073  \n",
       "16   [[6048, 1380], [98, 1407]]         0.779778  \n",
       "17   [[7815, 184], [104, 1489]]        23.907882  \n",
       "18   [[7760, 239], [105, 1488]]         5.613861  \n",
       "19   [[7838, 161], [105, 1488]]        24.394828  \n",
       "20  [[6160, 1268], [100, 1405]]         4.670596  \n",
       "21   [[7662, 337], [106, 1487]]         5.739857  \n",
       "22   [[7627, 372], [107, 1486]]         4.947870  \n",
       "23   [[7762, 237], [108, 1485]]        16.792476  \n",
       "24   [[7841, 158], [109, 1484]]        20.269477  \n",
       "25   [[7694, 305], [109, 1484]]         3.563765  \n",
       "26   [[7792, 207], [110, 1483]]        17.936487  \n",
       "27   [[7697, 302], [110, 1483]]         3.652912  \n",
       "28   [[7658, 341], [111, 1482]]       189.970039  \n",
       "29   [[7596, 403], [111, 1482]]         3.630563  \n",
       "30   [[7658, 341], [112, 1481]]       179.275656  \n",
       "31   [[7712, 287], [112, 1481]]         3.198139  \n",
       "32   [[7632, 367], [112, 1481]]        97.320380  \n",
       "33   [[7383, 616], [113, 1480]]        79.539320  \n",
       "34   [[7383, 616], [113, 1480]]        84.636412  \n",
       "35   [[7844, 155], [114, 1479]]        85.079001  \n",
       "36   [[7844, 155], [115, 1478]]        86.953273  \n",
       "37   [[7273, 726], [115, 1478]]       134.612169  \n",
       "38   [[6897, 531], [124, 1381]]         0.687529  \n",
       "39   [[6877, 551], [124, 1381]]         1.940526  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display all model evaluation results\n",
    "df_results = pd.read_csv(\"../results/all_model_evaluation_results.csv\")\n",
    "df_results = df_results.rename(\n",
    "    columns={\n",
    "        \"model\": \"Model\",\n",
    "        \"recall\": \"Recall\",\n",
    "        \"f2\": \"F2 Score\",\n",
    "        \"f1\": \"F1 Score\",\n",
    "        \"roc_auc\": \"ROC AUC\",\n",
    "        \"precision\": \"Precision\",\n",
    "        \"accuracy\": \"Accuracy\",\n",
    "        \"features\": \"Num Features\",\n",
    "        \"best_params\": \"Best Params\",\n",
    "        \"cv_best_score\": \"CV Best Score\",\n",
    "        \"conf_matrix\": \"Confusion Matrix\",\n",
    "        \"search_time\": \"Search Time (s)\",\n",
    "    }\n",
    ")\n",
    "df_results[\n",
    "    [\n",
    "        \"Model\",\n",
    "        \"Recall\",\n",
    "        \"F2 Score\",\n",
    "        \"F1 Score\",\n",
    "        \"Precision\",\n",
    "        \"Accuracy\",\n",
    "        \"ROC AUC\",\n",
    "        \"Num Features\",\n",
    "        \"Confusion Matrix\",\n",
    "        \"Search Time (s)\",\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<img src=\"../results/images/top_model_confusion_matrices_confusion_grid.png\" style=\"max-width:100%; height:auto; display:block; margin:auto;\">\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display confusion matrix for all models, top 9\n",
    "# display(Image(filename=\"../results/images/top_model_confusion_matrices_confusion_grid.png\", width=800))\n",
    "display(\n",
    "    HTML(\n",
    "        \"\"\"\n",
    "<img src=\"../results/images/top_model_confusion_matrices_confusion_grid.png\" style=\"max-width:100%; height:auto; display:block; margin:auto;\">\n",
    "\"\"\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZDG9v-NCS69j"
   },
   "source": [
    "<a id=\"model-building\"></a>\n",
    "\n",
    "## Model Building\n",
    "---\n",
    "\n",
    "[Back to top](#)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plan\n",
    "\n",
    "one section for each type of model (log reg, dt, rf, xgb)\n",
    "\n",
    "-\n",
    "\n",
    "-\n",
    "\n",
    "show the df of results for versions of that type of model\n",
    "\n",
    "give a rationale for why i'm choosing which version of the model (show confusion matrix)\n",
    "\n",
    "run it through the cross-validation model evaluation function again and save the model, using the exact same Pipeline and random_state\n",
    "\n",
    "train the saved model on all of X_train (save it again)\n",
    "\n",
    "run the model on X_test\n",
    "\n",
    "compare results of training and testing confusion matrices, plot feature importance, pr-roc, etc.\n",
    "\n",
    "interpret the model, how it makes decisions, business implications\n",
    "\n",
    "-\n",
    "\n",
    "-\n",
    "\n",
    "summarize all four versions at the end, with summary table / bullet points comparing recall, precision, F1, and interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Choose evaluation metric**\n",
    "\n",
    "While **ROC AUC** is a common metric for evaluating binary classifiers—offering a threshold-independent measure of how well the model distinguishes between classes—it is **not ideal for imbalanced problems like employee churn**, where the positive class (those likely to leave) is much smaller and more critical to identify.\n",
    "\n",
    "During model development, I did review ROC AUC to get a general sense of model discrimination. However, for **model selection and tuning**, I ultimately prioritized **recall**. A high recall ensures that we identify as many at-risk employees as possible, aligning with the company's goal to support retention through early intervention. Missing a potential churner (a false negative) is generally more costly than mistakenly flagging someone who is not at risk (a false positive), especially when interventions are supportive rather than punitive.\n",
    "\n",
    "While precision is also important—since too many false positives could dilute resources or create unnecessary concern—recall is more aligned with a **proactive retention strategy**. This tradeoff assumes that HR interventions are constructive and that the company has systems in place to act ethically on model outputs.\n",
    "\n",
    "To avoid unintended harm, I recommend implementing **clear usage guidelines** and **transparency** measures, ensuring that predictions are used to help employees, not penalize them. Calibration and regular fairness audits should accompany any deployment of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Evaluation Tie Breaker**\n",
    "\n",
    " One final twist (I hope). I made the classic mistake of not clearly and rigidly defining success, and I now have a bunch of models that are all excellent at recall, hovering in the 0.93-0.96 range. So I'm making a post-hoc call. At least this one, I'm planning ahead of time. The best model (of each type) will be chosen based on the following tie-breakers (in order):\n",
    "- recall > 0.935\n",
    "- f2 > 0.85 (f2 is a new score, weighing recall at 80%, and precision at 20%)\n",
    "- fewest number of features\n",
    "- highest f2\n",
    "- highest precision\n",
    "\n",
    "I should hope i can make a choice by then. there can't be that many models. I... hehehe... predict... that I'll have it by number three, fewest number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Set evaluation metric'''\n",
    "\n",
    "scoring = \"recall\"\n",
    "\n",
    "\n",
    "# for XGBoost eval_metric\n",
    "def get_xgb_eval_metric(scoring):\n",
    "    mapping = {\n",
    "        \"roc_auc\": \"auc\",  # area under ROC curve\n",
    "        \"accuracy\": \"error\",  # classification error rate\n",
    "        \"f1\": \"logloss\",  # logarithmic loss (not F1, but closest available)\n",
    "        \"precision\": \"logloss\",  # no direct precision metric, logloss is a common fallback\n",
    "        \"recall\": \"logloss\",  # no direct recall metric, logloss is a common fallback\n",
    "    }\n",
    "    return mapping.get(scoring, \"auc\")  # default to 'auc' if not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Encode categorical variables.'''\n",
    "\n",
    "# copy the dataframe to avoid modifying the original\n",
    "df_enc = df.copy()\n",
    "\n",
    "# encode salary as ordinal\n",
    "df_enc[\"salary\"] = df_enc[\"salary\"].map({\"low\": 0, \"medium\": 1, \"high\": 2})\n",
    "\n",
    "# encode department as dummies\n",
    "df_enc = pd.get_dummies(df_enc, columns=[\"department\"])\n",
    "\n",
    "# confirm the changes\n",
    "# print(\"Original salary values:\\n\", df[\"salary\"].value_counts())\n",
    "# print(\"\\nEncoded salary values:\\n\", df_enc[\"salary\"].value_counts())\n",
    "# df_enc.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Split the data into train / test sets for different models\n",
    "\n",
    "One set for tree-based models (decision tree, random forest, XGBoost)\n",
    "Another set for logistic regression (which must have outliers removed and data normalized)\n",
    "Stratify the target variable each time to account for class imbalance.\n",
    "'''\n",
    "\n",
    "# split the data into features and target variable for tree-based models\n",
    "X = df_enc.drop(columns=[\"left\"])\n",
    "y = df_enc[\"left\"]\n",
    "\n",
    "# split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# scale_pos_weight for XGBoost (ratio of negative to positive class in training set)\n",
    "# handles class imbalance\n",
    "scale_pos_weight_value = (y_train == 0).sum() / (y_train == 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into features and target variable for logistic regression\n",
    "# remove outliers from tenure for logistic regression\n",
    "df_enc_lr = df_enc.copy()\n",
    "\n",
    "\"\"\"\n",
    "outliers defined waaaaaay up above, \n",
    "at end of inital data exploration and cleaning\n",
    "code not needed here, but copied for reference\n",
    "\"\"\"\n",
    "# q1 = df.tenure.quantile(0.25)\n",
    "# q3 = df.tenure.quantile(0.75)\n",
    "# iqr = q3 - q1\n",
    "# upper_bound = q3 + 1.5 * iqr\n",
    "\n",
    "# remove outliers\n",
    "df_enc_lr = df_enc_lr[df_enc_lr.tenure < upper_bound]\n",
    "\n",
    "X_lr = df_enc_lr.drop(columns=[\"left\"])\n",
    "y_lr = df_enc_lr[\"left\"]\n",
    "\n",
    "# split the data into training and testing sets for logistic regression\n",
    "X_train_lr, X_test_lr, y_train_lr, y_test_lr = train_test_split(\n",
    "    X_lr, y_lr, test_size=0.2, random_state=42, stratify=y_lr\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Supporting functions for model evaluation and feature engineering.\n",
    "\n",
    "Functions to assemble model configurations,\n",
    "run those models through cross-validation and hyperparameter tuning for evaluation,\n",
    "plot confusion matrices,\n",
    "plot feature importances, \n",
    "and do feature engineering.\n",
    "'''\n",
    "\n",
    "# build models_config for run_model_evaluation\n",
    "def make_models_config(\n",
    "    models,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    feature_func=None,  # can be a function, list, dict, or None\n",
    "    param_grids=None,\n",
    "    scaler=None,\n",
    "    name_suffix=\"\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Build models_config for run_model_evaluation.\n",
    "    - models: dict of {name: estimator}\n",
    "    - X_train, y_train: training data\n",
    "    - feature_func: function, list of functions, dict of {name: func}, or None\n",
    "    - param_grids: dict of {name: param_grid} (or None for empty)\n",
    "    - scaler: sklearn transformer (e.g., StandardScaler) or None\n",
    "    - name_suffix: string to append to model name\n",
    "    \"\"\"\n",
    "    configs = []\n",
    "    for name, model in models.items():\n",
    "        # order of steps matters, features first, then scaler, then model\n",
    "        steps = []\n",
    "\n",
    "        # determine which feature_func to use for this model\n",
    "        func = None\n",
    "        if isinstance(feature_func, dict):  # dict of {name: func}\n",
    "            func = feature_func.get(name)\n",
    "        elif callable(feature_func) or isinstance(feature_func, list):\n",
    "            func = feature_func\n",
    "        # handles a list of feature functions (apply in sequence), or a single function\n",
    "        if func is not None:\n",
    "            if isinstance(func, list):\n",
    "                for i, f in enumerate(func):\n",
    "                    steps.append((f\"features_{i+1}\", FunctionTransformer(f)))\n",
    "            else:\n",
    "                steps.append((\"features\", FunctionTransformer(func)))\n",
    "\n",
    "        # add scaler if provided\n",
    "        if scaler is not None:\n",
    "            steps.append((\"scaler\", scaler))\n",
    "\n",
    "        # add model\n",
    "        steps.append((\"model\", model))\n",
    "\n",
    "        # create the pipeline\n",
    "        pipe = Pipeline(steps)\n",
    "\n",
    "        # add parameter grid if provided\n",
    "        param_grid = {}\n",
    "        if isinstance(param_grids, dict):\n",
    "            param_grid = param_grids.get(name, {})\n",
    "\n",
    "        # add model configuration to the list\n",
    "        configs.append(\n",
    "            {\n",
    "                \"name\": f\"{name}{name_suffix}\",\n",
    "                \"X_train\": X_train,\n",
    "                \"y_train\": y_train,\n",
    "                \"pipeline\": pipe,\n",
    "                \"param_grid\": param_grid,\n",
    "            }\n",
    "        )\n",
    "    return configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run model evaluation function\n",
    "def run_model_evaluation(\n",
    "    models_config,\n",
    "    results_df=None,\n",
    "    scoring=\"recall\",\n",
    "    save_model=False,\n",
    "    search_type=\"grid\",\n",
    "    n_iter=20,\n",
    "):\n",
    "    \"\"\"\n",
    "    Run model training and evaluation for a list of model configurations using cross-validated hyperparameter search.\n",
    "\n",
    "    For each model configuration, performs hyperparameter tuning (GridSearchCV or RandomizedSearchCV),\n",
    "    fits the best pipeline, evaluates cross-validated performance metrics, and optionally saves the best model.\n",
    "\n",
    "    Parameters:\n",
    "        models_config (list of dict): List of model configurations, each containing:\n",
    "            - 'name': Model name (str)\n",
    "            - 'X_train': Training features (pd.DataFrame or np.ndarray)\n",
    "            - 'y_train': Training labels (pd.Series or np.ndarray)\n",
    "            - 'pipeline': sklearn Pipeline object\n",
    "            - 'param_grid': dict of hyperparameters for search\n",
    "        results_df (pd.DataFrame or None): Existing results DataFrame to append to, or None to create a new one.\n",
    "        scoring (str): Scoring metric for model selection (e.g., 'recall', 'accuracy', 'roc_auc').\n",
    "        save_model (bool): If True, saves the best model pipeline to disk for each configuration.\n",
    "        search_type (str): 'grid' for GridSearchCV, 'random' for RandomizedSearchCV.\n",
    "        n_iter (int): Number of parameter settings sampled for RandomizedSearchCV (ignored for grid search).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Results DataFrame with model name, metrics (recall, f1, roc_auc, precision, accuracy),\n",
    "                      number of features, best hyperparameters, best CV score, confusion matrix, and search time.\n",
    "\n",
    "    Notes:\n",
    "        - Uses stratified 5-fold cross-validation for both hyperparameter search and out-of-fold predictions.\n",
    "        - Calculates metrics on cross-validated predictions for robust performance estimates.\n",
    "        - Handles models that do not support predict_proba for ROC AUC gracefully.\n",
    "        - Saves models to '../results/saved_models/' if save_model=True.\n",
    "    \"\"\"\n",
    "    if results_df is None:\n",
    "        results_df = pd.DataFrame(\n",
    "            columns=[\n",
    "                \"model\",\n",
    "                \"recall\",\n",
    "                \"f2\",  # 80% recall, 20% precision (metric created to weigh recall more heavily)\n",
    "                \"f1\",  # 50% recall, 50% precision\n",
    "                \"roc_auc\",\n",
    "                \"precision\",\n",
    "                \"accuracy\",\n",
    "                \"features\",\n",
    "                \"best_params\",\n",
    "                \"cv_best_score\",\n",
    "                \"conf_matrix\",\n",
    "                \"search_time\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    # ensure cross-validation is stratified for balanced class distribution\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    for cfg in models_config:\n",
    "        # time the model training and evaluation\n",
    "        start_time = time.time()\n",
    "        print(f\"Running model: {cfg['name']}...\")\n",
    "\n",
    "        # conditional to choose search type, instantiate the appropriate search class\n",
    "        if search_type == \"random\":\n",
    "            grid = RandomizedSearchCV(\n",
    "                cfg[\"pipeline\"],\n",
    "                cfg[\"param_grid\"],\n",
    "                n_iter=n_iter,\n",
    "                cv=cv,\n",
    "                scoring=scoring,\n",
    "                n_jobs=-1,\n",
    "                verbose=2,\n",
    "                random_state=42,\n",
    "            )\n",
    "        else:\n",
    "            grid = GridSearchCV(\n",
    "                cfg[\"pipeline\"],\n",
    "                cfg[\"param_grid\"],\n",
    "                cv=cv,\n",
    "                scoring=scoring,\n",
    "                n_jobs=-1,\n",
    "                verbose=2,\n",
    "            )\n",
    "\n",
    "        # fit the grid search to the training data\n",
    "        grid.fit(cfg[\"X_train\"], cfg[\"y_train\"])\n",
    "\n",
    "        # print the execution time\n",
    "        end_time = time.time()\n",
    "        search_time = end_time - start_time\n",
    "        print(f\"Execution time for {cfg['name']}: {search_time:.2f} seconds\")\n",
    "\n",
    "        # get the best model and its parameters\n",
    "        best_model = grid.best_estimator_\n",
    "        print(f\"Best parameters for {cfg['name']}: {grid.best_params_}\")\n",
    "        print(f\"Best score for {cfg['name']}: {grid.best_score_:.4f} ({scoring})\")\n",
    "\n",
    "        # --- get the number of features after all pipeline steps ---\n",
    "        # try to transform X_train through all steps except the final estimator\n",
    "        try:\n",
    "            if hasattr(best_model, \"named_steps\"):\n",
    "                # Remove the final estimator step\n",
    "                steps = list(best_model.named_steps.items())\n",
    "                if len(steps) > 1:\n",
    "                    # Remove last step (the model)\n",
    "                    feature_pipeline = Pipeline(steps[:-1])\n",
    "                    X_transformed = feature_pipeline.transform(cfg[\"X_train\"])\n",
    "                    n_features = X_transformed.shape[1]\n",
    "                else:\n",
    "                    n_features = cfg[\"X_train\"].shape[1]\n",
    "            else:\n",
    "                n_features = cfg[\"X_train\"].shape[1]\n",
    "        except Exception as e:\n",
    "            print(f\"Could not determine number of features: {e}\")\n",
    "            n_features = cfg[\"X_train\"].shape[1]\n",
    "\n",
    "        # conditional to save the best model\n",
    "        if save_model:\n",
    "            model_path = f\"../results/saved_models/{cfg['name'].replace(' ', '_').lower()}.joblib\"\n",
    "            joblib.dump(best_model, model_path)\n",
    "            print(f\"Model {cfg['name']} saved successfully.\\n\")\n",
    "        else:\n",
    "            print(f\"Model {cfg['name']} not saved. Set save_model=True to save it.\\n\")\n",
    "\n",
    "        # make predictions using cross-validation to generate out-of-fold predictions for each training sample\n",
    "        # translation:\n",
    "        # substitute for setting aside a validation set\n",
    "        # takes more time, but provides better estimates of model performance\n",
    "        # it makes a prediction for each sample in the training set, using a different fold of the data for each prediction...\n",
    "        # ...the fold where the sample is not included in the 80% training set (the sample is in the 20%)\n",
    "        y_pred = cross_val_predict(\n",
    "            best_model, cfg[\"X_train\"], cfg[\"y_train\"], cv=cv, n_jobs=-1\n",
    "        )\n",
    "\n",
    "        # # check misclassified cases for further analysis\n",
    "        # print(f\"Misclassified cases for {cfg['name']}:\")\n",
    "        # misclassified = cfg['X_train'].copy()\n",
    "        # misclassified['actual'] = cfg[\"y_train\"]\n",
    "        # misclassified['predicted'] = y_pred\n",
    "        # misclassified = misclassified[misclassified['actual'] != misclassified['predicted']]\n",
    "\n",
    "        # # Show counts of each type of misclassification\n",
    "        # counts = misclassified.groupby(['actual', 'predicted']).size().rename('count')\n",
    "        # print(\"\\nMisclassification counts:\")\n",
    "        # print(counts)\n",
    "        # print()\n",
    "\n",
    "        # # Show .describe() for each group, side by side\n",
    "        # pd.set_option('display.max_columns', None)\n",
    "        # for (actual, predicted), group in misclassified.groupby(['actual', 'predicted']):\n",
    "        #     label_map = {0: \"Stayed\", 1: \"Left\"}\n",
    "        #     print(f\"--- Misclassified: Actual={label_map.get(actual, actual)}, Predicted={label_map.get(predicted, predicted)} (n={len(group)}) ---\")\n",
    "        #     print(group.describe().T)\n",
    "        #     print()\n",
    "        # pd.reset_option('display.max_columns')\n",
    "        # print(\"\\n\")\n",
    "\n",
    "        # calculate the ROC AUC score, need predicted probabilities (not just class labels, but confidence in those labels)\n",
    "        # try / except block to handle models that do not support predict_proba (e.g., SVC)\n",
    "        try:\n",
    "            y_proba = cross_val_predict(\n",
    "                best_model,\n",
    "                cfg[\"X_train\"],\n",
    "                cfg[\"y_train\"],\n",
    "                cv=cv,\n",
    "                method=\"predict_proba\",\n",
    "                n_jobs=-1,\n",
    "            )\n",
    "            roc_auc = roc_auc_score(cfg[\"y_train\"], y_proba[:, 1])\n",
    "        except (AttributeError, ValueError):\n",
    "            roc_auc = np.nan\n",
    "            print(f\"Model {cfg['name']} does not support predict_proba.\")\n",
    "\n",
    "        # save results in the results dataframe\n",
    "        results_df.loc[len(results_df)] = {\n",
    "            \"model\": cfg[\"name\"],\n",
    "            \"features\": n_features,\n",
    "            \"accuracy\": accuracy_score(cfg[\"y_train\"], y_pred),\n",
    "            \"precision\": precision_score(cfg[\"y_train\"], y_pred),\n",
    "            \"recall\": recall_score(cfg[\"y_train\"], y_pred),\n",
    "            \"f1\": f1_score(cfg[\"y_train\"], y_pred),\n",
    "            \"f2\": fbeta_score(\n",
    "                cfg[\"y_train\"], y_pred, beta=2\n",
    "            ),  # 80% recall, 20% precision (ratio is \"beta squared : 1\", b^2:1, 2^2:1, 4:1)\n",
    "            \"roc_auc\": roc_auc,\n",
    "            \"conf_matrix\": confusion_matrix(cfg[\"y_train\"], y_pred).tolist(),\n",
    "            \"best_params\": grid.best_params_,\n",
    "            \"cv_best_score\": grid.best_score_,\n",
    "            \"search_time\": search_time,\n",
    "        }\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot confusion matrices from dataframe\n",
    "def plot_confusion_from_results(results_df, save_png=False):\n",
    "    \"\"\"Plots SINGLE confusion matrices from results dataframe and optionally saves png.\"\"\"\n",
    "\n",
    "    class_labels = [\"Stayed\", \"Left\"]\n",
    "\n",
    "    for idx, row in results_df.iterrows():\n",
    "        cm = row[\"conf_matrix\"]\n",
    "        model_name = row[\"model\"]\n",
    "\n",
    "        plt.figure(figsize=(5, 4))\n",
    "        sns.heatmap(\n",
    "            cm,\n",
    "            annot=True,\n",
    "            fmt=\"d\",\n",
    "            cmap=\"Blues\",\n",
    "            cbar=False,\n",
    "            xticklabels=class_labels,\n",
    "            yticklabels=class_labels,\n",
    "        )\n",
    "        plt.title(f\"Confusion Matrix: {model_name}\")\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"Actual\")\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # conditional to save the confusion matrix as a PNG file\n",
    "        if save_png:\n",
    "            plt.savefig(\n",
    "                f\"../results/images/{model_name.replace(' ', '_').lower()}_confusion_matrix.png\"\n",
    "            )\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot confusion matrix grid from dataframe\n",
    "def plot_confusion_grid_from_results(results_df, png_title=None):\n",
    "    \"\"\"Plots ALL confusion matrices from results_df IN A GRID and optionally saves png.\"\"\"\n",
    "    import math\n",
    "\n",
    "    class_labels = [\"Stayed\", \"Left\"]\n",
    "    n_models = len(results_df)\n",
    "    n_cols = 2 if n_models <= 4 else 3\n",
    "    n_rows = math.ceil(n_models / n_cols)\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 4 * n_rows))\n",
    "    axes = axes.flatten() if n_models > 1 else [axes]\n",
    "\n",
    "    for idx, (i, row) in enumerate(results_df.iterrows()):\n",
    "        cm = row[\"conf_matrix\"]\n",
    "        model_name = row[\"model\"]\n",
    "        ax = axes[idx]\n",
    "        sns.heatmap(\n",
    "            cm,\n",
    "            annot=True,\n",
    "            fmt=\"d\",\n",
    "            cmap=\"Blues\",\n",
    "            cbar=False,\n",
    "            xticklabels=class_labels,\n",
    "            yticklabels=class_labels,\n",
    "            ax=ax,\n",
    "        )\n",
    "        ax.set_title(f\"{model_name}\")\n",
    "        ax.set_xlabel(\"Predicted\")\n",
    "        ax.set_ylabel(\"Actual\")\n",
    "\n",
    "    # hide any unused subplots\n",
    "    for j in range(idx + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # conditional to save the confusion grid as a PNG file\n",
    "    if png_title:\n",
    "        fig.suptitle(png_title, fontsize=16, y=1.03)\n",
    "        fig.savefig(\n",
    "            f\"../results/images/{png_title.replace(' ', '_').lower()}_confusion_grid.png\",\n",
    "            bbox_inches=\"tight\",\n",
    "        )\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot feature importances function\n",
    "def load_and_plot_feature_importance(\n",
    "    file_name, model_name, feature_names, top_n=10, save_png=False\n",
    "):\n",
    "    \"\"\"Load a model and plot its feature importance, optionally saves png.\"\"\"\n",
    "\n",
    "    # load model\n",
    "    model_path = os.path.join(\"../results/saved_models\", file_name)\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "    # if model is a pipeline, get the estimator\n",
    "    if hasattr(model, \"named_steps\"):\n",
    "        # for logistic regression, get the scaler's feature names if available\n",
    "        # NOTE: StandardScaler does not change feature names, so X_train_lr.columns is correct here\n",
    "        # if using a transformer that changes the feature set (e.g., OneHotEncoder, ColumnTransformer)...\n",
    "        # ...one would need to extract the transformed feature names from the transformer\n",
    "        estimator = model.named_steps[\"model\"]\n",
    "    # if model is not a pipeline, use it directly (irrelevant for this case, but included for future-proofing)\n",
    "    else:\n",
    "        estimator = model\n",
    "\n",
    "    # get importances\n",
    "    # for tree-based models, use feature_importances_ or coef_ for logistic regression\n",
    "    if hasattr(estimator, \"feature_importances_\"):\n",
    "        importances = estimator.feature_importances_\n",
    "        title = \"Feature Importance\"\n",
    "    elif hasattr(estimator, \"coef_\"):\n",
    "        importances = np.abs(estimator.coef_[0])\n",
    "        title = \"Absolute Coefficient Magnitude\"\n",
    "    else:\n",
    "        print(f\"Model {model_name} does not support feature importance.\")\n",
    "        return\n",
    "\n",
    "    # sort and select top N\n",
    "    indices = np.argsort(importances)[::-1][:top_n]\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.barh(np.array(feature_names)[indices][::-1], importances[indices][::-1])\n",
    "    plt.xlabel(title)\n",
    "    plt.title(f\"{model_name}: Top {top_n} Features\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # conditional to save the feature importance plot as a PNG file\n",
    "    if save_png:\n",
    "        plt.savefig(\n",
    "            f\"../results/images/{model_name.replace(' ', '_').lower()}_feature_importance.png\"\n",
    "        )\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# feature engineering / feature selection functions\n",
    "# the functions commented out below are not used in the final notebook,\n",
    "# but were used during model development\n",
    "# and are included for reference\n",
    "'''\n",
    "\n",
    "# NOTE used in Logistic Regression's winner - \"with Binning (feature selection)\"\n",
    "# add binning features\n",
    "def add_binning_features(df):\n",
    "    df = df.copy()\n",
    "    df[\"satisfaction_bin\"] = pd.cut(\n",
    "        df[\"satisfaction_level\"],\n",
    "        bins=[-0.01, 0.4, 0.7, 1.0],\n",
    "        labels=[\"low\", \"medium\", \"high\"],\n",
    "    )\n",
    "    df[\"hours_bin\"] = pd.cut(\n",
    "        df[\"average_monthly_hours\"],\n",
    "        bins=[0, 160, 240, np.inf],\n",
    "        labels=[\"low\", \"medium\", \"high\"],\n",
    "    )\n",
    "    df[\"projects_bin\"] = pd.cut(\n",
    "        df[\"number_project\"], bins=[0, 2, 5, np.inf], labels=[\"low\", \"medium\", \"high\"]\n",
    "    )\n",
    "    df[\"tenure_bin\"] = pd.cut(\n",
    "        df[\"tenure\"], bins=[0, 3, 5, np.inf], labels=[\"short\", \"mid\", \"long\"]\n",
    "    )\n",
    "    # encode the binned features as dummies\n",
    "    df = pd.get_dummies(\n",
    "        df,\n",
    "        columns=[\"satisfaction_bin\", \"hours_bin\", \"projects_bin\", \"tenure_bin\"],\n",
    "        drop_first=True,\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "# # add interaction features\n",
    "# def add_interaction_features(df):\n",
    "#     df = df.copy()\n",
    "#     df[\"satisfaction_x_projects\"] = df[\"satisfaction_level\"] * df[\"number_project\"]\n",
    "#     df[\"satisfaction_x_hours\"] = df[\"satisfaction_level\"] * df[\"average_monthly_hours\"]\n",
    "#     df[\"evaluation_x_satisfaction\"] = df[\"last_evaluation\"] * df[\"satisfaction_level\"]\n",
    "#     df[\"hours_per_project\"] = df[\"average_monthly_hours\"] / df[\"number_project\"]\n",
    "#     return df\n",
    "\n",
    "\n",
    "# # add flag features\n",
    "# def add_flag_features(df):\n",
    "#     df = df.copy()\n",
    "#     df[\"burnout\"] = (\n",
    "#         (df[\"number_project\"] >= 6) | (df[\"average_monthly_hours\"] >= 240)\n",
    "#     ) & (df[\"satisfaction_level\"] <= 0.3)\n",
    "#     df[\"disengaged\"] = (\n",
    "#         (df[\"number_project\"] <= 2)\n",
    "#         & (df[\"average_monthly_hours\"] < 160)\n",
    "#         & (df[\"satisfaction_level\"] <= 0.5)\n",
    "#     )\n",
    "#     df[\"no_promo_4yr\"] = (df[\"promotion_last_5years\"] == 0) & (df[\"tenure\"] >= 4)\n",
    "#     return df\n",
    "\n",
    "# NOTE used in Logistic Regression's winner - \"with Binning (feature selection)\"\n",
    "# feature selection for logistic regression\n",
    "drop_cols = [col for col in X_train_lr.columns if col.startswith(\"department_\")]\n",
    "drop_cols += [\"salary\", \"work_accident\"]\n",
    "X_train_lr_fs = X_train_lr.drop(columns=drop_cols)\n",
    "\n",
    "# # feature selection for tree-based models\n",
    "# drop_cols = [col for col in X_train.columns if col.startswith(\"department_\")]\n",
    "# drop_cols += [\"salary\", \"work_accident\"]\n",
    "# X_train_fs = X_train.drop(columns=drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE This is the group of features that 'won' for all tree-based models\n",
    "# selected features + burnout flag\n",
    "def select_core_features_with_burnout(df):\n",
    "    df = df.copy()\n",
    "    # burnout flag: (projects >= 6 or hours >= 240) & satisfaction <= 0.3\n",
    "    df[\"burnout\"] = (\n",
    "        (df[\"number_project\"] >= 6) | (df[\"average_monthly_hours\"] >= 240)\n",
    "    ) & (df[\"satisfaction_level\"] <= 0.3)\n",
    "    return df[\n",
    "        [\n",
    "            \"satisfaction_level\",\n",
    "            \"last_evaluation\",\n",
    "            \"number_project\",\n",
    "            \"average_monthly_hours\",\n",
    "            \"tenure\",\n",
    "            \"promotion_last_5years\",\n",
    "            \"burnout\",\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "\n",
    "# # selected features + interactions\n",
    "# def select_core_features_with_interactions(df):\n",
    "#     df = df.copy()\n",
    "#     # interactions\n",
    "#     df[\"satisfaction_x_projects\"] = df[\"satisfaction_level\"] * df[\"number_project\"]\n",
    "#     df[\"hours_per_project\"] = df[\"average_monthly_hours\"] / df[\"number_project\"]\n",
    "#     return df[\n",
    "#         [\n",
    "#             \"satisfaction_level\",\n",
    "#             \"number_project\",\n",
    "#             \"average_monthly_hours\",\n",
    "#             \"tenure\",\n",
    "#             \"satisfaction_x_projects\",\n",
    "#             \"hours_per_project\",\n",
    "#         ]\n",
    "#     ]\n",
    "\n",
    "\n",
    "# # selected features + interactions + burnout flag\n",
    "# def select_core_features_with_interactions_and_burnout(df):\n",
    "#     df = df.copy()\n",
    "#     # burnout flag: (projects >= 6 or hours >= 240) & satisfaction <= 0.3\n",
    "#     df[\"burnout\"] = (\n",
    "#         (df[\"number_project\"] >= 6) | (df[\"average_monthly_hours\"] >= 240)\n",
    "#     ) & (df[\"satisfaction_level\"] <= 0.3)\n",
    "#     # interaction\n",
    "#     df[\"satisfaction_x_projects\"] = df[\"satisfaction_level\"] * df[\"number_project\"]\n",
    "#     return df[\n",
    "#         [\n",
    "#             \"satisfaction_level\",\n",
    "#             \"number_project\",\n",
    "#             \"average_monthly_hours\",\n",
    "#             \"tenure\",\n",
    "#             \"burnout\",\n",
    "#             \"satisfaction_x_projects\",\n",
    "#         ]\n",
    "#     ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class=\"center-icons\">\n",
    "  <i class=\"fa fa-circle spacer-icons\"></i>\n",
    "  <i class=\"fa fa-circle spacer-icons\"></i>\n",
    "  <i class=\"fa fa-circle spacer-icons\"></i>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"logistic-regression\"></a>\n",
    "\n",
    "## Logistic Regression\n",
    "\n",
    "[Back to top](#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression feature engineering hyperparameters\n",
    "lr_fe_params = {\n",
    "    \"model__C\": [0.1, 1.0, 10.0],  # regularization strength (inverse)\n",
    "    \"model__penalty\": [\"l1\", \"l2\"],  # regularization type (L1 = Lasso, L2 = Ridge)\n",
    "    \"model__solver\": [\"liblinear\"],  # optimization algorithm (liblinear supports L1/L2)\n",
    "    \"model__class_weight\": [None, \"balanced\"],  # None or balanced for class imbalance\n",
    "}\n",
    "\n",
    "# define feature engineered logistic regression models, their feature functions, and parameter grids\n",
    "lr_fe_models = {\n",
    "    \"Logistic Regression with Binning\": LogisticRegression(\n",
    "        max_iter=1000, random_state=42\n",
    "    ),\n",
    "}\n",
    "lr_fe_feature_funcs = {\n",
    "    \"Logistic Regression with Binning\": add_binning_features,\n",
    "}\n",
    "lr_fe_param_grids = {\n",
    "    \"Logistic Regression with Binning\": lr_fe_params,\n",
    "}\n",
    "\n",
    "# create models_config for logistic regression with feature engineering and feature selection\n",
    "lr_fe_fs_configs = make_models_config(\n",
    "    lr_fe_models,\n",
    "    X_train_lr_fs,\n",
    "    y_train_lr,\n",
    "    feature_func=lr_fe_feature_funcs,\n",
    "    scaler=StandardScaler(),\n",
    "    param_grids=lr_fe_param_grids,\n",
    "    name_suffix=\" (feature selection)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"emphasis-box\">\n",
    "  <strong>Model selection criteria aligned with both business and performance goals (in order):</strong>\n",
    "  <ul>\n",
    "    <li>A <strong>minimum recall threshold</strong> of 0.935</li>\n",
    "    <li>A <strong>minimum F<sub>2</sub> score</strong> of 0.85 (prioritizing recall at a 4:1 weight over precision)</li>\n",
    "    <li>Preference for models with <strong>fewer features</strong></li>\n",
    "    <li>Tie-breaking by <strong>highest F<sub>2</sub></strong>, followed by <strong>precision</strong></li>\n",
    "  </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All Logistic Regression Models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F2 Score</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>Num Features</th>\n",
       "      <th>Confusion Matrix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression (Core + Interactions)</td>\n",
       "      <td>0.962126</td>\n",
       "      <td>0.817433</td>\n",
       "      <td>0.666974</td>\n",
       "      <td>0.510398</td>\n",
       "      <td>0.838128</td>\n",
       "      <td>0.888335</td>\n",
       "      <td>6</td>\n",
       "      <td>[[6039, 1389], [57, 1448]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression with Interaction (feature ...</td>\n",
       "      <td>0.960133</td>\n",
       "      <td>0.819161</td>\n",
       "      <td>0.671312</td>\n",
       "      <td>0.516071</td>\n",
       "      <td>0.841599</td>\n",
       "      <td>0.891666</td>\n",
       "      <td>10</td>\n",
       "      <td>[[6073, 1355], [60, 1445]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic Regression (Core + Interactions + Bur...</td>\n",
       "      <td>0.951495</td>\n",
       "      <td>0.825836</td>\n",
       "      <td>0.689290</td>\n",
       "      <td>0.540377</td>\n",
       "      <td>0.855480</td>\n",
       "      <td>0.903177</td>\n",
       "      <td>6</td>\n",
       "      <td>[[6210, 1218], [73, 1432]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logistic Regression (base)</td>\n",
       "      <td>0.947508</td>\n",
       "      <td>0.796203</td>\n",
       "      <td>0.642342</td>\n",
       "      <td>0.485860</td>\n",
       "      <td>0.822232</td>\n",
       "      <td>0.891388</td>\n",
       "      <td>18</td>\n",
       "      <td>[[5919, 1509], [79, 1426]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Logistic Regression with Binning (feature sele...</td>\n",
       "      <td>0.939535</td>\n",
       "      <td>0.855104</td>\n",
       "      <td>0.753531</td>\n",
       "      <td>0.629004</td>\n",
       "      <td>0.896451</td>\n",
       "      <td>0.947481</td>\n",
       "      <td>14</td>\n",
       "      <td>[[6594, 834], [91, 1414]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Logistic Regression with Binning</td>\n",
       "      <td>0.937542</td>\n",
       "      <td>0.854944</td>\n",
       "      <td>0.755151</td>\n",
       "      <td>0.632168</td>\n",
       "      <td>0.897571</td>\n",
       "      <td>0.952159</td>\n",
       "      <td>26</td>\n",
       "      <td>[[6607, 821], [94, 1411]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Logistic Regression (Core + Burnout)</td>\n",
       "      <td>0.934884</td>\n",
       "      <td>0.798796</td>\n",
       "      <td>0.655638</td>\n",
       "      <td>0.504844</td>\n",
       "      <td>0.834546</td>\n",
       "      <td>0.886564</td>\n",
       "      <td>7</td>\n",
       "      <td>[[6048, 1380], [98, 1407]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Logistic Regression with Interaction</td>\n",
       "      <td>0.933555</td>\n",
       "      <td>0.808121</td>\n",
       "      <td>0.672571</td>\n",
       "      <td>0.525627</td>\n",
       "      <td>0.846860</td>\n",
       "      <td>0.901327</td>\n",
       "      <td>22</td>\n",
       "      <td>[[6160, 1268], [100, 1405]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Logistic Regression with Flags (feature select...</td>\n",
       "      <td>0.917608</td>\n",
       "      <td>0.870524</td>\n",
       "      <td>0.808311</td>\n",
       "      <td>0.722280</td>\n",
       "      <td>0.926676</td>\n",
       "      <td>0.953360</td>\n",
       "      <td>9</td>\n",
       "      <td>[[6897, 531], [124, 1381]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Logistic Regression with Flags</td>\n",
       "      <td>0.917608</td>\n",
       "      <td>0.868335</td>\n",
       "      <td>0.803608</td>\n",
       "      <td>0.714803</td>\n",
       "      <td>0.924437</td>\n",
       "      <td>0.957952</td>\n",
       "      <td>21</td>\n",
       "      <td>[[6877, 551], [124, 1381]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Model    Recall  F2 Score  \\\n",
       "0           Logistic Regression (Core + Interactions)  0.962126  0.817433   \n",
       "1   Logistic Regression with Interaction (feature ...  0.960133  0.819161   \n",
       "2   Logistic Regression (Core + Interactions + Bur...  0.951495  0.825836   \n",
       "3                          Logistic Regression (base)  0.947508  0.796203   \n",
       "8   Logistic Regression with Binning (feature sele...  0.939535  0.855104   \n",
       "9                    Logistic Regression with Binning  0.937542  0.854944   \n",
       "16               Logistic Regression (Core + Burnout)  0.934884  0.798796   \n",
       "20               Logistic Regression with Interaction  0.933555  0.808121   \n",
       "38  Logistic Regression with Flags (feature select...  0.917608  0.870524   \n",
       "39                     Logistic Regression with Flags  0.917608  0.868335   \n",
       "\n",
       "    F1 Score  Precision  Accuracy   ROC AUC  Num Features  \\\n",
       "0   0.666974   0.510398  0.838128  0.888335             6   \n",
       "1   0.671312   0.516071  0.841599  0.891666            10   \n",
       "2   0.689290   0.540377  0.855480  0.903177             6   \n",
       "3   0.642342   0.485860  0.822232  0.891388            18   \n",
       "8   0.753531   0.629004  0.896451  0.947481            14   \n",
       "9   0.755151   0.632168  0.897571  0.952159            26   \n",
       "16  0.655638   0.504844  0.834546  0.886564             7   \n",
       "20  0.672571   0.525627  0.846860  0.901327            22   \n",
       "38  0.808311   0.722280  0.926676  0.953360             9   \n",
       "39  0.803608   0.714803  0.924437  0.957952            21   \n",
       "\n",
       "               Confusion Matrix  \n",
       "0    [[6039, 1389], [57, 1448]]  \n",
       "1    [[6073, 1355], [60, 1445]]  \n",
       "2    [[6210, 1218], [73, 1432]]  \n",
       "3    [[5919, 1509], [79, 1426]]  \n",
       "8     [[6594, 834], [91, 1414]]  \n",
       "9     [[6607, 821], [94, 1411]]  \n",
       "16   [[6048, 1380], [98, 1407]]  \n",
       "20  [[6160, 1268], [100, 1405]]  \n",
       "38   [[6897, 531], [124, 1381]]  \n",
       "39   [[6877, 551], [124, 1381]]  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display model evaluation results for all logistic regression models\n",
    "df_lr = df_results[df_results[\"Model\"].str.contains(\"Logistic Regression\")]\n",
    "if df_lr.empty:\n",
    "    print(\"No logistic regression models found in results.\")\n",
    "\n",
    "# print(\"Logistic Regression Model Evaluation Results:\")\n",
    "df_lr = df_lr[\n",
    "    [\n",
    "        \"Model\",\n",
    "        \"Recall\",\n",
    "        \"F2 Score\",\n",
    "        \"F1 Score\",\n",
    "        \"Precision\",\n",
    "        \"Accuracy\",\n",
    "        \"ROC AUC\",\n",
    "        \"Num Features\",\n",
    "        \"Confusion Matrix\",\n",
    "    ]\n",
    "]\n",
    "df_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above the minimum recall threshold of 0.935:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F2 Score</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>Num Features</th>\n",
       "      <th>Confusion Matrix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression (Core + Interactions)</td>\n",
       "      <td>0.962126</td>\n",
       "      <td>0.817433</td>\n",
       "      <td>0.666974</td>\n",
       "      <td>0.510398</td>\n",
       "      <td>0.838128</td>\n",
       "      <td>0.888335</td>\n",
       "      <td>6</td>\n",
       "      <td>[[6039, 1389], [57, 1448]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression with Interaction (feature ...</td>\n",
       "      <td>0.960133</td>\n",
       "      <td>0.819161</td>\n",
       "      <td>0.671312</td>\n",
       "      <td>0.516071</td>\n",
       "      <td>0.841599</td>\n",
       "      <td>0.891666</td>\n",
       "      <td>10</td>\n",
       "      <td>[[6073, 1355], [60, 1445]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic Regression (Core + Interactions + Bur...</td>\n",
       "      <td>0.951495</td>\n",
       "      <td>0.825836</td>\n",
       "      <td>0.689290</td>\n",
       "      <td>0.540377</td>\n",
       "      <td>0.855480</td>\n",
       "      <td>0.903177</td>\n",
       "      <td>6</td>\n",
       "      <td>[[6210, 1218], [73, 1432]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logistic Regression (base)</td>\n",
       "      <td>0.947508</td>\n",
       "      <td>0.796203</td>\n",
       "      <td>0.642342</td>\n",
       "      <td>0.485860</td>\n",
       "      <td>0.822232</td>\n",
       "      <td>0.891388</td>\n",
       "      <td>18</td>\n",
       "      <td>[[5919, 1509], [79, 1426]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Logistic Regression with Binning (feature sele...</td>\n",
       "      <td>0.939535</td>\n",
       "      <td>0.855104</td>\n",
       "      <td>0.753531</td>\n",
       "      <td>0.629004</td>\n",
       "      <td>0.896451</td>\n",
       "      <td>0.947481</td>\n",
       "      <td>14</td>\n",
       "      <td>[[6594, 834], [91, 1414]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Logistic Regression with Binning</td>\n",
       "      <td>0.937542</td>\n",
       "      <td>0.854944</td>\n",
       "      <td>0.755151</td>\n",
       "      <td>0.632168</td>\n",
       "      <td>0.897571</td>\n",
       "      <td>0.952159</td>\n",
       "      <td>26</td>\n",
       "      <td>[[6607, 821], [94, 1411]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Model    Recall  F2 Score  \\\n",
       "0          Logistic Regression (Core + Interactions)  0.962126  0.817433   \n",
       "1  Logistic Regression with Interaction (feature ...  0.960133  0.819161   \n",
       "2  Logistic Regression (Core + Interactions + Bur...  0.951495  0.825836   \n",
       "3                         Logistic Regression (base)  0.947508  0.796203   \n",
       "8  Logistic Regression with Binning (feature sele...  0.939535  0.855104   \n",
       "9                   Logistic Regression with Binning  0.937542  0.854944   \n",
       "\n",
       "   F1 Score  Precision  Accuracy   ROC AUC  Num Features  \\\n",
       "0  0.666974   0.510398  0.838128  0.888335             6   \n",
       "1  0.671312   0.516071  0.841599  0.891666            10   \n",
       "2  0.689290   0.540377  0.855480  0.903177             6   \n",
       "3  0.642342   0.485860  0.822232  0.891388            18   \n",
       "8  0.753531   0.629004  0.896451  0.947481            14   \n",
       "9  0.755151   0.632168  0.897571  0.952159            26   \n",
       "\n",
       "             Confusion Matrix  \n",
       "0  [[6039, 1389], [57, 1448]]  \n",
       "1  [[6073, 1355], [60, 1445]]  \n",
       "2  [[6210, 1218], [73, 1432]]  \n",
       "3  [[5919, 1509], [79, 1426]]  \n",
       "8   [[6594, 834], [91, 1414]]  \n",
       "9   [[6607, 821], [94, 1411]]  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lr_1 = df_lr[df_lr[\"Recall\"] >= 0.935]\n",
    "df_lr_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above the minimum F₂ threshold of 0.85:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F2 Score</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>Num Features</th>\n",
       "      <th>Confusion Matrix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Logistic Regression with Binning (feature sele...</td>\n",
       "      <td>0.939535</td>\n",
       "      <td>0.855104</td>\n",
       "      <td>0.753531</td>\n",
       "      <td>0.629004</td>\n",
       "      <td>0.896451</td>\n",
       "      <td>0.947481</td>\n",
       "      <td>14</td>\n",
       "      <td>[[6594, 834], [91, 1414]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Logistic Regression with Binning</td>\n",
       "      <td>0.937542</td>\n",
       "      <td>0.854944</td>\n",
       "      <td>0.755151</td>\n",
       "      <td>0.632168</td>\n",
       "      <td>0.897571</td>\n",
       "      <td>0.952159</td>\n",
       "      <td>26</td>\n",
       "      <td>[[6607, 821], [94, 1411]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Model    Recall  F2 Score  \\\n",
       "8  Logistic Regression with Binning (feature sele...  0.939535  0.855104   \n",
       "9                   Logistic Regression with Binning  0.937542  0.854944   \n",
       "\n",
       "   F1 Score  Precision  Accuracy   ROC AUC  Num Features  \\\n",
       "8  0.753531   0.629004  0.896451  0.947481            14   \n",
       "9  0.755151   0.632168  0.897571  0.952159            26   \n",
       "\n",
       "            Confusion Matrix  \n",
       "8  [[6594, 834], [91, 1414]]  \n",
       "9  [[6607, 821], [94, 1411]]  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lr_2 = df_lr_1[df_lr_1[\"F2 Score\"] >= 0.85]\n",
    "df_lr_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the fewest features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F2 Score</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>Num Features</th>\n",
       "      <th>Confusion Matrix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Logistic Regression with Binning (feature sele...</td>\n",
       "      <td>0.939535</td>\n",
       "      <td>0.855104</td>\n",
       "      <td>0.753531</td>\n",
       "      <td>0.629004</td>\n",
       "      <td>0.896451</td>\n",
       "      <td>0.947481</td>\n",
       "      <td>14</td>\n",
       "      <td>[[6594, 834], [91, 1414]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Model    Recall  F2 Score  \\\n",
       "8  Logistic Regression with Binning (feature sele...  0.939535  0.855104   \n",
       "\n",
       "   F1 Score  Precision  Accuracy   ROC AUC  Num Features  \\\n",
       "8  0.753531   0.629004  0.896451  0.947481            14   \n",
       "\n",
       "            Confusion Matrix  \n",
       "8  [[6594, 834], [91, 1414]]  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lr_winner = df_lr_2[df_lr_2[\"Num Features\"] == df_lr_2[\"Num Features\"].min()]\n",
    "df_lr_winner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the winner is \"Logistic Regression with Binning (feature selection)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model: Logistic Regression with Binning (feature selection)...\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Execution time for Logistic Regression with Binning (feature selection): 6.32 seconds\n",
      "Best parameters for Logistic Regression with Binning (feature selection): {'model__C': 0.1, 'model__class_weight': 'balanced', 'model__penalty': 'l1', 'model__solver': 'liblinear'}\n",
      "Best score for Logistic Regression with Binning (feature selection): 0.9395 (recall)\n",
      "Model Logistic Regression with Binning (feature selection) not saved. Set save_model=True to save it.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "Model, feature function, and parameter grid definitions\n",
    " (used to build pipelines) \n",
    " located at beginning of Logistic Regression section \n",
    " '''\n",
    "results_training = run_model_evaluation(lr_fe_fs_configs, scoring=scoring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Dictionary: Logistic Regression with Binning (Feature Selection)\n",
    "\n",
    "This model uses a subset of the original features, applies binning to key variables, and drops department, salary, and work_accident.\n",
    "\n",
    "| Variable                       | Description                                                                                 |\n",
    "|---------------------------------|---------------------------------------------------------------------------------------------|\n",
    "| satisfaction_level              | Employee-reported job satisfaction level [0–1]                                              |\n",
    "| last_evaluation                 | Score of employee's last performance review [0–1]                                           |\n",
    "| number_project                  | Number of projects employee contributes to                                                  |\n",
    "| average_monthly_hours           | Average number of hours employee worked per month                                           |\n",
    "| tenure                          | How long the employee has been with the company (years)                                     |\n",
    "| promotion_last_5years           | Whether or not the employee was promoted in the last 5 years                                |\n",
    "| satisfaction_bin_medium         | Binary indicator: satisfaction_level is medium (> 0.4 and ≤ 0.7)                            |\n",
    "| satisfaction_bin_high           | Binary indicator: satisfaction_level is high (> 0.7)                                       |\n",
    "| hours_bin_medium                | Binary indicator: average_monthly_hours is medium (> 160 and ≤ 240)                         |\n",
    "| hours_bin_high                  | Binary indicator: average_monthly_hours is high (> 240)                                    |\n",
    "| projects_bin_medium             | Binary indicator: number_project is medium (> 2 and ≤ 5)                                    |\n",
    "| projects_bin_high               | Binary indicator: number_project is high (> 5)                                             |\n",
    "| tenure_bin_mid                  | Binary indicator: tenure is mid (> 3 and ≤ 5 years)                                        |\n",
    "| tenure_bin_long                 | Binary indicator: tenure is long (> 5 years)                                               |\n",
    "\n",
    "- All binned features are one-hot encoded, with the first category dropped (i.e., \"low\" or \"short\" is the reference).\n",
    "\n",
    "Best hyperparameters for **Logistic Regression with Binning (feature selection)**:\n",
    "\n",
    "```python\n",
    "{\n",
    "    'model__C': 0.1,                     # Strong L1 regularization (encourages sparsity/feature selection)\n",
    "    'model__class_weight': 'balanced',   # Adjusts for class imbalance\n",
    "    'model__penalty': 'l1',              # L1 penalty (Lasso)\n",
    "    'model__solver': 'liblinear'         # Solver supporting L1/L2 penalties\n",
    "}\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F2 Score</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>Num Features</th>\n",
       "      <th>Confusion Matrix</th>\n",
       "      <th>Best Params</th>\n",
       "      <th>CV Best Score</th>\n",
       "      <th>Search Time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Logistic Regression with Binning (feature sele...</td>\n",
       "      <td>0.939535</td>\n",
       "      <td>0.855104</td>\n",
       "      <td>0.753531</td>\n",
       "      <td>0.629004</td>\n",
       "      <td>0.896451</td>\n",
       "      <td>0.947481</td>\n",
       "      <td>14</td>\n",
       "      <td>[[6594, 834], [91, 1414]]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Logistic Regression with Binning (feature sele...</td>\n",
       "      <td>0.939535</td>\n",
       "      <td>0.855104</td>\n",
       "      <td>0.753531</td>\n",
       "      <td>0.629004</td>\n",
       "      <td>0.896451</td>\n",
       "      <td>0.947481</td>\n",
       "      <td>14</td>\n",
       "      <td>[[6594, 834], [91, 1414]]</td>\n",
       "      <td>{'model__C': 0.1, 'model__class_weight': 'bala...</td>\n",
       "      <td>0.939535</td>\n",
       "      <td>1.31248</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Model    Recall  F2 Score  \\\n",
       "8  Logistic Regression with Binning (feature sele...  0.939535  0.855104   \n",
       "8  Logistic Regression with Binning (feature sele...  0.939535  0.855104   \n",
       "\n",
       "   F1 Score  Precision  Accuracy   ROC AUC  Num Features  \\\n",
       "8  0.753531   0.629004  0.896451  0.947481            14   \n",
       "8  0.753531   0.629004  0.896451  0.947481            14   \n",
       "\n",
       "            Confusion Matrix  \\\n",
       "8  [[6594, 834], [91, 1414]]   \n",
       "8  [[6594, 834], [91, 1414]]   \n",
       "\n",
       "                                         Best Params  CV Best Score  \\\n",
       "8                                                NaN            NaN   \n",
       "8  {'model__C': 0.1, 'model__class_weight': 'bala...       0.939535   \n",
       "\n",
       "   Search Time (s)  \n",
       "8              NaN  \n",
       "8          1.31248  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare results_training with df_dt_winner\n",
    "confirm = pd.concat(\n",
    "    [\n",
    "        df_lr_winner,\n",
    "        df_results[\n",
    "            df_results[\"Model\"].str.contains(\n",
    "                \"Logistic Regression with Binning (feature selection)\", regex=False\n",
    "            )\n",
    "        ],\n",
    "    ]\n",
    ")\n",
    "confirm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"decision-tree\"></a>\n",
    "\n",
    "## Decision Tree\n",
    "\n",
    "[Back to top](#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tree-based feature engineering parameters\n",
    "tree_fe_params = {\n",
    "    \"Random Forest\": {\n",
    "        \"model__n_estimators\": [100, 300],  # 300 was best, but 100 is faster for FE\n",
    "        \"model__max_depth\": [\n",
    "            3,\n",
    "            4,\n",
    "            5,\n",
    "            8,\n",
    "        ],  # 5 was best, trying for regularization, deeper trees can overfit, take longer to train\n",
    "        \"model__max_features\": [\"sqrt\", 1.0],  # 1.0 was best, but sqrt is common\n",
    "        \"model__max_samples\": [0.7, 1.0],  # 1.0 was best\n",
    "        \"model__min_samples_leaf\": [1, 2],  # 1 or 2\n",
    "        \"model__min_samples_split\": [2, 3],  # 2 or 3\n",
    "        \"model__class_weight\": [\n",
    "            None,\n",
    "            \"balanced\",\n",
    "        ],  # None or balanced for class imbalance\n",
    "    },\n",
    "    \"XGBoost\": {\n",
    "        \"model__n_estimators\": [100, 300],  # 300 was best\n",
    "        \"model__max_depth\": [\n",
    "            3,\n",
    "            4,\n",
    "            5,\n",
    "            8,\n",
    "        ],  # 3 was best (moderate increase in training time)\n",
    "        \"model__learning_rate\": [\n",
    "            0.1,\n",
    "            0.2,\n",
    "        ],  # 0.1 is standard, 0.2 for speed, step size shrinkage\n",
    "        \"model__subsample\": [\n",
    "            0.6,\n",
    "            0.8,\n",
    "            1.0,\n",
    "        ],  # 1.0 was best, row subsampling (adds randomness, helps generalization)\n",
    "        \"model__colsample_bytree\": [\n",
    "            0.6,\n",
    "            0.8,\n",
    "            1.0,\n",
    "        ],  # 1.0 was best, column subsampling (adds randomness, helps generalization)\n",
    "        \"model__min_child_weight\": [\n",
    "            1,\n",
    "            5,\n",
    "        ],  # 1 is default, 5 for regularization, minimum sum of instance weight in a child\n",
    "        \"model__gamma\": [\n",
    "            0,\n",
    "            0.1,\n",
    "            0.2,\n",
    "        ],  # 0.2 was best, try 0 for comparison, minimum loss reduction required to make a split\n",
    "        \"model__scale_pos_weight\": [\n",
    "            1,\n",
    "            scale_pos_weight_value,\n",
    "        ],  # 1 or calculated value for class imbalance\n",
    "        \"model__reg_alpha\": [\n",
    "            0,\n",
    "            0.1,\n",
    "            1,\n",
    "        ],  # L1 regularization (helps control overfitting)\n",
    "        \"model__reg_lambda\": [1, 2, 5],  # L2 regularization (helps control overfitting)\n",
    "    },\n",
    "    \"Decision Tree\": {\n",
    "        \"model__max_depth\": [3, 4, 5, 6, 8],  # best was 8\n",
    "        \"model__min_samples_leaf\": [1, 2, 3],  # 1 was best\n",
    "        \"model__min_samples_split\": [2, 3, 4],  # 2 was best\n",
    "        \"model__class_weight\": [\n",
    "            None,\n",
    "            \"balanced\",\n",
    "        ],  # None or balanced for class imbalance\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# winning tree-based models, feature funcs, param grids\n",
    "dt_fe2_models = {\n",
    "    \"Decision Tree (Core + Burnout)\": DecisionTreeClassifier(random_state=42),\n",
    "}\n",
    "rf_xgb_fe2_models = {\n",
    "    \"Random Forest (Core + Burnout)\": RandomForestClassifier(\n",
    "        random_state=42, n_jobs=-1\n",
    "    ),\n",
    "    \"XGBoost (Core + Burnout)\": XGBClassifier(\n",
    "        eval_metric=get_xgb_eval_metric(scoring), random_state=42, n_jobs=-1\n",
    "    ),\n",
    "}\n",
    "tree_fe2_feature_funcs = {\n",
    "    \"Decision Tree (Core + Burnout)\": select_core_features_with_burnout,\n",
    "    \"Random Forest (Core + Burnout)\": select_core_features_with_burnout,\n",
    "    \"XGBoost (Core + Burnout)\": select_core_features_with_burnout,\n",
    "}\n",
    "tree_fe2_param_grids = {\n",
    "    \"Decision Tree (Core + Burnout)\": tree_fe_params[\"Decision Tree\"],\n",
    "    \"Random Forest (Core + Burnout)\": tree_fe_params[\"Random Forest\"],\n",
    "    \"XGBoost (Core + Burnout)\": tree_fe_params[\"XGBoost\"],\n",
    "}\n",
    "\n",
    "# create models_config for decision tree and random forest / XGBoost with feature engineering and feature selection\n",
    "dt_fe2_configs = make_models_config(\n",
    "    dt_fe2_models,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    feature_func=tree_fe2_feature_funcs,\n",
    "    param_grids=tree_fe2_param_grids,\n",
    ")\n",
    "rf_xgb_fe2_configs = make_models_config(\n",
    "    rf_xgb_fe2_models,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    feature_func=tree_fe2_feature_funcs,\n",
    "    param_grids=tree_fe2_param_grids,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"emphasis-box\">\n",
    "  <strong>Model selection criteria aligned with both business and performance goals (in order):</strong>\n",
    "  <ul>\n",
    "    <li>A <strong>minimum recall threshold</strong> of 0.935</li>\n",
    "    <li>A <strong>minimum F<sub>2</sub> score</strong> of 0.85 (prioritizing recall at a 4:1 weight over precision)</li>\n",
    "    <li>Preference for models with <strong>fewer features</strong></li>\n",
    "    <li>Tie-breaking by <strong>highest F<sub>2</sub></strong>, followed by <strong>precision</strong></li>\n",
    "  </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All Decision Tree Models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F2 Score</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>Num Features</th>\n",
       "      <th>Confusion Matrix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Decision Tree (Core + Burnout)</td>\n",
       "      <td>0.943503</td>\n",
       "      <td>0.887040</td>\n",
       "      <td>0.813972</td>\n",
       "      <td>0.715714</td>\n",
       "      <td>0.928378</td>\n",
       "      <td>0.956257</td>\n",
       "      <td>7</td>\n",
       "      <td>[[7402, 597], [90, 1503]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Decision Tree (base)</td>\n",
       "      <td>0.942247</td>\n",
       "      <td>0.881179</td>\n",
       "      <td>0.803103</td>\n",
       "      <td>0.699767</td>\n",
       "      <td>0.923269</td>\n",
       "      <td>0.945551</td>\n",
       "      <td>18</td>\n",
       "      <td>[[7355, 644], [92, 1501]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Decision Tree with Binning (feature selection)</td>\n",
       "      <td>0.935342</td>\n",
       "      <td>0.920776</td>\n",
       "      <td>0.899758</td>\n",
       "      <td>0.866783</td>\n",
       "      <td>0.965388</td>\n",
       "      <td>0.959523</td>\n",
       "      <td>14</td>\n",
       "      <td>[[7770, 229], [103, 1490]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Decision Tree with Binning</td>\n",
       "      <td>0.934087</td>\n",
       "      <td>0.918632</td>\n",
       "      <td>0.896386</td>\n",
       "      <td>0.861610</td>\n",
       "      <td>0.964137</td>\n",
       "      <td>0.956223</td>\n",
       "      <td>26</td>\n",
       "      <td>[[7760, 239], [105, 1488]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Decision Tree with Interaction</td>\n",
       "      <td>0.933459</td>\n",
       "      <td>0.907150</td>\n",
       "      <td>0.870354</td>\n",
       "      <td>0.815241</td>\n",
       "      <td>0.953816</td>\n",
       "      <td>0.960404</td>\n",
       "      <td>22</td>\n",
       "      <td>[[7662, 337], [106, 1487]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Decision Tree with Interaction (feature select...</td>\n",
       "      <td>0.932831</td>\n",
       "      <td>0.902795</td>\n",
       "      <td>0.861200</td>\n",
       "      <td>0.799785</td>\n",
       "      <td>0.950063</td>\n",
       "      <td>0.957669</td>\n",
       "      <td>10</td>\n",
       "      <td>[[7627, 372], [107, 1486]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Decision Tree (Core + Interactions)</td>\n",
       "      <td>0.931576</td>\n",
       "      <td>0.909202</td>\n",
       "      <td>0.877587</td>\n",
       "      <td>0.829514</td>\n",
       "      <td>0.956839</td>\n",
       "      <td>0.960284</td>\n",
       "      <td>6</td>\n",
       "      <td>[[7694, 305], [109, 1484]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Decision Tree with Flags</td>\n",
       "      <td>0.930948</td>\n",
       "      <td>0.909035</td>\n",
       "      <td>0.878034</td>\n",
       "      <td>0.830812</td>\n",
       "      <td>0.957048</td>\n",
       "      <td>0.966076</td>\n",
       "      <td>21</td>\n",
       "      <td>[[7697, 302], [110, 1483]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Decision Tree (Core + Interactions + Burnout)</td>\n",
       "      <td>0.930320</td>\n",
       "      <td>0.897420</td>\n",
       "      <td>0.852214</td>\n",
       "      <td>0.786207</td>\n",
       "      <td>0.946414</td>\n",
       "      <td>0.955136</td>\n",
       "      <td>6</td>\n",
       "      <td>[[7596, 403], [111, 1482]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Decision Tree with Flags (feature selection)</td>\n",
       "      <td>0.929692</td>\n",
       "      <td>0.909705</td>\n",
       "      <td>0.881285</td>\n",
       "      <td>0.837670</td>\n",
       "      <td>0.958403</td>\n",
       "      <td>0.965284</td>\n",
       "      <td>9</td>\n",
       "      <td>[[7712, 287], [112, 1481]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Model    Recall  F2 Score  \\\n",
       "4                      Decision Tree (Core + Burnout)  0.943503  0.887040   \n",
       "5                                Decision Tree (base)  0.942247  0.881179   \n",
       "15     Decision Tree with Binning (feature selection)  0.935342  0.920776   \n",
       "18                         Decision Tree with Binning  0.934087  0.918632   \n",
       "21                     Decision Tree with Interaction  0.933459  0.907150   \n",
       "22  Decision Tree with Interaction (feature select...  0.932831  0.902795   \n",
       "25                Decision Tree (Core + Interactions)  0.931576  0.909202   \n",
       "27                           Decision Tree with Flags  0.930948  0.909035   \n",
       "29      Decision Tree (Core + Interactions + Burnout)  0.930320  0.897420   \n",
       "31       Decision Tree with Flags (feature selection)  0.929692  0.909705   \n",
       "\n",
       "    F1 Score  Precision  Accuracy   ROC AUC  Num Features  \\\n",
       "4   0.813972   0.715714  0.928378  0.956257             7   \n",
       "5   0.803103   0.699767  0.923269  0.945551            18   \n",
       "15  0.899758   0.866783  0.965388  0.959523            14   \n",
       "18  0.896386   0.861610  0.964137  0.956223            26   \n",
       "21  0.870354   0.815241  0.953816  0.960404            22   \n",
       "22  0.861200   0.799785  0.950063  0.957669            10   \n",
       "25  0.877587   0.829514  0.956839  0.960284             6   \n",
       "27  0.878034   0.830812  0.957048  0.966076            21   \n",
       "29  0.852214   0.786207  0.946414  0.955136             6   \n",
       "31  0.881285   0.837670  0.958403  0.965284             9   \n",
       "\n",
       "              Confusion Matrix  \n",
       "4    [[7402, 597], [90, 1503]]  \n",
       "5    [[7355, 644], [92, 1501]]  \n",
       "15  [[7770, 229], [103, 1490]]  \n",
       "18  [[7760, 239], [105, 1488]]  \n",
       "21  [[7662, 337], [106, 1487]]  \n",
       "22  [[7627, 372], [107, 1486]]  \n",
       "25  [[7694, 305], [109, 1484]]  \n",
       "27  [[7697, 302], [110, 1483]]  \n",
       "29  [[7596, 403], [111, 1482]]  \n",
       "31  [[7712, 287], [112, 1481]]  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display model evaluation results for all decision tree models\n",
    "df_dt = df_results[df_results[\"Model\"].str.contains(\"Decision Tree\")]\n",
    "if df_dt.empty:\n",
    "    print(\"No decision tree models found in results.\")\n",
    "\n",
    "# print(\"Decision Tree Model Evaluation Results:\")\n",
    "df_dt = df_dt[\n",
    "    [\n",
    "        \"Model\",\n",
    "        \"Recall\",\n",
    "        \"F2 Score\",\n",
    "        \"F1 Score\",\n",
    "        \"Precision\",\n",
    "        \"Accuracy\",\n",
    "        \"ROC AUC\",\n",
    "        \"Num Features\",\n",
    "        \"Confusion Matrix\",\n",
    "    ]\n",
    "]\n",
    "df_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above the minimum recall threshold of 0.935:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F2 Score</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>Num Features</th>\n",
       "      <th>Confusion Matrix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Decision Tree (Core + Burnout)</td>\n",
       "      <td>0.943503</td>\n",
       "      <td>0.887040</td>\n",
       "      <td>0.813972</td>\n",
       "      <td>0.715714</td>\n",
       "      <td>0.928378</td>\n",
       "      <td>0.956257</td>\n",
       "      <td>7</td>\n",
       "      <td>[[7402, 597], [90, 1503]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Decision Tree (base)</td>\n",
       "      <td>0.942247</td>\n",
       "      <td>0.881179</td>\n",
       "      <td>0.803103</td>\n",
       "      <td>0.699767</td>\n",
       "      <td>0.923269</td>\n",
       "      <td>0.945551</td>\n",
       "      <td>18</td>\n",
       "      <td>[[7355, 644], [92, 1501]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Decision Tree with Binning (feature selection)</td>\n",
       "      <td>0.935342</td>\n",
       "      <td>0.920776</td>\n",
       "      <td>0.899758</td>\n",
       "      <td>0.866783</td>\n",
       "      <td>0.965388</td>\n",
       "      <td>0.959523</td>\n",
       "      <td>14</td>\n",
       "      <td>[[7770, 229], [103, 1490]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Model    Recall  F2 Score  \\\n",
       "4                   Decision Tree (Core + Burnout)  0.943503  0.887040   \n",
       "5                             Decision Tree (base)  0.942247  0.881179   \n",
       "15  Decision Tree with Binning (feature selection)  0.935342  0.920776   \n",
       "\n",
       "    F1 Score  Precision  Accuracy   ROC AUC  Num Features  \\\n",
       "4   0.813972   0.715714  0.928378  0.956257             7   \n",
       "5   0.803103   0.699767  0.923269  0.945551            18   \n",
       "15  0.899758   0.866783  0.965388  0.959523            14   \n",
       "\n",
       "              Confusion Matrix  \n",
       "4    [[7402, 597], [90, 1503]]  \n",
       "5    [[7355, 644], [92, 1501]]  \n",
       "15  [[7770, 229], [103, 1490]]  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dt_1 = df_dt[df_dt[\"Recall\"] >= 0.935]\n",
    "df_dt_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above the minimum F₂ threshold of 0.85:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F2 Score</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>Num Features</th>\n",
       "      <th>Confusion Matrix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Decision Tree (Core + Burnout)</td>\n",
       "      <td>0.943503</td>\n",
       "      <td>0.887040</td>\n",
       "      <td>0.813972</td>\n",
       "      <td>0.715714</td>\n",
       "      <td>0.928378</td>\n",
       "      <td>0.956257</td>\n",
       "      <td>7</td>\n",
       "      <td>[[7402, 597], [90, 1503]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Decision Tree (base)</td>\n",
       "      <td>0.942247</td>\n",
       "      <td>0.881179</td>\n",
       "      <td>0.803103</td>\n",
       "      <td>0.699767</td>\n",
       "      <td>0.923269</td>\n",
       "      <td>0.945551</td>\n",
       "      <td>18</td>\n",
       "      <td>[[7355, 644], [92, 1501]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Decision Tree with Binning (feature selection)</td>\n",
       "      <td>0.935342</td>\n",
       "      <td>0.920776</td>\n",
       "      <td>0.899758</td>\n",
       "      <td>0.866783</td>\n",
       "      <td>0.965388</td>\n",
       "      <td>0.959523</td>\n",
       "      <td>14</td>\n",
       "      <td>[[7770, 229], [103, 1490]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Model    Recall  F2 Score  \\\n",
       "4                   Decision Tree (Core + Burnout)  0.943503  0.887040   \n",
       "5                             Decision Tree (base)  0.942247  0.881179   \n",
       "15  Decision Tree with Binning (feature selection)  0.935342  0.920776   \n",
       "\n",
       "    F1 Score  Precision  Accuracy   ROC AUC  Num Features  \\\n",
       "4   0.813972   0.715714  0.928378  0.956257             7   \n",
       "5   0.803103   0.699767  0.923269  0.945551            18   \n",
       "15  0.899758   0.866783  0.965388  0.959523            14   \n",
       "\n",
       "              Confusion Matrix  \n",
       "4    [[7402, 597], [90, 1503]]  \n",
       "5    [[7355, 644], [92, 1501]]  \n",
       "15  [[7770, 229], [103, 1490]]  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dt_2 = df_dt_1[df_dt_1[\"F2 Score\"] >= 0.85]\n",
    "df_dt_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the fewest features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F2 Score</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>Num Features</th>\n",
       "      <th>Confusion Matrix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Decision Tree (Core + Burnout)</td>\n",
       "      <td>0.943503</td>\n",
       "      <td>0.88704</td>\n",
       "      <td>0.813972</td>\n",
       "      <td>0.715714</td>\n",
       "      <td>0.928378</td>\n",
       "      <td>0.956257</td>\n",
       "      <td>7</td>\n",
       "      <td>[[7402, 597], [90, 1503]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Model    Recall  F2 Score  F1 Score  Precision  \\\n",
       "4  Decision Tree (Core + Burnout)  0.943503   0.88704  0.813972   0.715714   \n",
       "\n",
       "   Accuracy   ROC AUC  Num Features           Confusion Matrix  \n",
       "4  0.928378  0.956257             7  [[7402, 597], [90, 1503]]  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dt_winner = df_dt_2[df_dt_2[\"Num Features\"] == df_dt_2[\"Num Features\"].min()]\n",
    "df_dt_winner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the winner is \"Decision Tree (Core + Burnout)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model: Decision Tree (Core + Burnout)...\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "Execution time for Decision Tree (Core + Burnout): 3.03 seconds\n",
      "Best parameters for Decision Tree (Core + Burnout): {'model__class_weight': 'balanced', 'model__max_depth': 5, 'model__min_samples_leaf': 1, 'model__min_samples_split': 2}\n",
      "Best score for Decision Tree (Core + Burnout): 0.9435 (recall)\n",
      "Model Decision Tree (Core + Burnout) not saved. Set save_model=True to save it.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>recall</th>\n",
       "      <th>f2</th>\n",
       "      <th>f1</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>precision</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>features</th>\n",
       "      <th>best_params</th>\n",
       "      <th>cv_best_score</th>\n",
       "      <th>conf_matrix</th>\n",
       "      <th>search_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision Tree (Core + Burnout)</td>\n",
       "      <td>0.943503</td>\n",
       "      <td>0.88704</td>\n",
       "      <td>0.813972</td>\n",
       "      <td>0.956257</td>\n",
       "      <td>0.715714</td>\n",
       "      <td>0.928378</td>\n",
       "      <td>7</td>\n",
       "      <td>{'model__class_weight': 'balanced', 'model__ma...</td>\n",
       "      <td>0.943495</td>\n",
       "      <td>[[7402, 597], [90, 1503]]</td>\n",
       "      <td>3.033708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            model    recall       f2        f1   roc_auc  \\\n",
       "1  Decision Tree (Core + Burnout)  0.943503  0.88704  0.813972  0.956257   \n",
       "\n",
       "   precision  accuracy  features  \\\n",
       "1   0.715714  0.928378         7   \n",
       "\n",
       "                                         best_params  cv_best_score  \\\n",
       "1  {'model__class_weight': 'balanced', 'model__ma...       0.943495   \n",
       "\n",
       "                 conf_matrix  search_time  \n",
       "1  [[7402, 597], [90, 1503]]     3.033708  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "Model, feature function, and parameter grid definitions for winning tree-based models\n",
    " (used to build pipelines) \n",
    " located at beginning of Decision Tree section \n",
    " '''\n",
    "results_training = run_model_evaluation(\n",
    "    dt_fe2_configs, results_df=results_training, scoring=scoring\n",
    ")\n",
    "results_training[results_training[\"model\"] == \"Decision Tree (Core + Burnout)\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Dictionary: Decision Tree — \"Core + Burnout\" Features\n",
    "\n",
    "This model uses a focused set of original features plus an engineered \"burnout\" flag.\n",
    "\n",
    "| Variable                | Description                                                                                       |\n",
    "|-------------------------|---------------------------------------------------------------------------------------------------|\n",
    "| satisfaction_level      | Employee-reported job satisfaction level [0–1]                                                    |\n",
    "| last_evaluation         | Score of employee's last performance review [0–1]                                                 |\n",
    "| number_project          | Number of projects employee contributes to                                                        |\n",
    "| average_monthly_hours   | Average number of hours employee worked per month                                                 |\n",
    "| tenure                  | How long the employee has been with the company (years)                                           |\n",
    "| promotion_last_5years   | Whether or not the employee was promoted in the last 5 years                                      |\n",
    "| burnout                 | Flag: True if (number_project ≥ 6 or average_monthly_hours ≥ 240) and satisfaction_level ≤ 0.3    |\n",
    "\n",
    "- The \"burnout\" feature is a logical flag engineered to capture high-risk, overworked, and dissatisfied employees.\n",
    "\n",
    "Best hyperparameters for **Decision Tree (Core + Burnout)**:\n",
    "\n",
    "```python\n",
    "{\n",
    "    'model__class_weight': 'balanced',   # Adjusts for class imbalance\n",
    "    'model__max_depth': 5,               # Shallow tree to prevent overfitting\n",
    "    'model__min_samples_leaf': 1,        # Minimum samples per leaf node\n",
    "    'model__min_samples_split': 2        # Minimum samples to split an internal node\n",
    "}\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F2 Score</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>Num Features</th>\n",
       "      <th>Confusion Matrix</th>\n",
       "      <th>Best Params</th>\n",
       "      <th>CV Best Score</th>\n",
       "      <th>Search Time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Decision Tree (Core + Burnout)</td>\n",
       "      <td>0.943503</td>\n",
       "      <td>0.88704</td>\n",
       "      <td>0.813972</td>\n",
       "      <td>0.715714</td>\n",
       "      <td>0.928378</td>\n",
       "      <td>0.956257</td>\n",
       "      <td>7</td>\n",
       "      <td>[[7402, 597], [90, 1503]]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Decision Tree (Core + Burnout)</td>\n",
       "      <td>0.943503</td>\n",
       "      <td>0.88704</td>\n",
       "      <td>0.813972</td>\n",
       "      <td>0.715714</td>\n",
       "      <td>0.928378</td>\n",
       "      <td>0.956257</td>\n",
       "      <td>7</td>\n",
       "      <td>[[7402, 597], [90, 1503]]</td>\n",
       "      <td>{'model__class_weight': 'balanced', 'model__ma...</td>\n",
       "      <td>0.943495</td>\n",
       "      <td>2.831705</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Model    Recall  F2 Score  F1 Score  Precision  \\\n",
       "4  Decision Tree (Core + Burnout)  0.943503   0.88704  0.813972   0.715714   \n",
       "4  Decision Tree (Core + Burnout)  0.943503   0.88704  0.813972   0.715714   \n",
       "\n",
       "   Accuracy   ROC AUC  Num Features           Confusion Matrix  \\\n",
       "4  0.928378  0.956257             7  [[7402, 597], [90, 1503]]   \n",
       "4  0.928378  0.956257             7  [[7402, 597], [90, 1503]]   \n",
       "\n",
       "                                         Best Params  CV Best Score  \\\n",
       "4                                                NaN            NaN   \n",
       "4  {'model__class_weight': 'balanced', 'model__ma...       0.943495   \n",
       "\n",
       "   Search Time (s)  \n",
       "4              NaN  \n",
       "4         2.831705  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare results_training with df_dt_winner\n",
    "confirm = pd.concat(\n",
    "    [\n",
    "        df_dt_winner,\n",
    "        df_results[\n",
    "            df_results[\"Model\"].str.contains(\n",
    "                \"Decision Tree (Core + Burnout)\", regex=False\n",
    "            )\n",
    "        ],\n",
    "    ]\n",
    ")\n",
    "confirm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"random-forest\"></a>\n",
    "\n",
    "## Random Forest\n",
    "\n",
    "[Back to top](#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"emphasis-box\">\n",
    "  <strong>Model selection criteria aligned with both business and performance goals (in order):</strong>\n",
    "  <ul>\n",
    "    <li>A <strong>minimum recall threshold</strong> of 0.935</li>\n",
    "    <li>A <strong>minimum F<sub>2</sub> score</strong> of 0.85 (prioritizing recall at a 4:1 weight over precision)</li>\n",
    "    <li>Preference for models with <strong>fewer features</strong></li>\n",
    "    <li>Tie-breaking by <strong>highest F<sub>2</sub></strong>, followed by <strong>precision</strong></li>\n",
    "  </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All Random Forest Models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F2 Score</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>Num Features</th>\n",
       "      <th>Confusion Matrix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Random Forest (Core + Burnout)</td>\n",
       "      <td>0.940992</td>\n",
       "      <td>0.896317</td>\n",
       "      <td>0.836729</td>\n",
       "      <td>0.753266</td>\n",
       "      <td>0.939012</td>\n",
       "      <td>0.976981</td>\n",
       "      <td>7</td>\n",
       "      <td>[[7508, 491], [94, 1499]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Random Forest (base)</td>\n",
       "      <td>0.940364</td>\n",
       "      <td>0.863799</td>\n",
       "      <td>0.769784</td>\n",
       "      <td>0.651588</td>\n",
       "      <td>0.906589</td>\n",
       "      <td>0.964169</td>\n",
       "      <td>18</td>\n",
       "      <td>[[7198, 801], [95, 1498]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Random Forest with Interaction (feature select...</td>\n",
       "      <td>0.930320</td>\n",
       "      <td>0.904210</td>\n",
       "      <td>0.867681</td>\n",
       "      <td>0.812946</td>\n",
       "      <td>0.952877</td>\n",
       "      <td>0.975869</td>\n",
       "      <td>10</td>\n",
       "      <td>[[7658, 341], [111, 1482]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Random Forest with Interaction</td>\n",
       "      <td>0.929692</td>\n",
       "      <td>0.903710</td>\n",
       "      <td>0.867350</td>\n",
       "      <td>0.812843</td>\n",
       "      <td>0.952773</td>\n",
       "      <td>0.975924</td>\n",
       "      <td>22</td>\n",
       "      <td>[[7658, 341], [112, 1481]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Random Forest (Core + Interactions + Burnout)</td>\n",
       "      <td>0.929692</td>\n",
       "      <td>0.900852</td>\n",
       "      <td>0.860796</td>\n",
       "      <td>0.801407</td>\n",
       "      <td>0.950063</td>\n",
       "      <td>0.973889</td>\n",
       "      <td>6</td>\n",
       "      <td>[[7632, 367], [112, 1481]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Random Forest with Flags (feature selection)</td>\n",
       "      <td>0.929065</td>\n",
       "      <td>0.873878</td>\n",
       "      <td>0.802385</td>\n",
       "      <td>0.706107</td>\n",
       "      <td>0.923999</td>\n",
       "      <td>0.973045</td>\n",
       "      <td>9</td>\n",
       "      <td>[[7383, 616], [113, 1480]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Random Forest with Flags</td>\n",
       "      <td>0.929065</td>\n",
       "      <td>0.873878</td>\n",
       "      <td>0.802385</td>\n",
       "      <td>0.706107</td>\n",
       "      <td>0.923999</td>\n",
       "      <td>0.973169</td>\n",
       "      <td>21</td>\n",
       "      <td>[[7383, 616], [113, 1480]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Random Forest with Binning (feature selection)</td>\n",
       "      <td>0.928437</td>\n",
       "      <td>0.923682</td>\n",
       "      <td>0.916641</td>\n",
       "      <td>0.905141</td>\n",
       "      <td>0.971956</td>\n",
       "      <td>0.974975</td>\n",
       "      <td>14</td>\n",
       "      <td>[[7844, 155], [114, 1479]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Random Forest with Binning</td>\n",
       "      <td>0.927809</td>\n",
       "      <td>0.923173</td>\n",
       "      <td>0.916305</td>\n",
       "      <td>0.905083</td>\n",
       "      <td>0.971852</td>\n",
       "      <td>0.974756</td>\n",
       "      <td>26</td>\n",
       "      <td>[[7844, 155], [115, 1478]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Random Forest (Core + Interactions)</td>\n",
       "      <td>0.927809</td>\n",
       "      <td>0.861707</td>\n",
       "      <td>0.778509</td>\n",
       "      <td>0.670599</td>\n",
       "      <td>0.912323</td>\n",
       "      <td>0.960897</td>\n",
       "      <td>6</td>\n",
       "      <td>[[7273, 726], [115, 1478]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Model    Recall  F2 Score  \\\n",
       "6                      Random Forest (Core + Burnout)  0.940992  0.896317   \n",
       "7                                Random Forest (base)  0.940364  0.863799   \n",
       "28  Random Forest with Interaction (feature select...  0.930320  0.904210   \n",
       "30                     Random Forest with Interaction  0.929692  0.903710   \n",
       "32      Random Forest (Core + Interactions + Burnout)  0.929692  0.900852   \n",
       "33       Random Forest with Flags (feature selection)  0.929065  0.873878   \n",
       "34                           Random Forest with Flags  0.929065  0.873878   \n",
       "35     Random Forest with Binning (feature selection)  0.928437  0.923682   \n",
       "36                         Random Forest with Binning  0.927809  0.923173   \n",
       "37                Random Forest (Core + Interactions)  0.927809  0.861707   \n",
       "\n",
       "    F1 Score  Precision  Accuracy   ROC AUC  Num Features  \\\n",
       "6   0.836729   0.753266  0.939012  0.976981             7   \n",
       "7   0.769784   0.651588  0.906589  0.964169            18   \n",
       "28  0.867681   0.812946  0.952877  0.975869            10   \n",
       "30  0.867350   0.812843  0.952773  0.975924            22   \n",
       "32  0.860796   0.801407  0.950063  0.973889             6   \n",
       "33  0.802385   0.706107  0.923999  0.973045             9   \n",
       "34  0.802385   0.706107  0.923999  0.973169            21   \n",
       "35  0.916641   0.905141  0.971956  0.974975            14   \n",
       "36  0.916305   0.905083  0.971852  0.974756            26   \n",
       "37  0.778509   0.670599  0.912323  0.960897             6   \n",
       "\n",
       "              Confusion Matrix  \n",
       "6    [[7508, 491], [94, 1499]]  \n",
       "7    [[7198, 801], [95, 1498]]  \n",
       "28  [[7658, 341], [111, 1482]]  \n",
       "30  [[7658, 341], [112, 1481]]  \n",
       "32  [[7632, 367], [112, 1481]]  \n",
       "33  [[7383, 616], [113, 1480]]  \n",
       "34  [[7383, 616], [113, 1480]]  \n",
       "35  [[7844, 155], [114, 1479]]  \n",
       "36  [[7844, 155], [115, 1478]]  \n",
       "37  [[7273, 726], [115, 1478]]  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display model evaluation results for all random forest models\n",
    "df_rf = df_results[df_results[\"Model\"].str.contains(\"Random Forest\")]\n",
    "if df_rf.empty:\n",
    "    print(\"No random forest models found in results.\")\n",
    "\n",
    "# print(\"Random Forest Model Evaluation Results:\")\n",
    "df_rf = df_rf[\n",
    "    [\n",
    "        \"Model\",\n",
    "        \"Recall\",\n",
    "        \"F2 Score\",\n",
    "        \"F1 Score\",\n",
    "        \"Precision\",\n",
    "        \"Accuracy\",\n",
    "        \"ROC AUC\",\n",
    "        \"Num Features\",\n",
    "        \"Confusion Matrix\",\n",
    "    ]\n",
    "]\n",
    "df_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above the minimum recall threshold of 0.935:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F2 Score</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>Num Features</th>\n",
       "      <th>Confusion Matrix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Random Forest (Core + Burnout)</td>\n",
       "      <td>0.940992</td>\n",
       "      <td>0.896317</td>\n",
       "      <td>0.836729</td>\n",
       "      <td>0.753266</td>\n",
       "      <td>0.939012</td>\n",
       "      <td>0.976981</td>\n",
       "      <td>7</td>\n",
       "      <td>[[7508, 491], [94, 1499]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Random Forest (base)</td>\n",
       "      <td>0.940364</td>\n",
       "      <td>0.863799</td>\n",
       "      <td>0.769784</td>\n",
       "      <td>0.651588</td>\n",
       "      <td>0.906589</td>\n",
       "      <td>0.964169</td>\n",
       "      <td>18</td>\n",
       "      <td>[[7198, 801], [95, 1498]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Model    Recall  F2 Score  F1 Score  Precision  \\\n",
       "6  Random Forest (Core + Burnout)  0.940992  0.896317  0.836729   0.753266   \n",
       "7            Random Forest (base)  0.940364  0.863799  0.769784   0.651588   \n",
       "\n",
       "   Accuracy   ROC AUC  Num Features           Confusion Matrix  \n",
       "6  0.939012  0.976981             7  [[7508, 491], [94, 1499]]  \n",
       "7  0.906589  0.964169            18  [[7198, 801], [95, 1498]]  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rf_1 = df_rf[df_rf[\"Recall\"] >= 0.935]\n",
    "df_rf_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above the minimum F₂ threshold of 0.85:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F2 Score</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>Num Features</th>\n",
       "      <th>Confusion Matrix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Random Forest (Core + Burnout)</td>\n",
       "      <td>0.940992</td>\n",
       "      <td>0.896317</td>\n",
       "      <td>0.836729</td>\n",
       "      <td>0.753266</td>\n",
       "      <td>0.939012</td>\n",
       "      <td>0.976981</td>\n",
       "      <td>7</td>\n",
       "      <td>[[7508, 491], [94, 1499]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Random Forest (base)</td>\n",
       "      <td>0.940364</td>\n",
       "      <td>0.863799</td>\n",
       "      <td>0.769784</td>\n",
       "      <td>0.651588</td>\n",
       "      <td>0.906589</td>\n",
       "      <td>0.964169</td>\n",
       "      <td>18</td>\n",
       "      <td>[[7198, 801], [95, 1498]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Model    Recall  F2 Score  F1 Score  Precision  \\\n",
       "6  Random Forest (Core + Burnout)  0.940992  0.896317  0.836729   0.753266   \n",
       "7            Random Forest (base)  0.940364  0.863799  0.769784   0.651588   \n",
       "\n",
       "   Accuracy   ROC AUC  Num Features           Confusion Matrix  \n",
       "6  0.939012  0.976981             7  [[7508, 491], [94, 1499]]  \n",
       "7  0.906589  0.964169            18  [[7198, 801], [95, 1498]]  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rf_2 = df_rf_1[df_rf_1[\"F2 Score\"] >= 0.85]\n",
    "df_rf_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the fewest features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F2 Score</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>Num Features</th>\n",
       "      <th>Confusion Matrix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Random Forest (Core + Burnout)</td>\n",
       "      <td>0.940992</td>\n",
       "      <td>0.896317</td>\n",
       "      <td>0.836729</td>\n",
       "      <td>0.753266</td>\n",
       "      <td>0.939012</td>\n",
       "      <td>0.976981</td>\n",
       "      <td>7</td>\n",
       "      <td>[[7508, 491], [94, 1499]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Model    Recall  F2 Score  F1 Score  Precision  \\\n",
       "6  Random Forest (Core + Burnout)  0.940992  0.896317  0.836729   0.753266   \n",
       "\n",
       "   Accuracy   ROC AUC  Num Features           Confusion Matrix  \n",
       "6  0.939012  0.976981             7  [[7508, 491], [94, 1499]]  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rf_winner = df_rf_2[df_rf_2[\"Num Features\"] == df_rf_2[\"Num Features\"].min()]\n",
    "df_rf_winner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the winner is \"Random Forest (Core + Burnout)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model: Random Forest (Core + Burnout)...\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Execution time for Random Forest (Core + Burnout): 71.20 seconds\n",
      "Best parameters for Random Forest (Core + Burnout): {'model__n_estimators': 300, 'model__min_samples_split': 2, 'model__min_samples_leaf': 1, 'model__max_samples': 1.0, 'model__max_features': 1.0, 'model__max_depth': 5, 'model__class_weight': 'balanced'}\n",
      "Best score for Random Forest (Core + Burnout): 0.9410 (recall)\n",
      "Model Random Forest (Core + Burnout) not saved. Set save_model=True to save it.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>recall</th>\n",
       "      <th>f2</th>\n",
       "      <th>f1</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>precision</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>features</th>\n",
       "      <th>best_params</th>\n",
       "      <th>cv_best_score</th>\n",
       "      <th>conf_matrix</th>\n",
       "      <th>search_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest (Core + Burnout)</td>\n",
       "      <td>0.940992</td>\n",
       "      <td>0.896317</td>\n",
       "      <td>0.836729</td>\n",
       "      <td>0.976981</td>\n",
       "      <td>0.753266</td>\n",
       "      <td>0.939012</td>\n",
       "      <td>7</td>\n",
       "      <td>{'model__n_estimators': 300, 'model__min_sampl...</td>\n",
       "      <td>0.940991</td>\n",
       "      <td>[[7508, 491], [94, 1499]]</td>\n",
       "      <td>71.203241</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            model    recall        f2        f1   roc_auc  \\\n",
       "2  Random Forest (Core + Burnout)  0.940992  0.896317  0.836729  0.976981   \n",
       "\n",
       "   precision  accuracy  features  \\\n",
       "2   0.753266  0.939012         7   \n",
       "\n",
       "                                         best_params  cv_best_score  \\\n",
       "2  {'model__n_estimators': 300, 'model__min_sampl...       0.940991   \n",
       "\n",
       "                 conf_matrix  search_time  \n",
       "2  [[7508, 491], [94, 1499]]    71.203241  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "Model, feature function, and parameter grid definitions for winning tree-based models\n",
    " (used to build pipelines) \n",
    " located at beginning of Decision Tree section \n",
    " '''\n",
    "rf_config = [\n",
    "    cfg for cfg in rf_xgb_fe2_configs if cfg[\"name\"] == \"Random Forest (Core + Burnout)\"\n",
    "]\n",
    "results_training = run_model_evaluation(\n",
    "    rf_config,\n",
    "    results_df=results_training,\n",
    "    scoring=scoring,\n",
    "    search_type=\"random\",\n",
    "    n_iter=50,\n",
    ")\n",
    "results_training[results_training[\"model\"] == \"Random Forest (Core + Burnout)\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Dictionary: Random Forest — \"Core + Burnout\" Features\n",
    "\n",
    "This model uses a focused set of original features plus an engineered \"burnout\" flag.\n",
    "\n",
    "| Variable                | Description                                                                                       |\n",
    "|-------------------------|---------------------------------------------------------------------------------------------------|\n",
    "| satisfaction_level      | Employee-reported job satisfaction level [0–1]                                                    |\n",
    "| last_evaluation         | Score of employee's last performance review [0–1]                                                 |\n",
    "| number_project          | Number of projects employee contributes to                                                        |\n",
    "| average_monthly_hours   | Average number of hours employee worked per month                                                 |\n",
    "| tenure                  | How long the employee has been with the company (years)                                           |\n",
    "| promotion_last_5years   | Whether or not the employee was promoted in the last 5 years                                      |\n",
    "| burnout                 | Flag: True if (number_project ≥ 6 or average_monthly_hours ≥ 240) and satisfaction_level ≤ 0.3    |\n",
    "\n",
    "**Notes:**\n",
    "- The \"burnout\" feature is a logical flag engineered to capture high-risk, overworked, and dissatisfied employees.\n",
    "\n",
    "Best hyperparameters for **Random Forest (Core + Burnout)**:\n",
    "\n",
    "```python\n",
    "{\n",
    "    'model__n_estimators': 300,          # Number of trees in the forest\n",
    "    'model__min_samples_split': 2,       # Minimum samples to split an internal node\n",
    "    'model__min_samples_leaf': 1,        # Minimum samples per leaf node\n",
    "    'model__max_samples': 1.0,           # Use all samples for each tree\n",
    "    'model__max_features': 1.0,          # Use all features for each split\n",
    "    'model__max_depth': 5,               # Maximum depth of each tree\n",
    "    'model__class_weight': 'balanced'    # Adjusts for class imbalance\n",
    "}\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F2 Score</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>Num Features</th>\n",
       "      <th>Confusion Matrix</th>\n",
       "      <th>Best Params</th>\n",
       "      <th>CV Best Score</th>\n",
       "      <th>Search Time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Random Forest (Core + Burnout)</td>\n",
       "      <td>0.940992</td>\n",
       "      <td>0.896317</td>\n",
       "      <td>0.836729</td>\n",
       "      <td>0.753266</td>\n",
       "      <td>0.939012</td>\n",
       "      <td>0.976981</td>\n",
       "      <td>7</td>\n",
       "      <td>[[7508, 491], [94, 1499]]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Random Forest (Core + Burnout)</td>\n",
       "      <td>0.940992</td>\n",
       "      <td>0.896317</td>\n",
       "      <td>0.836729</td>\n",
       "      <td>0.753266</td>\n",
       "      <td>0.939012</td>\n",
       "      <td>0.976981</td>\n",
       "      <td>7</td>\n",
       "      <td>[[7508, 491], [94, 1499]]</td>\n",
       "      <td>{'model__n_estimators': 300, 'model__min_sampl...</td>\n",
       "      <td>0.940991</td>\n",
       "      <td>78.417716</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Model    Recall  F2 Score  F1 Score  Precision  \\\n",
       "6  Random Forest (Core + Burnout)  0.940992  0.896317  0.836729   0.753266   \n",
       "6  Random Forest (Core + Burnout)  0.940992  0.896317  0.836729   0.753266   \n",
       "\n",
       "   Accuracy   ROC AUC  Num Features           Confusion Matrix  \\\n",
       "6  0.939012  0.976981             7  [[7508, 491], [94, 1499]]   \n",
       "6  0.939012  0.976981             7  [[7508, 491], [94, 1499]]   \n",
       "\n",
       "                                         Best Params  CV Best Score  \\\n",
       "6                                                NaN            NaN   \n",
       "6  {'model__n_estimators': 300, 'model__min_sampl...       0.940991   \n",
       "\n",
       "   Search Time (s)  \n",
       "6              NaN  \n",
       "6        78.417716  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confirm = pd.concat(\n",
    "    [\n",
    "        df_rf_winner,\n",
    "        df_results[\n",
    "            df_results[\"Model\"].str.contains(\n",
    "                \"Random Forest (Core + Burnout)\", regex=False\n",
    "            )\n",
    "        ],\n",
    "    ]\n",
    ")\n",
    "confirm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"xgboost\"></a>\n",
    "\n",
    "## XGBoost\n",
    "\n",
    "[Back to top](#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"emphasis-box\">\n",
    "  <strong>Model selection criteria aligned with both business and performance goals (in order):</strong>\n",
    "  <ul>\n",
    "    <li>A <strong>minimum recall threshold</strong> of 0.935</li>\n",
    "    <li>A <strong>minimum F<sub>2</sub> score</strong> of 0.85 (prioritizing recall at a 4:1 weight over precision)</li>\n",
    "    <li>Preference for models with <strong>fewer features</strong></li>\n",
    "    <li>Tie-breaking by <strong>highest F<sub>2</sub></strong>, followed by <strong>precision</strong></li>\n",
    "  </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All XGBoost Models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F2 Score</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>Num Features</th>\n",
       "      <th>Confusion Matrix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>XGBoost (base)</td>\n",
       "      <td>0.936598</td>\n",
       "      <td>0.926018</td>\n",
       "      <td>0.910589</td>\n",
       "      <td>0.885986</td>\n",
       "      <td>0.969454</td>\n",
       "      <td>0.986294</td>\n",
       "      <td>18</td>\n",
       "      <td>[[7807, 192], [101, 1492]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>XGBoost (Core + Burnout)</td>\n",
       "      <td>0.936598</td>\n",
       "      <td>0.923953</td>\n",
       "      <td>0.905615</td>\n",
       "      <td>0.876616</td>\n",
       "      <td>0.967577</td>\n",
       "      <td>0.983741</td>\n",
       "      <td>7</td>\n",
       "      <td>[[7789, 210], [101, 1492]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>XGBoost with Binning</td>\n",
       "      <td>0.936598</td>\n",
       "      <td>0.924182</td>\n",
       "      <td>0.906165</td>\n",
       "      <td>0.877647</td>\n",
       "      <td>0.967786</td>\n",
       "      <td>0.985069</td>\n",
       "      <td>26</td>\n",
       "      <td>[[7791, 208], [101, 1492]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>XGBoost with Binning (feature selection)</td>\n",
       "      <td>0.935970</td>\n",
       "      <td>0.926087</td>\n",
       "      <td>0.911648</td>\n",
       "      <td>0.888558</td>\n",
       "      <td>0.969871</td>\n",
       "      <td>0.984534</td>\n",
       "      <td>14</td>\n",
       "      <td>[[7812, 187], [102, 1491]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>XGBoost with Flags (feature selection)</td>\n",
       "      <td>0.935342</td>\n",
       "      <td>0.926847</td>\n",
       "      <td>0.914391</td>\n",
       "      <td>0.894358</td>\n",
       "      <td>0.970913</td>\n",
       "      <td>0.983656</td>\n",
       "      <td>9</td>\n",
       "      <td>[[7823, 176], [103, 1490]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>XGBoost with Flags</td>\n",
       "      <td>0.934714</td>\n",
       "      <td>0.925420</td>\n",
       "      <td>0.911819</td>\n",
       "      <td>0.890018</td>\n",
       "      <td>0.969975</td>\n",
       "      <td>0.985902</td>\n",
       "      <td>21</td>\n",
       "      <td>[[7815, 184], [104, 1489]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>XGBoost with Interaction</td>\n",
       "      <td>0.934087</td>\n",
       "      <td>0.927565</td>\n",
       "      <td>0.917952</td>\n",
       "      <td>0.902365</td>\n",
       "      <td>0.972269</td>\n",
       "      <td>0.983251</td>\n",
       "      <td>22</td>\n",
       "      <td>[[7838, 161], [105, 1488]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>XGBoost (Core + Interactions + Burnout)</td>\n",
       "      <td>0.932203</td>\n",
       "      <td>0.917346</td>\n",
       "      <td>0.895928</td>\n",
       "      <td>0.862369</td>\n",
       "      <td>0.964033</td>\n",
       "      <td>0.980458</td>\n",
       "      <td>6</td>\n",
       "      <td>[[7762, 237], [108, 1485]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>XGBoost with Interaction (feature selection)</td>\n",
       "      <td>0.931576</td>\n",
       "      <td>0.925880</td>\n",
       "      <td>0.917465</td>\n",
       "      <td>0.903776</td>\n",
       "      <td>0.972164</td>\n",
       "      <td>0.983235</td>\n",
       "      <td>10</td>\n",
       "      <td>[[7841, 158], [109, 1484]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>XGBoost (Core + Interactions)</td>\n",
       "      <td>0.930948</td>\n",
       "      <td>0.919747</td>\n",
       "      <td>0.903442</td>\n",
       "      <td>0.877515</td>\n",
       "      <td>0.966952</td>\n",
       "      <td>0.980297</td>\n",
       "      <td>6</td>\n",
       "      <td>[[7792, 207], [110, 1483]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Model    Recall  F2 Score  \\\n",
       "10                                XGBoost (base)  0.936598  0.926018   \n",
       "11                      XGBoost (Core + Burnout)  0.936598  0.923953   \n",
       "12                          XGBoost with Binning  0.936598  0.924182   \n",
       "13      XGBoost with Binning (feature selection)  0.935970  0.926087   \n",
       "14        XGBoost with Flags (feature selection)  0.935342  0.926847   \n",
       "17                            XGBoost with Flags  0.934714  0.925420   \n",
       "19                      XGBoost with Interaction  0.934087  0.927565   \n",
       "23       XGBoost (Core + Interactions + Burnout)  0.932203  0.917346   \n",
       "24  XGBoost with Interaction (feature selection)  0.931576  0.925880   \n",
       "26                 XGBoost (Core + Interactions)  0.930948  0.919747   \n",
       "\n",
       "    F1 Score  Precision  Accuracy   ROC AUC  Num Features  \\\n",
       "10  0.910589   0.885986  0.969454  0.986294            18   \n",
       "11  0.905615   0.876616  0.967577  0.983741             7   \n",
       "12  0.906165   0.877647  0.967786  0.985069            26   \n",
       "13  0.911648   0.888558  0.969871  0.984534            14   \n",
       "14  0.914391   0.894358  0.970913  0.983656             9   \n",
       "17  0.911819   0.890018  0.969975  0.985902            21   \n",
       "19  0.917952   0.902365  0.972269  0.983251            22   \n",
       "23  0.895928   0.862369  0.964033  0.980458             6   \n",
       "24  0.917465   0.903776  0.972164  0.983235            10   \n",
       "26  0.903442   0.877515  0.966952  0.980297             6   \n",
       "\n",
       "              Confusion Matrix  \n",
       "10  [[7807, 192], [101, 1492]]  \n",
       "11  [[7789, 210], [101, 1492]]  \n",
       "12  [[7791, 208], [101, 1492]]  \n",
       "13  [[7812, 187], [102, 1491]]  \n",
       "14  [[7823, 176], [103, 1490]]  \n",
       "17  [[7815, 184], [104, 1489]]  \n",
       "19  [[7838, 161], [105, 1488]]  \n",
       "23  [[7762, 237], [108, 1485]]  \n",
       "24  [[7841, 158], [109, 1484]]  \n",
       "26  [[7792, 207], [110, 1483]]  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display model evaluation results for all XGBoost models\n",
    "df_xgb = df_results[df_results[\"Model\"].str.contains(\"XGBoost\")]\n",
    "if df_xgb.empty:\n",
    "    print(\"No XGBoost models found in results.\")\n",
    "# print(\"XGBoost Model Evaluation Results:\")\n",
    "df_xgb = df_xgb[\n",
    "    [\n",
    "        \"Model\",\n",
    "        \"Recall\",\n",
    "        \"F2 Score\",\n",
    "        \"F1 Score\",\n",
    "        \"Precision\",\n",
    "        \"Accuracy\",\n",
    "        \"ROC AUC\",\n",
    "        \"Num Features\",\n",
    "        \"Confusion Matrix\",\n",
    "    ]\n",
    "]\n",
    "df_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above the minimum recall threshold of 0.935:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F2 Score</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>Num Features</th>\n",
       "      <th>Confusion Matrix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>XGBoost (base)</td>\n",
       "      <td>0.936598</td>\n",
       "      <td>0.926018</td>\n",
       "      <td>0.910589</td>\n",
       "      <td>0.885986</td>\n",
       "      <td>0.969454</td>\n",
       "      <td>0.986294</td>\n",
       "      <td>18</td>\n",
       "      <td>[[7807, 192], [101, 1492]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>XGBoost (Core + Burnout)</td>\n",
       "      <td>0.936598</td>\n",
       "      <td>0.923953</td>\n",
       "      <td>0.905615</td>\n",
       "      <td>0.876616</td>\n",
       "      <td>0.967577</td>\n",
       "      <td>0.983741</td>\n",
       "      <td>7</td>\n",
       "      <td>[[7789, 210], [101, 1492]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>XGBoost with Binning</td>\n",
       "      <td>0.936598</td>\n",
       "      <td>0.924182</td>\n",
       "      <td>0.906165</td>\n",
       "      <td>0.877647</td>\n",
       "      <td>0.967786</td>\n",
       "      <td>0.985069</td>\n",
       "      <td>26</td>\n",
       "      <td>[[7791, 208], [101, 1492]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>XGBoost with Binning (feature selection)</td>\n",
       "      <td>0.935970</td>\n",
       "      <td>0.926087</td>\n",
       "      <td>0.911648</td>\n",
       "      <td>0.888558</td>\n",
       "      <td>0.969871</td>\n",
       "      <td>0.984534</td>\n",
       "      <td>14</td>\n",
       "      <td>[[7812, 187], [102, 1491]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>XGBoost with Flags (feature selection)</td>\n",
       "      <td>0.935342</td>\n",
       "      <td>0.926847</td>\n",
       "      <td>0.914391</td>\n",
       "      <td>0.894358</td>\n",
       "      <td>0.970913</td>\n",
       "      <td>0.983656</td>\n",
       "      <td>9</td>\n",
       "      <td>[[7823, 176], [103, 1490]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Model    Recall  F2 Score  F1 Score  \\\n",
       "10                            XGBoost (base)  0.936598  0.926018  0.910589   \n",
       "11                  XGBoost (Core + Burnout)  0.936598  0.923953  0.905615   \n",
       "12                      XGBoost with Binning  0.936598  0.924182  0.906165   \n",
       "13  XGBoost with Binning (feature selection)  0.935970  0.926087  0.911648   \n",
       "14    XGBoost with Flags (feature selection)  0.935342  0.926847  0.914391   \n",
       "\n",
       "    Precision  Accuracy   ROC AUC  Num Features            Confusion Matrix  \n",
       "10   0.885986  0.969454  0.986294            18  [[7807, 192], [101, 1492]]  \n",
       "11   0.876616  0.967577  0.983741             7  [[7789, 210], [101, 1492]]  \n",
       "12   0.877647  0.967786  0.985069            26  [[7791, 208], [101, 1492]]  \n",
       "13   0.888558  0.969871  0.984534            14  [[7812, 187], [102, 1491]]  \n",
       "14   0.894358  0.970913  0.983656             9  [[7823, 176], [103, 1490]]  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_xgb_1 = df_xgb[df_xgb[\"Recall\"] >= 0.935]\n",
    "df_xgb_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above the minimum F₂ threshold of 0.85:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F2 Score</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>Num Features</th>\n",
       "      <th>Confusion Matrix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>XGBoost (base)</td>\n",
       "      <td>0.936598</td>\n",
       "      <td>0.926018</td>\n",
       "      <td>0.910589</td>\n",
       "      <td>0.885986</td>\n",
       "      <td>0.969454</td>\n",
       "      <td>0.986294</td>\n",
       "      <td>18</td>\n",
       "      <td>[[7807, 192], [101, 1492]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>XGBoost (Core + Burnout)</td>\n",
       "      <td>0.936598</td>\n",
       "      <td>0.923953</td>\n",
       "      <td>0.905615</td>\n",
       "      <td>0.876616</td>\n",
       "      <td>0.967577</td>\n",
       "      <td>0.983741</td>\n",
       "      <td>7</td>\n",
       "      <td>[[7789, 210], [101, 1492]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>XGBoost with Binning</td>\n",
       "      <td>0.936598</td>\n",
       "      <td>0.924182</td>\n",
       "      <td>0.906165</td>\n",
       "      <td>0.877647</td>\n",
       "      <td>0.967786</td>\n",
       "      <td>0.985069</td>\n",
       "      <td>26</td>\n",
       "      <td>[[7791, 208], [101, 1492]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>XGBoost with Binning (feature selection)</td>\n",
       "      <td>0.935970</td>\n",
       "      <td>0.926087</td>\n",
       "      <td>0.911648</td>\n",
       "      <td>0.888558</td>\n",
       "      <td>0.969871</td>\n",
       "      <td>0.984534</td>\n",
       "      <td>14</td>\n",
       "      <td>[[7812, 187], [102, 1491]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>XGBoost with Flags (feature selection)</td>\n",
       "      <td>0.935342</td>\n",
       "      <td>0.926847</td>\n",
       "      <td>0.914391</td>\n",
       "      <td>0.894358</td>\n",
       "      <td>0.970913</td>\n",
       "      <td>0.983656</td>\n",
       "      <td>9</td>\n",
       "      <td>[[7823, 176], [103, 1490]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Model    Recall  F2 Score  F1 Score  \\\n",
       "10                            XGBoost (base)  0.936598  0.926018  0.910589   \n",
       "11                  XGBoost (Core + Burnout)  0.936598  0.923953  0.905615   \n",
       "12                      XGBoost with Binning  0.936598  0.924182  0.906165   \n",
       "13  XGBoost with Binning (feature selection)  0.935970  0.926087  0.911648   \n",
       "14    XGBoost with Flags (feature selection)  0.935342  0.926847  0.914391   \n",
       "\n",
       "    Precision  Accuracy   ROC AUC  Num Features            Confusion Matrix  \n",
       "10   0.885986  0.969454  0.986294            18  [[7807, 192], [101, 1492]]  \n",
       "11   0.876616  0.967577  0.983741             7  [[7789, 210], [101, 1492]]  \n",
       "12   0.877647  0.967786  0.985069            26  [[7791, 208], [101, 1492]]  \n",
       "13   0.888558  0.969871  0.984534            14  [[7812, 187], [102, 1491]]  \n",
       "14   0.894358  0.970913  0.983656             9  [[7823, 176], [103, 1490]]  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_xgb_2 = df_xgb_1[df_xgb_1[\"F2 Score\"] >= 0.85]\n",
    "df_xgb_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the fewest features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F2 Score</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>Num Features</th>\n",
       "      <th>Confusion Matrix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>XGBoost (Core + Burnout)</td>\n",
       "      <td>0.936598</td>\n",
       "      <td>0.923953</td>\n",
       "      <td>0.905615</td>\n",
       "      <td>0.876616</td>\n",
       "      <td>0.967577</td>\n",
       "      <td>0.983741</td>\n",
       "      <td>7</td>\n",
       "      <td>[[7789, 210], [101, 1492]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Model    Recall  F2 Score  F1 Score  Precision  \\\n",
       "11  XGBoost (Core + Burnout)  0.936598  0.923953  0.905615   0.876616   \n",
       "\n",
       "    Accuracy   ROC AUC  Num Features            Confusion Matrix  \n",
       "11  0.967577  0.983741             7  [[7789, 210], [101, 1492]]  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_xgb_winner = df_xgb_2[df_xgb_2[\"Num Features\"] == df_xgb_2[\"Num Features\"].min()]\n",
    "df_xgb_winner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the winner is \"XGBoost (Core + Burnout)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model: XGBoost (Core + Burnout)...\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Execution time for XGBoost (Core + Burnout): 16.06 seconds\n",
      "Best parameters for XGBoost (Core + Burnout): {'model__subsample': 1.0, 'model__scale_pos_weight': 5.02134337727558, 'model__reg_lambda': 2, 'model__reg_alpha': 1, 'model__n_estimators': 100, 'model__min_child_weight': 5, 'model__max_depth': 3, 'model__learning_rate': 0.2, 'model__gamma': 0.1, 'model__colsample_bytree': 0.6}\n",
      "Best score for XGBoost (Core + Burnout): 0.9366 (recall)\n",
      "Model XGBoost (Core + Burnout) not saved. Set save_model=True to save it.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>recall</th>\n",
       "      <th>f2</th>\n",
       "      <th>f1</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>precision</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>features</th>\n",
       "      <th>best_params</th>\n",
       "      <th>cv_best_score</th>\n",
       "      <th>conf_matrix</th>\n",
       "      <th>search_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGBoost (Core + Burnout)</td>\n",
       "      <td>0.936598</td>\n",
       "      <td>0.923953</td>\n",
       "      <td>0.905615</td>\n",
       "      <td>0.983741</td>\n",
       "      <td>0.876616</td>\n",
       "      <td>0.967577</td>\n",
       "      <td>7</td>\n",
       "      <td>{'model__subsample': 1.0, 'model__scale_pos_we...</td>\n",
       "      <td>0.936592</td>\n",
       "      <td>[[7789, 210], [101, 1492]]</td>\n",
       "      <td>16.056963</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      model    recall        f2        f1   roc_auc  \\\n",
       "3  XGBoost (Core + Burnout)  0.936598  0.923953  0.905615  0.983741   \n",
       "\n",
       "   precision  accuracy  features  \\\n",
       "3   0.876616  0.967577         7   \n",
       "\n",
       "                                         best_params  cv_best_score  \\\n",
       "3  {'model__subsample': 1.0, 'model__scale_pos_we...       0.936592   \n",
       "\n",
       "                  conf_matrix  search_time  \n",
       "3  [[7789, 210], [101, 1492]]    16.056963  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "Model, feature function, and parameter grid definitions for winning tree-based models\n",
    " (used to build pipelines) \n",
    " located at beginning of Decision Tree section \n",
    " '''\n",
    "xgb_config = [\n",
    "    cfg for cfg in rf_xgb_fe2_configs if cfg[\"name\"] == \"XGBoost (Core + Burnout)\"\n",
    "]\n",
    "results_training = run_model_evaluation(\n",
    "    xgb_config,\n",
    "    results_df=results_training,\n",
    "    scoring=scoring,\n",
    "    search_type=\"random\",\n",
    "    n_iter=50,\n",
    ")\n",
    "results_training[results_training[\"model\"] == \"XGBoost (Core + Burnout)\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Dictionary: XGBoost — \"Core + Burnout\" Features\n",
    "\n",
    "This model uses a focused set of original features plus an engineered \"burnout\" flag.\n",
    "\n",
    "| Variable                | Description                                                                                       |\n",
    "|-------------------------|---------------------------------------------------------------------------------------------------|\n",
    "| satisfaction_level      | Employee-reported job satisfaction level [0–1]                                                    |\n",
    "| last_evaluation         | Score of employee's last performance review [0–1]                                                 |\n",
    "| number_project          | Number of projects employee contributes to                                                        |\n",
    "| average_monthly_hours   | Average number of hours employee worked per month                                                 |\n",
    "| tenure                  | How long the employee has been with the company (years)                                           |\n",
    "| promotion_last_5years   | Whether or not the employee was promoted in the last 5 years                                      |\n",
    "| burnout                 | Flag: True if (number_project ≥ 6 or average_monthly_hours ≥ 240) and satisfaction_level ≤ 0.3    |\n",
    "\n",
    "**Notes:**\n",
    "- The \"burnout\" feature is a logical flag engineered to capture high-risk, overworked, and dissatisfied employees.\n",
    "\n",
    "Best hyperparameters for **XGBoost (Core + Burnout)**:\n",
    "\n",
    "```python\n",
    "{\n",
    "    'model__subsample': 1.0,                 # Use all rows for each tree\n",
    "    'model__scale_pos_weight': 5.02134337727558, # Balances positive/negative classes\n",
    "    'model__reg_lambda': 2,                  # L2 regularization term\n",
    "    'model__reg_alpha': 1,                   # L1 regularization term\n",
    "    'model__n_estimators': 100,              # Number of boosting rounds\n",
    "    'model__min_child_weight': 5,            # Minimum sum of instance weight in a child\n",
    "    'model__max_depth': 3,                   # Maximum tree depth\n",
    "    'model__learning_rate': 0.2,             # Step size shrinkage\n",
    "    'model__gamma': 0.1,                     # Minimum loss reduction to make a split\n",
    "    'model__colsample_bytree': 0.6           # Fraction of features for each tree\n",
    "}\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F2 Score</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>Num Features</th>\n",
       "      <th>Confusion Matrix</th>\n",
       "      <th>Best Params</th>\n",
       "      <th>CV Best Score</th>\n",
       "      <th>Search Time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>XGBoost (Core + Burnout)</td>\n",
       "      <td>0.936598</td>\n",
       "      <td>0.923953</td>\n",
       "      <td>0.905615</td>\n",
       "      <td>0.876616</td>\n",
       "      <td>0.967577</td>\n",
       "      <td>0.983741</td>\n",
       "      <td>7</td>\n",
       "      <td>[[7789, 210], [101, 1492]]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>XGBoost (Core + Burnout)</td>\n",
       "      <td>0.936598</td>\n",
       "      <td>0.923953</td>\n",
       "      <td>0.905615</td>\n",
       "      <td>0.876616</td>\n",
       "      <td>0.967577</td>\n",
       "      <td>0.983741</td>\n",
       "      <td>7</td>\n",
       "      <td>[[7789, 210], [101, 1492]]</td>\n",
       "      <td>{'model__subsample': 1.0, 'model__scale_pos_we...</td>\n",
       "      <td>0.936592</td>\n",
       "      <td>16.96148</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Model    Recall  F2 Score  F1 Score  Precision  \\\n",
       "11  XGBoost (Core + Burnout)  0.936598  0.923953  0.905615   0.876616   \n",
       "11  XGBoost (Core + Burnout)  0.936598  0.923953  0.905615   0.876616   \n",
       "\n",
       "    Accuracy   ROC AUC  Num Features            Confusion Matrix  \\\n",
       "11  0.967577  0.983741             7  [[7789, 210], [101, 1492]]   \n",
       "11  0.967577  0.983741             7  [[7789, 210], [101, 1492]]   \n",
       "\n",
       "                                          Best Params  CV Best Score  \\\n",
       "11                                                NaN            NaN   \n",
       "11  {'model__subsample': 1.0, 'model__scale_pos_we...       0.936592   \n",
       "\n",
       "    Search Time (s)  \n",
       "11              NaN  \n",
       "11         16.96148  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confirm = pd.concat(\n",
    "    [\n",
    "        df_xgb_winner,\n",
    "        df_results[\n",
    "            df_results[\"Model\"].str.contains(\"XGBoost (Core + Burnout)\", regex=False)\n",
    "        ],\n",
    "    ]\n",
    ")\n",
    "confirm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>recall</th>\n",
       "      <th>f2</th>\n",
       "      <th>f1</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>precision</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>features</th>\n",
       "      <th>best_params</th>\n",
       "      <th>cv_best_score</th>\n",
       "      <th>conf_matrix</th>\n",
       "      <th>search_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression with Binning (feature sele...</td>\n",
       "      <td>0.939535</td>\n",
       "      <td>0.855104</td>\n",
       "      <td>0.753531</td>\n",
       "      <td>0.947481</td>\n",
       "      <td>0.629004</td>\n",
       "      <td>0.896451</td>\n",
       "      <td>14</td>\n",
       "      <td>{'model__C': 0.1, 'model__class_weight': 'bala...</td>\n",
       "      <td>0.939535</td>\n",
       "      <td>[[6594, 834], [91, 1414]]</td>\n",
       "      <td>6.323299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision Tree (Core + Burnout)</td>\n",
       "      <td>0.943503</td>\n",
       "      <td>0.887040</td>\n",
       "      <td>0.813972</td>\n",
       "      <td>0.956257</td>\n",
       "      <td>0.715714</td>\n",
       "      <td>0.928378</td>\n",
       "      <td>7</td>\n",
       "      <td>{'model__class_weight': 'balanced', 'model__ma...</td>\n",
       "      <td>0.943495</td>\n",
       "      <td>[[7402, 597], [90, 1503]]</td>\n",
       "      <td>3.033708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest (Core + Burnout)</td>\n",
       "      <td>0.940992</td>\n",
       "      <td>0.896317</td>\n",
       "      <td>0.836729</td>\n",
       "      <td>0.976981</td>\n",
       "      <td>0.753266</td>\n",
       "      <td>0.939012</td>\n",
       "      <td>7</td>\n",
       "      <td>{'model__n_estimators': 300, 'model__min_sampl...</td>\n",
       "      <td>0.940991</td>\n",
       "      <td>[[7508, 491], [94, 1499]]</td>\n",
       "      <td>71.203241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGBoost (Core + Burnout)</td>\n",
       "      <td>0.936598</td>\n",
       "      <td>0.923953</td>\n",
       "      <td>0.905615</td>\n",
       "      <td>0.983741</td>\n",
       "      <td>0.876616</td>\n",
       "      <td>0.967577</td>\n",
       "      <td>7</td>\n",
       "      <td>{'model__subsample': 1.0, 'model__scale_pos_we...</td>\n",
       "      <td>0.936592</td>\n",
       "      <td>[[7789, 210], [101, 1492]]</td>\n",
       "      <td>16.056963</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               model    recall        f2  \\\n",
       "0  Logistic Regression with Binning (feature sele...  0.939535  0.855104   \n",
       "1                     Decision Tree (Core + Burnout)  0.943503  0.887040   \n",
       "2                     Random Forest (Core + Burnout)  0.940992  0.896317   \n",
       "3                           XGBoost (Core + Burnout)  0.936598  0.923953   \n",
       "\n",
       "         f1   roc_auc  precision  accuracy  features  \\\n",
       "0  0.753531  0.947481   0.629004  0.896451        14   \n",
       "1  0.813972  0.956257   0.715714  0.928378         7   \n",
       "2  0.836729  0.976981   0.753266  0.939012         7   \n",
       "3  0.905615  0.983741   0.876616  0.967577         7   \n",
       "\n",
       "                                         best_params  cv_best_score  \\\n",
       "0  {'model__C': 0.1, 'model__class_weight': 'bala...       0.939535   \n",
       "1  {'model__class_weight': 'balanced', 'model__ma...       0.943495   \n",
       "2  {'model__n_estimators': 300, 'model__min_sampl...       0.940991   \n",
       "3  {'model__subsample': 1.0, 'model__scale_pos_we...       0.936592   \n",
       "\n",
       "                  conf_matrix  search_time  \n",
       "0   [[6594, 834], [91, 1414]]     6.323299  \n",
       "1   [[7402, 597], [90, 1503]]     3.033708  \n",
       "2   [[7508, 491], [94, 1499]]    71.203241  \n",
       "3  [[7789, 210], [101, 1492]]    16.056963  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_training.to_csv(\n",
    "    \"../results/winning_model_evaluation_results.csv\", index=False\n",
    ")\n",
    "results_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model-evaluation-results\"></a>\n",
    "\n",
    "## **Model Evaluation Results**\n",
    "\n",
    "[Back to top](#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ex8pgn5iNzau"
   },
   "source": [
    "## Recall evaluation metrics\n",
    "\n",
    "- **AUC** is the area under the ROC curve; it's also considered the probability that the model ranks a random positive example more highly than a random negative example.\n",
    "- **Precision** measures the proportion of data points predicted as True that are actually True, in other words, the proportion of positive predictions that are true positives.\n",
    "- **Recall** measures the proportion of data points that are predicted as True, out of all the data points that are actually True. In other words, it measures the proportion of positives that are correctly classified.\n",
    "- **Accuracy** measures the proportion of data points that are correctly classified.\n",
    "- **F1-score** is an aggregation of precision and recall.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aF1C4CBhLXbN"
   },
   "source": [
    "> ### Execute Stage Reflection\n",
    "> \n",
    "> #### What key insights emerged from your model(s)?\n",
    "> - **Satisfaction level** and **workload** (number of projects, monthly hours) are the strongest predictors of attrition.\n",
    "> - Two main at-risk groups: **overworked/burned-out employees** (many projects, long hours, low satisfaction) and **underworked/disengaged employees** (few projects, low satisfaction).\n",
    "> - **Tenure** is important: attrition peaks at 4–5 years, then drops sharply.\n",
    "> - **Salary, department, and recent promotions** have minimal predictive value.\n",
    "> - **Tree-based models (Random Forest, XGBoost)** achieved the best balance of recall, precision, and F1. With feature engineering, **logistic regression** became competitive and highly interpretable.\n",
    "> \n",
    "> #### What business recommendations do you propose based on the models built?\n",
    "> - **Monitor satisfaction and workload:** Regularly survey employees and track workload to identify those at risk of burnout or disengagement.\n",
    "> - **Targeted retention efforts:** Focus on employees with low satisfaction and extreme workloads, especially those at the 4–5 year tenure mark.\n",
    "> - **Promotions and recognition:** Consider more frequent recognition or advancement opportunities.\n",
    "> - **Work-life balance:** Encourage reasonable project loads and monthly hours to reduce burnout risk.\n",
    "> \n",
    "> #### What potential recommendations would you make to your manager/company?\n",
    "> - **Implement early warning systems** using the model to flag at-risk employees for supportive HR outreach.\n",
    "> - **Review workload distribution** and ensure fair, manageable assignments.\n",
    "> - **Conduct stay interviews** with employees approaching 4–5 years of tenure.\n",
    "> - **Communicate transparently** about how predictive models are used, emphasizing support rather than punitive action.\n",
    "> \n",
    "> #### Do you think your model could be improved? Why or why not? How?\n",
    "> - **Feature engineering:** Further refine interaction terms or add time-based features if available.\n",
    "> - **External data:** Incorporate additional data (e.g., engagement surveys, manager ratings, exit interview themes).\n",
    "> - **Model calibration:** Regularly retrain and calibrate the model as new data becomes available.\n",
    "> - **Bias audits:** Routinely check for bias across demographic groups.\n",
    "> \n",
    "> #### Given what you know about the data and the models you were using, what other questions could you address for the team?\n",
    "> - What are the specific reasons for attrition in different departments or roles?\n",
    "> - Are there seasonal or cyclical patterns in attrition?\n",
    "> - How do external factors (e.g., economic conditions, industry trends) affect turnover?\n",
    "> - What interventions are most effective for retaining at-risk employees?\n",
    "> \n",
    "> #### What resources do you find yourself using as you complete this stage? (Make sure to include the links.)\n",
    "> - [pandas documentation](https://pandas.pydata.org/docs/)\n",
    "> - [matplotlib documentation](https://matplotlib.org/stable/users/index.html)\n",
    "> - [seaborn documentation](https://seaborn.pydata.org/)\n",
    "> - [scikit-learn documentation](https://scikit-learn.org/stable/user_guide.html)\n",
    "> - [XGBoost documentation](https://xgboost.readthedocs.io/en/stable/)\n",
    "> - [Kaggle HR Analytics Dataset](https://www.kaggle.com/datasets/mfaisalqureshi/hr-analytics-and-job-prediction?select=HR_comma_sep.csv)\n",
    "> \n",
    "> #### Do you have any ethical considerations in this stage?\n",
    "> - **Data privacy:** Ensure employee data is kept confidential and secure.\n",
    "> - **Fairness:** Avoid using the model to unfairly target or penalize specific groups.\n",
    "> - **Transparency:** Clearly communicate how predictions are generated and used.\n",
    "> - **Supportive use:** Use predictions to offer support and resources, not for punitive measures.\n",
    "> - **Ongoing monitoring:** Regularly audit the model for bias and unintended consequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9MOMqelNLn2v"
   },
   "source": [
    "### Conclusion, Recommendations, Next Steps\n",
    "\n",
    "> ### Conclusion\n",
    "> - **Satisfaction level** and **workload** (number of projects, monthly hours) are the strongest predictors of employee attrition.\n",
    "> - Two main at-risk groups emerged: **overworked/burned-out employees** (many projects, long hours, low satisfaction) and **underworked/disengaged employees** (few projects, low satisfaction).\n",
    "> - **Tenure** is important: attrition peaks at 4–5 years, then drops sharply.\n",
    "> - **Salary, department, and recent promotions** have minimal predictive value.\n",
    "> - **Tree-based models (Random Forest, XGBoost)** achieved the best balance of recall, precision, and F1. With feature engineering, **logistic regression** became competitive and highly interpretable.\n",
    "> \n",
    "> ### Recommendations\n",
    "> - **Monitor satisfaction and workload:** Regularly survey employees and track workload to identify those at risk of burnout or disengagement.\n",
    "> - **Targeted retention efforts:** Focus on employees with low satisfaction and extreme workloads, especially those at the 4–5 year tenure mark.\n",
    "> - **Promotions and recognition:** Consider more frequent recognition or advancement opportunities.\n",
    "> - **Work-life balance:** Encourage reasonable project loads and monthly hours to reduce burnout risk.\n",
    "> - **Implement early warning systems:** Use the model to flag at-risk employees for supportive HR outreach.\n",
    "> - **Review workload distribution:** Ensure fair, manageable assignments.\n",
    "> - **Conduct stay interviews:** Engage employees approaching 4–5 years of tenure.\n",
    "> - **Communicate transparently:** Clearly explain how predictive models are used, emphasizing support rather than punitive action.\n",
    "> \n",
    "> ### Next Steps\n",
    "> - **Model deployment:** Integrate the predictive model into HR processes for early identification of at-risk employees.\n",
    "> - **Continuous improvement:** Regularly retrain and calibrate the model as new data becomes available.\n",
    "> - **Expand data sources:** Incorporate additional data (e.g., engagement surveys, manager ratings, exit interview themes) to improve model accuracy.\n",
    "> - **Bias and fairness audits:** Routinely check for bias across demographic groups and monitor for unintended consequences.\n",
    "> - **Ethical safeguards:** Ensure employee data privacy, fairness, and transparency in all predictive analytics initiatives.\n",
    "> \n",
    "> ---\n",
    "> **Resources Used:**\n",
    "> - [pandas documentation](https://pandas.pydata.org/docs/)\n",
    "> - [matplotlib documentation](https://matplotlib.org/stable/users/index.html)\n",
    "> - [seaborn documentation](https://seaborn.pydata.org/)\n",
    "> - [scikit-learn documentation](https://scikit-learn.org/stable/user_guide.html)\n",
    "> - [XGBoost documentation](https://xgboost.readthedocs.io/en/stable/)\n",
    "> - [Kaggle HR Analytics Dataset](https://www.kaggle.com/datasets/mfaisalqureshi/hr-analytics-and-job-prediction?select=HR_comma_sep.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"appendix-data-dictionary\"></a>\n",
    "\n",
    "# Appendix: Data Dictionary\n",
    "---\n",
    "\n",
    "[Back to top](#)\n",
    "\n",
    "The dataset contains 15,000 rows and 10 columns for the variables listed below. \n",
    "\n",
    "**Note:** For more information about the data, refer to its source on [Kaggle](https://www.kaggle.com/datasets/mfaisalqureshi/hr-analytics-and-job-prediction?select=HR_comma_sep.csv).\n",
    "\n",
    "Variable  |Description |\n",
    "-----|-----|\n",
    "satisfaction_level|Employee-reported job satisfaction level [0&ndash;1]|\n",
    "last_evaluation|Score of employee's last performance review [0&ndash;1]|\n",
    "number_project|Number of projects employee contributes to|\n",
    "average_monthly_hours|Average number of hours employee worked per month|\n",
    "time_spend_company|How long the employee has been with the company (years)\n",
    "Work_accident|Whether or not the employee experienced an accident while at work\n",
    "left|Whether or not the employee left the company\n",
    "promotion_last_5years|Whether or not the employee was promoted in the last 5 years\n",
    "Department|The employee's department\n",
    "salary|The employee's salary (U.S. dollars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering Data Dictionary\n",
    "\n",
    "\n",
    "The following table describes the engineered features created for model development. These features are derived from the original dataset using binning, interaction terms, and logical flags to capture important patterns identified during exploratory data analysis.\n",
    "\n",
    "Variable  |Description |\n",
    "-----|-----|\n",
    "**Bins** | |\n",
    "satisfaction_bin_low | Binary indicator: satisfaction_level is low (≤ 0.4) |\n",
    "satisfaction_bin_medium | Binary indicator: satisfaction_level is medium (> 0.4 and ≤ 0.7) |\n",
    "satisfaction_bin_high | Binary indicator: satisfaction_level is high (> 0.7) |\n",
    "hours_bin_low | Binary indicator: average_monthly_hours is low (≤ 160) |\n",
    "hours_bin_medium | Binary indicator: average_monthly_hours is medium (> 160 and ≤ 240) |\n",
    "hours_bin_high | Binary indicator: average_monthly_hours is high (> 240) |\n",
    "projects_bin_low | Binary indicator: number_project is low (≤ 2) |\n",
    "projects_bin_medium | Binary indicator: number_project is medium (> 2 and ≤ 5) |\n",
    "projects_bin_high | Binary indicator: number_project is high (> 5) |\n",
    "tenure_bin_short | Binary indicator: tenure is short (≤ 3 years) |\n",
    "tenure_bin_mid | Binary indicator: tenure is mid (> 3 and ≤ 5 years) |\n",
    "tenure_bin_long | Binary indicator: tenure is long (> 5 years) |\n",
    "**Interactions** | |\n",
    "satisfaction_x_projects | Interaction: satisfaction_level × number_project |\n",
    "satisfaction_x_hours | Interaction: satisfaction_level × average_monthly_hours |\n",
    "evaluation_x_satisfaction | Interaction: last_evaluation × satisfaction_level |\n",
    "hours_per_project | Ratio: average_monthly_hours divided by number_project |\n",
    "**Flags** | |\n",
    "burnout | Flag: True if (number_project ≥ 6 or average_monthly_hours ≥ 240) and satisfaction_level ≤ 0.3 |\n",
    "disengaged | Flag: True if (number_project ≤ 2 and average_monthly_hours < 160 and satisfaction_level ≤ 0.5) |\n",
    "no_promo_4yr | Flag: True if promotion_last_5years == 0 and tenure ≥ 4 |\n",
    "\n",
    "**Note:**  \n",
    "Binned features are one-hot encoded as separate columns (e.g., satisfaction_bin_low, satisfaction_bin_medium, satisfaction_bin_high). Only the relevant dummy variables (excluding the first category for each bin) are included in the final dataset, depending on the encoding strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href id=\"appendix-feature-engineering\"></a>\n",
    "\n",
    "# Feature Engineering\n",
    "---\n",
    "\n",
    "[Back to Top](#)\n",
    "\n",
    "In addition to baseline models run with all original features, all feature engineering varieties below were tested with all four model types: **Logistic Regression**, **Decision Tree**, **Random Forest**, and **XGBoost**.\n",
    "\n",
    "### Feature Engineering Round One\n",
    "\n",
    "**Original features + binning**:\n",
    "This set adds binned versions of key variables (satisfaction, workload, projects, tenure) to the original features. Binning helps capture non-linear effects and threshold-based risk groups identified in EDA (e.g., low/medium/high satisfaction, extreme workloads, tenure windows).\n",
    "\n",
    "**Original features + interactions**:\n",
    "This set augments the original features with interaction terms that capture relationships between satisfaction, workload, and performance (e.g., satisfaction × projects, satisfaction × hours, evaluation × satisfaction, hours per project). These interactions help distinguish between disengaged, overworked, and healthy employees.\n",
    "\n",
    "**Original features + categorical flags**:\n",
    "This set adds logical flag features to the original data, such as \"burnout\" (overworked and dissatisfied), \"disengaged\" (underworked and dissatisfied), and \"no_promo_4yr\" (long tenure without promotion). These flags directly encode high-risk employee profiles identified in EDA.\n",
    "\n",
    "**Feature selection**:\n",
    "For each of the above, a feature selection variant was also run, dropping weak predictors (department, salary, work_accident) to reduce noise and multicollinearity, especially for logistic regression.\n",
    "\n",
    "### Feature Engineering Round Two\n",
    "\n",
    "**Selected features + burnout flag:**\n",
    "This set isolates the core predictors of attrition (satisfaction, workload, tenure, promotion) and adds a “burnout” flag to capture the high-risk group of overworked, dissatisfied employees\n",
    "\n",
    "**Selected features + interactions:**\n",
    "This set focuses on the main drivers (satisfaction, workload, tenure) and adds interaction terms (satisfaction × projects, hours per project) to capture non-linear effects and workload intensity, which EDA showed are important for distinguishing between underworked, overworked, and healthy employees.\n",
    "\n",
    "**Selected features + interactions + burnout flag:**\n",
    "This feature set combines the core predictors of attrition (satisfaction, workload, tenure) with a “burnout” flag to capture high-risk, overworked employees. It also includes a key interaction term, \"satisfaction × projects\", to distinguish between groups identified in EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href id=\"appendix-hyperparameters\"></a>\n",
    "\n",
    "## Appendix: Hyperparameter Tuning Grids\n",
    "---\n",
    "\n",
    "[Back to Top](#)\n",
    "\n",
    "This section summarizes the hyperparameter grids used for model selection and tuning via `GridSearchCV` and `RandomizedSearchCV` for all baseline and feature-engineered models.\n",
    "\n",
    "---\n",
    "\n",
    "### **Logistic Regression**\n",
    "\n",
    "**Baseline Grid:**\n",
    "```python\n",
    "lr_base_param_grid = {\n",
    "    \"Logistic Regression\": {\n",
    "        \"model__C\": [0.01, 0.1, 1.0, 10.0, 100.0],  # Regularization strength (inverse)\n",
    "        \"model__penalty\": [\"l1\", \"l2\"],             # L1 (Lasso) or L2 (Ridge)\n",
    "        \"model__solver\": [\"liblinear\"],             # Supports L1/L2\n",
    "        \"model__class_weight\": [None, \"balanced\"],  # Handle class imbalance\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**Feature Engineering Grid:**\n",
    "```python\n",
    "lr_fe_params = {\n",
    "    \"model__C\": [0.1, 1.0, 10.0],\n",
    "    \"model__penalty\": [\"l1\", \"l2\"],\n",
    "    \"model__solver\": [\"liblinear\"],\n",
    "    \"model__class_weight\": [None, \"balanced\"],\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Decision Tree**\n",
    "\n",
    "**Baseline Grid:**\n",
    "```python\n",
    "tree_base_param_grids = {\n",
    "    \"Decision Tree\": {\n",
    "        \"model__max_depth\": [4, 6, 8, None],        # Max tree depth\n",
    "        \"model__min_samples_leaf\": [1, 2, 5],       # Min samples at leaf\n",
    "        \"model__min_samples_split\": [2, 4, 6],      # Min samples to split\n",
    "        \"model__class_weight\": [None, \"balanced\"],  # Handle class imbalance\n",
    "    },\n",
    "    ...\n",
    "}\n",
    "```\n",
    "\n",
    "**Feature Engineering Grid:**\n",
    "```python\n",
    "tree_fe_params = {\n",
    "    \"Decision Tree\": {\n",
    "        \"model__max_depth\": [3, 4, 5, 6, 8],\n",
    "        \"model__min_samples_leaf\": [1, 2, 3],\n",
    "        \"model__min_samples_split\": [2, 3, 4],\n",
    "        \"model__class_weight\": [None, \"balanced\"],\n",
    "    },\n",
    "    ...\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Random Forest**\n",
    "\n",
    "**Baseline Grid:**\n",
    "```python\n",
    "tree_base_param_grids = {\n",
    "    \"Random Forest\": {\n",
    "        \"model__n_estimators\": [300, 500],          # Number of trees\n",
    "        \"model__max_depth\": [3, 5, None],           # Max tree depth\n",
    "        \"model__max_features\": [\"sqrt\", 1.0],       # Features considered at each split\n",
    "        \"model__max_samples\": [0.7, 1.0],           # Fraction of samples per tree\n",
    "        \"model__min_samples_leaf\": [1, 2, 3],       # Min samples at leaf\n",
    "        \"model__min_samples_split\": [2, 3, 4],      # Min samples to split\n",
    "        \"model__class_weight\": [None, \"balanced\"],  # Handle class imbalance\n",
    "    },\n",
    "    ...\n",
    "}\n",
    "```\n",
    "\n",
    "**Feature Engineering Grid:**\n",
    "```python\n",
    "tree_fe_params = {\n",
    "    \"Random Forest\": {\n",
    "        \"model__n_estimators\": [100, 300],\n",
    "        \"model__max_depth\": [3, 4, 5, 8],\n",
    "        \"model__max_features\": [\"sqrt\", 1.0],\n",
    "        \"model__max_samples\": [0.7, 1.0],\n",
    "        \"model__min_samples_leaf\": [1, 2],\n",
    "        \"model__min_samples_split\": [2, 3],\n",
    "        \"model__class_weight\": [None, \"balanced\"],\n",
    "    },\n",
    "    ...\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **XGBoost**\n",
    "\n",
    "**Baseline Grid:**\n",
    "```python\n",
    "tree_base_param_grids = {\n",
    "    \"XGBoost\": {\n",
    "        \"model__n_estimators\": [100, 300],              # Number of boosting rounds\n",
    "        \"model__max_depth\": [3, 5, 7],                  # Max tree depth\n",
    "        \"model__learning_rate\": [0.01, 0.1, 0.2],       # Step size shrinkage\n",
    "        \"model__subsample\": [0.6, 0.8, 1.0],            # Row subsampling\n",
    "        \"model__colsample_bytree\": [0.6, 0.8, 1.0],     # Feature subsampling\n",
    "        \"model__min_child_weight\": [1, 5, 10],          # Min sum of instance weight in a child\n",
    "        \"model__gamma\": [0, 0.1, 0.2],                  # Min loss reduction to split\n",
    "        \"model__scale_pos_weight\": [1, scale_pos_weight_value],  # Class imbalance\n",
    "    },\n",
    "    ...\n",
    "}\n",
    "```\n",
    "\n",
    "**Feature Engineering Grid:**\n",
    "```python\n",
    "tree_fe_params = {\n",
    "    \"XGBoost\": {\n",
    "        \"model__n_estimators\": [100, 300],\n",
    "        \"model__max_depth\": [3, 4, 5, 8],\n",
    "        \"model__learning_rate\": [0.1, 0.2],\n",
    "        \"model__subsample\": [0.6, 0.8, 1.0],\n",
    "        \"model__colsample_bytree\": [0.6, 0.8, 1.0],\n",
    "        \"model__min_child_weight\": [1, 5],\n",
    "        \"model__gamma\": [0, 0.1, 0.2],\n",
    "        \"model__scale_pos_weight\": [1, scale_pos_weight_value],\n",
    "        \"model__reg_alpha\": [0, 0.1, 1],            # L1 regularization\n",
    "        \"model__reg_lambda\": [1, 2, 5],             # L2 regularization\n",
    "    },\n",
    "    ...\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Notes:**\n",
    "- All hyperparameters were tuned using either `GridSearchCV` (for smaller grids or faster models) or `RandomizedSearchCV` (for larger grids or slower models).\n",
    "- `scale_pos_weight_value` is dynamically calculated as the ratio of negative to positive samples in the training set to address class imbalance for XGBoost.\n",
    "- For all models, `model__class_weight=\"balanced\"` was included as an option to help mitigate class imbalance.\n",
    "- Feature engineering grids used slightly smaller or more regularized search spaces for efficiency.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<script src=\"https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js\"></script>\n",
    "<script src=\"../static/js/scripts.js\"></script>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
