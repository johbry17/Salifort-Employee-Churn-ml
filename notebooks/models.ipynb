{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n",
    "<link href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css\" rel=\"stylesheet\">\n",
    "<link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css\">\n",
    "<link rel=\"stylesheet\" href=\"../static/css/styles.css\">\n",
    "\n",
    "\n",
    "        \n",
    "<!-- <body> -->\n",
    "<!-- Navigation-->\n",
    "<nav class=\"navbar navbar-expand-lg navbar-light fixed-top\" id=\"mainNav\">\n",
    "    <div class=\"container px-4 px-lg-5\">\n",
    "        <a class=\"navbar-brand\" href=\"../index.html\">Home</a>\n",
    "        <button class=\"navbar-toggler\" type=\"button\" data-bs-toggle=\"collapse\" data-bs-target=\"#navbarResponsive\" aria-controls=\"navbarResponsive\" aria-expanded=\"false\" aria-label=\"Toggle navigation\">\n",
    "            Menu\n",
    "            <i class=\"fas fa-bars\"></i>\n",
    "        </button>\n",
    "        <div class=\"collapse navbar-collapse\" id=\"navbarResponsive\">\n",
    "            <ul class=\"navbar-nav ms-auto py-4 py-lg-0\">\n",
    "                <li class=\"nav-item\"><a class=\"nav-link px-lg-3 py-3 py-lg-4\" href=\"../index.html\">Executive Summary</a></li>\n",
    "                <li class=\"nav-item\"><a class=\"nav-link px-lg-3 py-3 py-lg-4\" href=\"eda.html\">Exploratory Data Analysis</a></li>\n",
    "                <!-- <li class=\"nav-item\"><a class=\"nav-link px-lg-3 py-3 py-lg-4\" href=\"models.html\">Model Construction & Validation</a></li> -->\n",
    "                <li class=\"nav-item\"><a class=\"nav-link px-lg-3 py-3 py-lg-4\" href=\"initial_work.html\">Appendix: Model Development</a></li>\n",
    "            </ul>\n",
    "        </div>\n",
    "    </div>\n",
    "</nav>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ysS5rgTMWpwL"
   },
   "source": [
    "<h2 id=\"title\" style=\"text-align: center; width: 80%;\">Model Construction and Validation</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "- [Description and Deliverables](#description-and-deliverables)\n",
    "- [Data Dictionary](#data-dictionary)\n",
    "- [Exploratory Data Analysis Insights](#insights)\n",
    "- [Modeling Strategies Recap](#modeling-strategies-recap)\n",
    "  - [Cross-Validation Results](#cross-validation-results)\n",
    "- [Model Building](#model-building)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"button\">\n",
    "    <a href=\"models.html\">Back to Part 1<br>Exploratory Data Analysis</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZNZpq0EotHt"
   },
   "source": [
    "<a id=\"description-and-deliverables\"></a>\n",
    "\n",
    "# Description and Deliverables\n",
    "---\n",
    "\n",
    "[Back to top](#)\n",
    "\n",
    "The hypothetical HR department at the fictional Salifort Motors collected employee data to improve satisfaction. They requested data-driven suggestions based on an analysis of this data. The main question is: what factors are likely to make an employee leave the company?\n",
    "\n",
    "The **goal** of this project is to **analyze the data** and build a model to **predict employee attrition**. By identifying which employees are likely to leave, it may be possible to determine the factors contributing to their departure. The model should be interpretable so HR can design targeted interventions to improve retention. Improving retention can reduce the costs associated with hiring and training new employees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stakeholders:**  \n",
    "The primary stakeholder is the Human Resources (HR) department, as they will use the results to inform retention strategies. Secondary stakeholders include C-suite executives who oversee company direction, managers implementing day-to-day retention efforts, employees (whose experiences and outcomes are directly affected), and, indirectly, customers—since employee satisfaction can impact customer satisfaction.\n",
    "\n",
    "**Ethical Considerations:**  \n",
    "- Ensure employee data privacy and confidentiality throughout the analysis.\n",
    "- Avoid introducing or perpetuating bias in model predictions (e.g., not unfairly targeting specific groups).\n",
    "- Maintain transparency in how predictions are generated and how they will be used in HR decision-making.\n",
    "\n",
    "### This page summarizes the first part of the project: exploratory data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"data-dictionary\"></a>\n",
    "\n",
    "# Data Dictionary\n",
    "---\n",
    "\n",
    "[Back to top](#)\n",
    "\n",
    "The dataset contains 15,000 rows and 10 columns for the variables listed below. \n",
    "\n",
    "**Note:** For more information about the data, refer to its source on [Kaggle](https://www.kaggle.com/datasets/mfaisalqureshi/hr-analytics-and-job-prediction?select=HR_comma_sep.csv).\n",
    "\n",
    "Variable  |Description |\n",
    "-----|-----|\n",
    "satisfaction_level|Employee-reported job satisfaction level [0&ndash;1]|\n",
    "last_evaluation|Score of employee's last performance review [0&ndash;1]|\n",
    "number_project|Number of projects employee contributes to|\n",
    "average_monthly_hours|Average number of hours employee worked per month|\n",
    "time_spend_company|How long the employee has been with the company (years)\n",
    "Work_accident|Whether or not the employee experienced an accident while at work\n",
    "left|Whether or not the employee left the company\n",
    "promotion_last_5years|Whether or not the employee was promoted in the last 5 years\n",
    "Department|The employee's department\n",
    "salary|The employee's salary (U.S. dollars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import time\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    StratifiedKFold,\n",
    "    cross_val_predict,\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    fbeta_score,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get initial time, for measuring performance at the end\n",
    "nb_start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset into a dataframe\n",
    "df0 = pd.read_csv(\"../resources/HR_capstone_dataset.csv\")\n",
    "\n",
    "\n",
    "# Display first few rows of the dataframe\n",
    "# df0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns as needed\n",
    "df0.rename(\n",
    "    columns={\n",
    "        \"Department\": \"department\",\n",
    "        \"Work_accident\": \"work_accident\",\n",
    "        \"average_montly_hours\": \"average_monthly_hours\",\n",
    "        \"time_spend_company\": \"tenure\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "\n",
    "# Display all column names after the update\n",
    "# df0.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates and save resulting dataframe in a new variable as needed\n",
    "df = df0.drop_duplicates()\n",
    "\n",
    "\n",
    "# Display first few rows of new dataframe as needed\n",
    "# print(df.info())\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the number of rows containing outliers\n",
    "q1 = df.tenure.quantile(0.25)\n",
    "q3 = df.tenure.quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "upper_bound = q3 + 1.5 * iqr\n",
    "\n",
    "# # Filter the dataframe to find outliers\n",
    "# outliers = df[df.tenure > upper_bound]\n",
    "\n",
    "# # Display the number of outliers\n",
    "# print(f\"Number of tenure outliers: {len(outliers)}\")\n",
    "# print(f\"Outliers percentage of total: {len(outliers) / len(df) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DeTmNVlAANLd"
   },
   "source": [
    "<a id=\"insights\"></a>\n",
    "\n",
    "# Exploratory Data Analysis Insights\n",
    "---\n",
    "\n",
    "[Back to Top](#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQT8YqymD-yL"
   },
   "source": [
    "> The data suggests significant issues with employee retention at this company. Two main groups of leavers emerge:\n",
    ">\n",
    "> - **Underworked and Dissatisfied:** Some employees worked on fewer projects and logged fewer hours than a standard work week, with below-average satisfaction. These individuals may have been disengaged, assigned less work as they prepared to leave, or possibly let go.\n",
    "> - **Overworked and Burned Out:** Another group managed a high number of projects (up to 7) and worked exceptionally long hours (sometimes approaching 80-hour weeks). This group reported very low satisfaction and received few, if any, promotions.\n",
    ">\n",
    "> Most employees work well above a typical 40-hour work week (160–184 hours/month, 20-23 work days/month), indicating a culture of overwork. The lack of promotions and high workload likely contribute to dissatisfaction and attrition.\n",
    ">\n",
    "> **Employee evaluation scores** show only a weak relationship with attrition; both leavers and stayers have similar performance reviews. High-performing employees are not necessarily retained, especially if they are overworked or dissatisfied.\n",
    ">\n",
    "> Other variables—such as department, salary, and work accidents—do not show strong predictive value for employee churn compared to satisfaction and workload.\n",
    ">\n",
    "> Overall, the data points to management and workload issues as primary drivers of employee turnover."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"modeling-strategies-recap\"></a>\n",
    "\n",
    "# Modeling Strategies Recap\n",
    "---\n",
    "\n",
    "[Back to top](#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "moS7voiMOW67"
   },
   "source": [
    "> **Observations from Baseline Model Building:**  \n",
    "> - **Logistic Regression performed much worse** than tree-based models (recall: 0.24 vs. >0.90 for others). This suggests the relationship between features and attrition is highly non-linear, or that important interactions are not captured by a linear model.\n",
    "> - **Tree-based models (Decision Tree, Random Forest, XGBoost) all performed very well** (recall >0.90, AUC >0.97), with XGBoost slightly ahead. Surprisingly strong for a shallow Decision Tree (max depth 4). This may indicate the data is either easy to separate or possibly a bit too “clean” (the dataset is synthetic).\n",
    "> - **Confusion matrices show very few false negatives** for tree-based models, but Logistic Regression misses many true leavers.\n",
    ">\n",
    "> **Independent Variables Chosen:**  \n",
    "> - All available features were included: satisfaction_level, last_evaluation, number_project, average_monthly_hours, tenure, work_accident, promotion_last_5years, salary (ordinal), and department (one-hot encoded).\n",
    "> - This approach ensures the model can capture all possible relationships, especially since EDA showed satisfaction, workload, and tenure are strong predictors.\n",
    ">\n",
    "> **Model Assumptions Met:**  \n",
    "> - **Logistic Regression:** Outliers were removed and features were scaled. Outcome is categorical and observations are independent (dropped duplicates). Sample size is ample. Multicollinearity was checked in heatmap at end of EDA. The poor performance suggests the linearity assumption is not met.\n",
    "> - **Tree-based models:** No strong assumptions about feature scaling, linearity, or multicollinearity; these models are robust to the data structure provided.\n",
    ">\n",
    "> **Model Fit:**  \n",
    "> - **Tree-based models fit the data extremely well** (recall, precision, and AUC all very high). This suggests strong predictive power, but also raises the possibility of overfitting.\n",
    "> - **Logistic Regression fits poorly**, missing most true positives.\n",
    ">\n",
    "> **Potential Improvements:**  \n",
    "> - **Feature engineering:** (Will do.) Create interaction terms or non-linear transformations (e.g., satisfaction × workload, tenure bins) to help linear models like Logistic Regression capture more complex relationships. Consider feature selection to remove redundant or less informative variables.\n",
    "> - **Interpretability:** (Will do.) Use feature importance plots for tree-based models and SHAP values to explain individual predictions and overall model behavior. This will help stakeholders understand which factors drive attrition risk.\n",
    "> - **Model validation:** (Done.) Rigorously check for data leakage by reviewing the entire data pipeline, ensuring all preprocessing steps are performed only on training data within cross-validation folds.\n",
    "> - **Class imbalance:** (Might do.) Although recall is high, further address class imbalance by experimenting with resampling techniques (e.g., SMOTE, undersampling) or adjusting class weights, especially if the business wants to minimize false negatives.\n",
    "> - **Alternative Models:** (Won't do anytime soon.) Try other algorithms (e.g., LightGBM, SVM, or neural networks) or ensemble approaches to see if performance or interpretability can be improved.\n",
    "> - **Time series data** (Don't have it.) If this was real-world data, it would be nice to track changes over time in work satisfaction, performance reviews, workload, promotions, absences, etc.\n",
    ">\n",
    "> **Ethical Considerations:**  \n",
    "> - Ensure predictions are used to support employees (e.g., for retention efforts), not for punitive actions.\n",
    "> - Ensure the model does not unfairly target or disadvantage specific groups (e.g., by department, salary, or tenure).\n",
    "> - Clearly communicate how predictions are made and how they will be used by HR.\n",
    "> - Protect employee data and avoid using sensitive or personally identifiable information.\n",
    "> - Regularly audit the model for bias and unintended consequences after deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cross-validation-results)\"></a>\n",
    "\n",
    "## Cross-Validation Results\n",
    "\n",
    "\n",
    "[Back to top](#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display base model results df\n",
    "# df_base = pd.read_csv(\"../results/base_model_evaluation_results.csv\")\n",
    "# df_base[[\"model\", \"recall\", \"precision\", \"f1\", \"accuracy\", \"roc_auc\", \"features\", \"search_time\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show confusion matrix exemplar\n",
    "display(Image(filename=\"../resources/images/confusion_matrix_exemplar.png\", width=800))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display confusion matrix for base model\n",
    "display(Image(filename=\"../results/images/base_model_confusion_matrices_confusion_grid.png\", width=800))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display all model evaluation results\n",
    "df_results = pd.read_csv(\"../results/all_model_evaluation_results.csv\") \n",
    "df_results = df_results.rename(\n",
    "    columns={\n",
    "        \"model\": \"Model\",\n",
    "        \"recall\": \"Recall\",\n",
    "        \"f1\": \"F1 Score\",\n",
    "        \"roc_auc\": \"ROC AUC\",\n",
    "        \"precision\": \"Precision\",\n",
    "        \"accuracy\": \"Accuracy\",\n",
    "        \"features\": \"Num Features\",\n",
    "        \"best_params\": \"Best Params\",\n",
    "        \"cv_best_score\": \"CV Best Score\",\n",
    "        \"conf_matrix\": \"Confusion Matrix\",\n",
    "        \"search_time\": \"Search Time (s)\",\n",
    "    }\n",
    ")\n",
    "df_results[[\"Model\", \"Recall\", \"Precision\", \"F1 Score\", \"Accuracy\", \"ROC AUC\", \"Num Features\", \"Confusion Matrix\", \"Search Time (s)\"]]\n",
    "# df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display confusion matrix for all models, top 9\n",
    "display(Image(filename=\"../results/images/top_model_confusion_matrices_confusion_grid.png\", width=800))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZDG9v-NCS69j"
   },
   "source": [
    "<a id=\"model-building\"></a>\n",
    "\n",
    "## Model Building\n",
    "---\n",
    "\n",
    "[Back to top](#)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plan\n",
    "\n",
    "one section for each type of model (log reg, dt, rf, xgb)\n",
    "\n",
    "-\n",
    "\n",
    "-\n",
    "\n",
    "show the df of results for versions of that type of model\n",
    "\n",
    "give a rationale for why i'm choosing which version of the model (show confusion matrix)\n",
    "\n",
    "run it through the cross-validation model evaluation function again and save the model, using the exact same Pipeline and random_state\n",
    "\n",
    "train the saved model on all of X_train (save it again)\n",
    "\n",
    "run the model on X_test\n",
    "\n",
    "compare results of training and testing confusion matrices, plot feature importance, pr-roc, etc.\n",
    "\n",
    "interpret the model, how it makes decisions, business implications\n",
    "\n",
    "-\n",
    "\n",
    "-\n",
    "\n",
    "summarize all four versions at the end, with summary table / bullet points comparing recall, precision, F1, and interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Choose evaluation metric**\n",
    "\n",
    "While **ROC AUC** is a common metric for evaluating binary classifiers—offering a threshold-independent measure of how well the model distinguishes between classes—it is **not ideal for imbalanced problems like employee churn**, where the positive class (those likely to leave) is much smaller and more critical to identify.\n",
    "\n",
    "During model development, I did review ROC AUC to get a general sense of model discrimination. However, for **model selection and tuning**, I ultimately prioritized **recall**. A high recall ensures that we identify as many at-risk employees as possible, aligning with the company's goal to support retention through early intervention. Missing a potential churner (a false negative) is generally more costly than mistakenly flagging someone who is not at risk (a false positive), especially when interventions are supportive rather than punitive.\n",
    "\n",
    "While precision is also important—since too many false positives could dilute resources or create unnecessary concern—recall is more aligned with a **proactive retention strategy**. This tradeoff assumes that HR interventions are constructive and that the company has systems in place to act ethically on model outputs.\n",
    "\n",
    "To avoid unintended harm, I recommend implementing **clear usage guidelines** and **transparency** measures, ensuring that predictions are used to help employees, not penalize them. Calibration and regular fairness audits should accompany any deployment of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Evaluation Tie Breaker**\n",
    "\n",
    " One final twist (I hope). I made the classic mistake of not clearly and rigidly defining success, and I now have a bunch of models that are all excellent at recall, hovering in the 0.93-0.96 range. So I'm making a post-hoc call. At least this one, I'm planning ahead of time. The best model (of each type) will be chosen based on the following tie-breakers (in order):\n",
    "- recall > 0.935\n",
    "- f2 > 0.85 (f2 is a new score, weighing recall at 80%, and precision at 20%)\n",
    "- fewest number of features\n",
    "- highest f2\n",
    "- highest precision\n",
    "\n",
    "I should hope i can make a choice by then. there can't be that many models. I... hehehe... predict... that I'll have it by number three, fewest number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set evaluation metric\n",
    "scoring = \"recall\"\n",
    "\n",
    "\n",
    "# for XGBoost eval_metric\n",
    "def get_xgb_eval_metric(scoring):\n",
    "    mapping = {\n",
    "        \"roc_auc\": \"auc\",  # area under ROC curve\n",
    "        \"accuracy\": \"error\",  # classification error rate\n",
    "        \"f1\": \"logloss\",  # logarithmic loss (not F1, but closest available)\n",
    "        \"precision\": \"logloss\",  # no direct precision metric, logloss is a common fallback\n",
    "        \"recall\": \"logloss\",  # no direct recall metric, logloss is a common fallback\n",
    "    }\n",
    "    return mapping.get(scoring, \"auc\")  # default to 'auc' if not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode categorical variables\n",
    "\n",
    "\n",
    "# copy the dataframe to avoid modifying the original\n",
    "df_enc = df.copy()\n",
    "\n",
    "# encode salary as ordinal\n",
    "df_enc[\"salary\"] = df_enc[\"salary\"].map({\"low\": 0, \"medium\": 1, \"high\": 2})\n",
    "\n",
    "# encode department as dummies\n",
    "df_enc = pd.get_dummies(df_enc, columns=[\"department\"])\n",
    "\n",
    "# confirm the changes\n",
    "# print(\"Original salary values:\\n\", df[\"salary\"].value_counts())\n",
    "# print(\"\\nEncoded salary values:\\n\", df_enc[\"salary\"].value_counts())\n",
    "# df_enc.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train / test sets for different models\n",
    "\n",
    "# One set for tree-based models (decision tree, random forest, XGBoost)\n",
    "# Another set for logistic regression (which must have outliers removed and data normalized)\n",
    "# Stratify the target variable each time to account for class imbalance.\n",
    "\n",
    "\n",
    "\n",
    "# split the data into features and target variable for tree-based models\n",
    "X = df_enc.drop(columns=[\"left\"])\n",
    "y = df_enc[\"left\"]\n",
    "\n",
    "# split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# scale_pos_weight for XGBoost (ratio of negative to positive class in training set)\n",
    "scale_pos_weight_value = (y_train == 0).sum() / (y_train == 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into features and target variable for logistic regression\n",
    "# remove outliers from tenure for logistic regression\n",
    "df_enc_lr = df_enc.copy()\n",
    "\n",
    "\"\"\"\n",
    "outliers defined waaaaaay up above, \n",
    "at end of inital data exploration and cleaning\n",
    "code not needed here, but copied for reference\n",
    "\"\"\"\n",
    "# q1 = df.tenure.quantile(0.25)\n",
    "# q3 = df.tenure.quantile(0.75)\n",
    "# iqr = q3 - q1\n",
    "# upper_bound = q3 + 1.5 * iqr\n",
    "\n",
    "# remove outliers\n",
    "df_enc_lr = df_enc_lr[df_enc_lr.tenure < upper_bound]\n",
    "\n",
    "X_lr = df_enc_lr.drop(columns=[\"left\"])\n",
    "y_lr = df_enc_lr[\"left\"]\n",
    "\n",
    "# split the data into training and testing sets for logistic regression\n",
    "X_train_lr, X_test_lr, y_train_lr, y_test_lr = train_test_split(\n",
    "    X_lr, y_lr, test_size=0.2, random_state=42, stratify=y_lr\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### **Functions to make models, run models, plot confusion matrices and feature importances**\n",
    "\n",
    "# build models_config for run_model_evaluation\n",
    "\n",
    "def make_models_config(\n",
    "    models,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    feature_func=None,  # can be a function, list, dict, or None\n",
    "    param_grids=None,\n",
    "    scaler=None,\n",
    "    name_suffix=\"\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Build models_config for run_model_evaluation.\n",
    "    - models: dict of {name: estimator}\n",
    "    - X_train, y_train: training data\n",
    "    - feature_func: function, list of functions, dict of {name: func}, or None\n",
    "    - param_grids: dict of {name: param_grid} (or None for empty)\n",
    "    - scaler: sklearn transformer (e.g., StandardScaler) or None\n",
    "    - name_suffix: string to append to model name\n",
    "    \"\"\"\n",
    "    configs = []\n",
    "    for name, model in models.items():\n",
    "        # order of steps matters, features first, then scaler, then model\n",
    "        steps = []\n",
    "\n",
    "        # determine which feature_func to use for this model\n",
    "        func = None\n",
    "        if isinstance(feature_func, dict):  # dict of {name: func}\n",
    "            func = feature_func.get(name)\n",
    "        elif callable(feature_func) or isinstance(feature_func, list):\n",
    "            func = feature_func\n",
    "        # handles a list of feature functions (apply in sequence), or a single function\n",
    "        if func is not None:\n",
    "            if isinstance(func, list):\n",
    "                for i, f in enumerate(func):\n",
    "                    steps.append((f\"features_{i+1}\", FunctionTransformer(f)))\n",
    "            else:\n",
    "                steps.append((\"features\", FunctionTransformer(func)))\n",
    "\n",
    "        # add scaler if provided\n",
    "        if scaler is not None:\n",
    "            steps.append((\"scaler\", scaler))\n",
    "\n",
    "        # add model\n",
    "        steps.append((\"model\", model))\n",
    "\n",
    "        # create the pipeline\n",
    "        pipe = Pipeline(steps)\n",
    "\n",
    "        # add parameter grid if provided\n",
    "        param_grid = {}\n",
    "        if isinstance(param_grids, dict):\n",
    "            param_grid = param_grids.get(name, {})\n",
    "\n",
    "        # add model configuration to the list\n",
    "        configs.append(\n",
    "            {\n",
    "                \"name\": f\"{name}{name_suffix}\",\n",
    "                \"X_train\": X_train,\n",
    "                \"y_train\": y_train,\n",
    "                \"pipeline\": pipe,\n",
    "                \"param_grid\": param_grid,\n",
    "            }\n",
    "        )\n",
    "    return configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run model evaluation function\n",
    "\n",
    "\n",
    "def run_model_evaluation(\n",
    "    models_config,\n",
    "    results_df=None,\n",
    "    scoring=\"recall\",\n",
    "    save_model=False,\n",
    "    search_type=\"grid\",\n",
    "    n_iter=20,\n",
    "):\n",
    "    \"\"\"\n",
    "    Run model training and evaluation for a list of model configurations using cross-validated hyperparameter search.\n",
    "\n",
    "    For each model configuration, performs hyperparameter tuning (GridSearchCV or RandomizedSearchCV),\n",
    "    fits the best pipeline, evaluates cross-validated performance metrics, and optionally saves the best model.\n",
    "\n",
    "    Parameters:\n",
    "        models_config (list of dict): List of model configurations, each containing:\n",
    "            - 'name': Model name (str)\n",
    "            - 'X_train': Training features (pd.DataFrame or np.ndarray)\n",
    "            - 'y_train': Training labels (pd.Series or np.ndarray)\n",
    "            - 'pipeline': sklearn Pipeline object\n",
    "            - 'param_grid': dict of hyperparameters for search\n",
    "        results_df (pd.DataFrame or None): Existing results DataFrame to append to, or None to create a new one.\n",
    "        scoring (str): Scoring metric for model selection (e.g., 'recall', 'accuracy', 'roc_auc').\n",
    "        save_model (bool): If True, saves the best model pipeline to disk for each configuration.\n",
    "        search_type (str): 'grid' for GridSearchCV, 'random' for RandomizedSearchCV.\n",
    "        n_iter (int): Number of parameter settings sampled for RandomizedSearchCV (ignored for grid search).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Results DataFrame with model name, metrics (recall, f1, roc_auc, precision, accuracy),\n",
    "                      number of features, best hyperparameters, best CV score, confusion matrix, and search time.\n",
    "\n",
    "    Notes:\n",
    "        - Uses stratified 5-fold cross-validation for both hyperparameter search and out-of-fold predictions.\n",
    "        - Calculates metrics on cross-validated predictions for robust performance estimates.\n",
    "        - Handles models that do not support predict_proba for ROC AUC gracefully.\n",
    "        - Saves models to '../results/saved_models/' if save_model=True.\n",
    "    \"\"\"\n",
    "    if results_df is None:\n",
    "        results_df = pd.DataFrame(\n",
    "            columns=[\n",
    "                \"model\",\n",
    "                \"recall\",\n",
    "                \"f2\",  # 80% recall, 20% precision (metric created to weigh recall more heavily)\n",
    "                \"f1\", # 50% recall, 50% precision\n",
    "                \"roc_auc\",\n",
    "                \"precision\",\n",
    "                \"accuracy\",\n",
    "                \"features\",\n",
    "                \"best_params\",\n",
    "                \"cv_best_score\",\n",
    "                \"conf_matrix\",\n",
    "                \"search_time\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    # ensure cross-validation is stratified for balanced class distribution\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    for cfg in models_config:\n",
    "        # time the model training and evaluation\n",
    "        start_time = time.time()\n",
    "        print(f\"Running model: {cfg['name']}...\")\n",
    "\n",
    "        # conditional to choose search type, instantiate the appropriate search class\n",
    "        if search_type == \"random\":\n",
    "            grid = RandomizedSearchCV(\n",
    "                cfg[\"pipeline\"],\n",
    "                cfg[\"param_grid\"],\n",
    "                n_iter=n_iter,\n",
    "                cv=cv,\n",
    "                scoring=scoring,\n",
    "                n_jobs=-1,\n",
    "                verbose=2,\n",
    "                random_state=42,\n",
    "            )\n",
    "        else:\n",
    "            grid = GridSearchCV(\n",
    "                cfg[\"pipeline\"],\n",
    "                cfg[\"param_grid\"],\n",
    "                cv=cv,\n",
    "                scoring=scoring,\n",
    "                n_jobs=-1,\n",
    "                verbose=2,\n",
    "            )\n",
    "\n",
    "        # fit the grid search to the training data\n",
    "        grid.fit(cfg[\"X_train\"], cfg[\"y_train\"])\n",
    "\n",
    "        # print the execution time\n",
    "        end_time = time.time()\n",
    "        search_time = end_time - start_time\n",
    "        print(f\"Execution time for {cfg['name']}: {search_time:.2f} seconds\")\n",
    "\n",
    "        # get the best model and its parameters\n",
    "        best_model = grid.best_estimator_\n",
    "        print(f\"Best parameters for {cfg['name']}: {grid.best_params_}\")\n",
    "        print(f\"Best score for {cfg['name']}: {grid.best_score_:.4f} ({scoring})\")\n",
    "\n",
    "        # --- get the number of features after all pipeline steps ---\n",
    "        # try to transform X_train through all steps except the final estimator\n",
    "        try:\n",
    "            if hasattr(best_model, \"named_steps\"):\n",
    "                # Remove the final estimator step\n",
    "                steps = list(best_model.named_steps.items())\n",
    "                if len(steps) > 1:\n",
    "                    # Remove last step (the model)\n",
    "                    feature_pipeline = Pipeline(steps[:-1])\n",
    "                    X_transformed = feature_pipeline.transform(cfg[\"X_train\"])\n",
    "                    n_features = X_transformed.shape[1]\n",
    "                else:\n",
    "                    n_features = cfg[\"X_train\"].shape[1]\n",
    "            else:\n",
    "                n_features = cfg[\"X_train\"].shape[1]\n",
    "        except Exception as e:\n",
    "            print(f\"Could not determine number of features: {e}\")\n",
    "            n_features = cfg[\"X_train\"].shape[1]\n",
    "\n",
    "        # conditional to save the best model\n",
    "        if save_model:\n",
    "            model_path = f\"../results/saved_models/{cfg['name'].replace(' ', '_').lower()}.joblib\"\n",
    "            joblib.dump(best_model, model_path)\n",
    "            print(f\"Model {cfg['name']} saved successfully.\\n\")\n",
    "        else:\n",
    "            print(f\"Model {cfg['name']} not saved. Set save_model=True to save it.\\n\")\n",
    "\n",
    "        # make predictions using cross-validation to generate out-of-fold predictions for each training sample\n",
    "        # translation:\n",
    "        # substitute for setting aside a validation set\n",
    "        # takes more time, but provides better estimates of model performance\n",
    "        # it makes a prediction for each sample in the training set, using a different fold of the data for each prediction...\n",
    "        # ...the fold where the sample is not included in the 80% training set (the sample is in the 20%)\n",
    "        y_pred = cross_val_predict(\n",
    "            best_model, cfg[\"X_train\"], cfg[\"y_train\"], cv=cv, n_jobs=-1\n",
    "        )\n",
    "\n",
    "        # # check misclassified cases for further analysis\n",
    "        # print(f\"Misclassified cases for {cfg['name']}:\")\n",
    "        # misclassified = cfg['X_train'].copy()\n",
    "        # misclassified['actual'] = cfg[\"y_train\"]\n",
    "        # misclassified['predicted'] = y_pred\n",
    "        # misclassified = misclassified[misclassified['actual'] != misclassified['predicted']]\n",
    "\n",
    "        # # Show counts of each type of misclassification\n",
    "        # counts = misclassified.groupby(['actual', 'predicted']).size().rename('count')\n",
    "        # print(\"\\nMisclassification counts:\")\n",
    "        # print(counts)\n",
    "        # print()\n",
    "\n",
    "        # # Show .describe() for each group, side by side\n",
    "        # pd.set_option('display.max_columns', None)\n",
    "        # for (actual, predicted), group in misclassified.groupby(['actual', 'predicted']):\n",
    "        #     label_map = {0: \"Stayed\", 1: \"Left\"}\n",
    "        #     print(f\"--- Misclassified: Actual={label_map.get(actual, actual)}, Predicted={label_map.get(predicted, predicted)} (n={len(group)}) ---\")\n",
    "        #     print(group.describe().T)\n",
    "        #     print()\n",
    "        # pd.reset_option('display.max_columns')\n",
    "        # print(\"\\n\")\n",
    "\n",
    "        # calculate the ROC AUC score, need predicted probabilities (not just class labels, but confidence in those labels)\n",
    "        # try / except block to handle models that do not support predict_proba (e.g., SVC)\n",
    "        try:\n",
    "            y_proba = cross_val_predict(\n",
    "                best_model,\n",
    "                cfg[\"X_train\"],\n",
    "                cfg[\"y_train\"],\n",
    "                cv=cv,\n",
    "                method=\"predict_proba\",\n",
    "                n_jobs=-1,\n",
    "            )\n",
    "            roc_auc = roc_auc_score(cfg[\"y_train\"], y_proba[:, 1])\n",
    "        except (AttributeError, ValueError):\n",
    "            roc_auc = np.nan\n",
    "            print(f\"Model {cfg['name']} does not support predict_proba.\")\n",
    "\n",
    "        # save results in the results dataframe\n",
    "        results_df.loc[len(results_df)] = {\n",
    "            \"model\": cfg[\"name\"],\n",
    "            \"features\": n_features,\n",
    "            \"accuracy\": accuracy_score(cfg[\"y_train\"], y_pred),\n",
    "            \"precision\": precision_score(cfg[\"y_train\"], y_pred),\n",
    "            \"recall\": recall_score(cfg[\"y_train\"], y_pred),\n",
    "            \"f1\": f1_score(cfg[\"y_train\"], y_pred),\n",
    "            \"f2\": fbeta_score(cfg[\"y_train\"], y_pred, beta=2), # 80% recall, 20% precision (ratio is \"beta squared : 1\", b^2:1, 2^2:1, 4:1)\n",
    "            \"roc_auc\": roc_auc,\n",
    "            \"conf_matrix\": confusion_matrix(cfg[\"y_train\"], y_pred).tolist(),\n",
    "            \"best_params\": grid.best_params_,\n",
    "            \"cv_best_score\": grid.best_score_,\n",
    "            \"search_time\": search_time,\n",
    "        }\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot confusion matrices from dataframe\n",
    "\n",
    "def plot_confusion_from_results(results_df, save_png=False):\n",
    "    \"\"\"Plots SINGLE confusion matrices from results dataframe and optionally saves png.\"\"\"\n",
    "\n",
    "    class_labels = [\"Stayed\", \"Left\"]\n",
    "\n",
    "    for idx, row in results_df.iterrows():\n",
    "        cm = row[\"conf_matrix\"]\n",
    "        model_name = row[\"model\"]\n",
    "\n",
    "        plt.figure(figsize=(5, 4))\n",
    "        sns.heatmap(\n",
    "            cm,\n",
    "            annot=True,\n",
    "            fmt=\"d\",\n",
    "            cmap=\"Blues\",\n",
    "            cbar=False,\n",
    "            xticklabels=class_labels,\n",
    "            yticklabels=class_labels,\n",
    "        )\n",
    "        plt.title(f\"Confusion Matrix: {model_name}\")\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"Actual\")\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # conditional to save the confusion matrix as a PNG file\n",
    "        if save_png:\n",
    "            plt.savefig(\n",
    "                f\"../results/images/{model_name.replace(' ', '_').lower()}_confusion_matrix.png\"\n",
    "            )\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot confusion matrix grid from dataframe\n",
    "\n",
    "def plot_confusion_grid_from_results(results_df, png_title=None):\n",
    "    \"\"\"Plots ALL confusion matrices from results_df IN A GRID and optionally saves png.\"\"\"\n",
    "    import math\n",
    "\n",
    "    class_labels = [\"Stayed\", \"Left\"]\n",
    "    n_models = len(results_df)\n",
    "    n_cols = 2 if n_models <= 4 else 3\n",
    "    n_rows = math.ceil(n_models / n_cols)\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 4 * n_rows))\n",
    "    axes = axes.flatten() if n_models > 1 else [axes]\n",
    "\n",
    "    for idx, (i, row) in enumerate(results_df.iterrows()):\n",
    "        cm = row[\"conf_matrix\"]\n",
    "        model_name = row[\"model\"]\n",
    "        ax = axes[idx]\n",
    "        sns.heatmap(\n",
    "            cm,\n",
    "            annot=True,\n",
    "            fmt=\"d\",\n",
    "            cmap=\"Blues\",\n",
    "            cbar=False,\n",
    "            xticklabels=class_labels,\n",
    "            yticklabels=class_labels,\n",
    "            ax=ax,\n",
    "        )\n",
    "        ax.set_title(f\"{model_name}\")\n",
    "        ax.set_xlabel(\"Predicted\")\n",
    "        ax.set_ylabel(\"Actual\")\n",
    "\n",
    "    # hide any unused subplots\n",
    "    for j in range(idx + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # conditional to save the confusion grid as a PNG file\n",
    "    if png_title:\n",
    "        plt.suptitle(png_title, fontsize=16, y=1.02)\n",
    "        plt.savefig(\n",
    "            f\"../results/images/{png_title.replace(' ', '_').lower()}_confusion_grid.png\"\n",
    "        )\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot feature importances function\n",
    "\n",
    "def load_and_plot_feature_importance(\n",
    "    file_name, model_name, feature_names, top_n=10, save_png=False\n",
    "):\n",
    "    \"\"\"Load a model and plot its feature importance, optionally saves png.\"\"\"\n",
    "\n",
    "    # load model\n",
    "    model_path = os.path.join(\"../results/saved_models\", file_name)\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "    # if model is a pipeline, get the estimator\n",
    "    if hasattr(model, \"named_steps\"):\n",
    "        # for logistic regression, get the scaler's feature names if available\n",
    "        # NOTE: StandardScaler does not change feature names, so X_train_lr.columns is correct here\n",
    "        # if using a transformer that changes the feature set (e.g., OneHotEncoder, ColumnTransformer)...\n",
    "        # ...one would need to extract the transformed feature names from the transformer\n",
    "        estimator = model.named_steps[\"model\"]\n",
    "    # if model is not a pipeline, use it directly (irrelevant for this case, but included for future-proofing)\n",
    "    else:\n",
    "        estimator = model\n",
    "\n",
    "    # get importances\n",
    "    # for tree-based models, use feature_importances_ or coef_ for logistic regression\n",
    "    if hasattr(estimator, \"feature_importances_\"):\n",
    "        importances = estimator.feature_importances_\n",
    "        title = \"Feature Importance\"\n",
    "    elif hasattr(estimator, \"coef_\"):\n",
    "        importances = np.abs(estimator.coef_[0])\n",
    "        title = \"Absolute Coefficient Magnitude\"\n",
    "    else:\n",
    "        print(f\"Model {model_name} does not support feature importance.\")\n",
    "        return\n",
    "\n",
    "    # sort and select top N\n",
    "    indices = np.argsort(importances)[::-1][:top_n]\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.barh(np.array(feature_names)[indices][::-1], importances[indices][::-1])\n",
    "    plt.xlabel(title)\n",
    "    plt.title(f\"{model_name}: Top {top_n} Features\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # conditional to save the feature importance plot as a PNG file\n",
    "    if save_png:\n",
    "        plt.savefig(\n",
    "            f\"../results/images/{model_name.replace(' ', '_').lower()}_feature_importance.png\"\n",
    "        )\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"logistic regression\"></a>\n",
    "\n",
    "## Logistic Regression\n",
    "\n",
    "[Back to top](#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display model evaluation results for all logistic regression models\n",
    "df_lr = df_results[df_results[\"Model\"].str.contains(\"Logistic Regression\")]\n",
    "if df_lr.empty:\n",
    "    print(\"No logistic regression models found in results.\")\n",
    "\n",
    "# print(\"Logistic Regression Model Evaluation Results:\")\n",
    "df_lr[[\"Model\", \"Recall\", \"Precision\", \"F1 Score\", \"Accuracy\", \"ROC AUC\", \"Num Features\", \"Confusion Matrix\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define logistic regression base model and its parameter grid\n",
    "lr_base_model = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42)\n",
    "}\n",
    "lr_base_param_grid = {\n",
    "    \"Logistic Regression\": {\n",
    "        \"model__C\": [\n",
    "            0.01,\n",
    "            0.1,\n",
    "            1.0,\n",
    "            10.0,\n",
    "            100.0,\n",
    "        ],  # regularization strength (inverse): smaller = stronger regularization\n",
    "        \"model__penalty\": [\"l1\", \"l2\"],  # regularization type (L1 = Lasso, L2 = Ridge)\n",
    "        \"model__solver\": [\n",
    "            \"liblinear\"\n",
    "        ],  # optimization algorithm (liblinear supports L1/L2)\n",
    "        \"model__class_weight\": [\n",
    "            None,\n",
    "            \"balanced\",\n",
    "        ],  # handle class imbalance (None = no adjustment, balanced = adjust weights inversely proportional to class frequencies)\n",
    "    }\n",
    "}\n",
    "\n",
    "# create models_config for base logistic regression model\n",
    "lr_base_config = make_models_config(\n",
    "    models=lr_base_model,\n",
    "    X_train=X_train_lr,  # training features for logistic regression (outliers removed, to be scaled)\n",
    "    y_train=y_train_lr,  # training target labels for logistic regression (outliers removed)\n",
    "    feature_func=None,  # no additional features for base model\n",
    "    param_grids=lr_base_param_grid,\n",
    "    scaler=StandardScaler(),  # scale features for logistic regression\n",
    "    name_suffix=\" (base)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define tree-based base models and their parameter grids\n",
    "dt_base_model = {\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "}\n",
    "rf_xgb_base_models = {\n",
    "    \"Random Forest\": RandomForestClassifier(\n",
    "        random_state=42, n_jobs=-1\n",
    "    ),  # n_jobs=-1 uses all available cores, makes training faster\n",
    "    \"XGBoost\": XGBClassifier(\n",
    "        eval_metric=get_xgb_eval_metric(scoring), random_state=42, n_jobs=-1\n",
    "    ),  # eval_metric: sets evaluation metric for XGBoost (e.g., 'auc', 'logloss')\n",
    "}\n",
    "tree_base_param_grids = {\n",
    "    \"Decision Tree\": {\n",
    "        \"model__max_depth\": [4, 6, 8, None],  # max tree depth (None = unlimited)\n",
    "        \"model__min_samples_leaf\": [1, 2, 5],  # min samples required at a leaf node\n",
    "        \"model__min_samples_split\": [\n",
    "            2,\n",
    "            4,\n",
    "            6,\n",
    "        ],  # min samples required to split an internal node\n",
    "        \"model__class_weight\": [\n",
    "            None,\n",
    "            \"balanced\",\n",
    "        ],  # handle class imbalance (None = no weighting, 'balanced' = automatic weighting)\n",
    "    },\n",
    "    \"Random Forest\": {\n",
    "        \"model__n_estimators\": [300, 500],  # number of trees in the forest\n",
    "        \"model__max_depth\": [3, 5, None],  # max depth of each tree (None = unlimited)\n",
    "        \"model__max_features\": [\n",
    "            \"sqrt\",\n",
    "            1.0,\n",
    "        ],  # number of features to consider at each split (sqrt = square root, 1.0 = all)\n",
    "        \"model__max_samples\": [\n",
    "            0.7,\n",
    "            1.0,\n",
    "        ],  # fraction of samples to train each tree (0.7 = 70%, 1.0 = 100%)\n",
    "        \"model__min_samples_leaf\": [1, 2, 3],  # min samples at a leaf node\n",
    "        \"model__min_samples_split\": [2, 3, 4],  # min samples to split a node\n",
    "        \"model__class_weight\": [\n",
    "            None,\n",
    "            \"balanced\",\n",
    "        ],  # handle class imbalance (None = no weighting, 'balanced' = automatic weighting)\n",
    "    },\n",
    "    \"XGBoost\": {\n",
    "        \"model__n_estimators\": [100, 300],  # number of boosting rounds (trees)\n",
    "        \"model__max_depth\": [3, 5, 7],  # max tree depth for base learners\n",
    "        \"model__learning_rate\": [\n",
    "            0.01,\n",
    "            0.1,\n",
    "            0.2,\n",
    "        ],  # step size shrinkage (lower = slower, more robust, less overfitting, but more trees / training time)\n",
    "        \"model__subsample\": [\n",
    "            0.6,\n",
    "            0.8,\n",
    "            1.0,\n",
    "        ],  # fraction of samples used per tree (row sampling)\n",
    "        \"model__colsample_bytree\": [\n",
    "            0.6,\n",
    "            0.8,\n",
    "            1.0,\n",
    "        ],  # fraction of features used per tree (column sampling)\n",
    "        \"model__min_child_weight\": [\n",
    "            1,\n",
    "            5,\n",
    "            10,\n",
    "        ],  # min sum of instance weight needed in a child (higher = fewer larger leaves, less overfitting) (like min_samples_leaf)\n",
    "        \"model__gamma\": [\n",
    "            0,\n",
    "            0.1,\n",
    "            0.2,\n",
    "        ],  # min loss reduction required to make a split (higher = fewer splits, less overfitting)\n",
    "        \"model__scale_pos_weight\": [\n",
    "            1,\n",
    "            scale_pos_weight_value,\n",
    "        ],  # try 1 and the calculated value for class imbalance\n",
    "    },\n",
    "}\n",
    "\n",
    "# create models_config for base tree-based models\n",
    "dt_base_config = make_models_config(\n",
    "    models=dt_base_model,\n",
    "    X_train=X_train,  # training features for tree-based models\n",
    "    y_train=y_train,  # training target labels for tree-based models\n",
    "    feature_func=None,  # no additional features for base model\n",
    "    param_grids=tree_base_param_grids,\n",
    "    scaler=None,  # no scaling needed for tree-based models\n",
    "    name_suffix=\" (base)\",\n",
    ")\n",
    "rf_xgb_base_configs = make_models_config(\n",
    "    models=rf_xgb_base_models,\n",
    "    X_train=X_train,  # training features for tree-based models\n",
    "    y_train=y_train,  # training target labels for tree-based models\n",
    "    feature_func=None,  # no additional features for base model\n",
    "    param_grids=tree_base_param_grids,\n",
    "    scaler=None,  # no scaling needed for tree-based models\n",
    "    name_suffix=\" (base)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Run the baseline models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run base model evaluation\n",
    "results_df = run_model_evaluation(lr_base_config, scoring=scoring, save_model=True, search_type=\"grid\")\n",
    "results_df = run_model_evaluation(\n",
    "    dt_base_config, results_df=results_df, scoring=scoring, save_model=True, search_type=\"grid\"\n",
    ")\n",
    "results_df = run_model_evaluation(\n",
    "    rf_xgb_base_configs,\n",
    "    results_df=results_df,\n",
    "    scoring=scoring,\n",
    "    save_model=True,\n",
    "    search_type=\"random\",\n",
    "    n_iter=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Baseline results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print results, order by recall\n",
    "print(\"Model Evaluation Results:\")\n",
    "results_df.sort_values(by=\"recall\", ascending=False, inplace=True)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### **Observations on Baseline Results**\n",
    ">\n",
    "> - **XGBoost** had the best overall performance across metrics, including recall, precision, F1, and ROC AUC.\n",
    ">\n",
    "> - **Random Forest** was close behind but took the longest to run.\n",
    ">\n",
    "> - **Decision Tree** was fast and reasonably strong—good for quick baselines or interpretation.\n",
    ">\n",
    "> - **Logistic Regression** severely underperformed in recall—unsuitable if false negatives are costly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix plots, saved as PNG files\n",
    "plot_confusion_grid_from_results(results_df, png_title=\"Base Model Confusion Matrices\")\n",
    "# plot_confusion_from_results(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Summary of Observations from the Confusion Matrices:**\n",
    ">\n",
    "> - **Tree-based models (Decision Tree, Random Forest, XGBoost)** show very high recall, correctly identifying most employees who left (true positives), with very few false positives. They also have relatively few false negatives, indicating strong overall performance.\n",
    "> - **Logistic Regression** has a much higher number of false negatives, missing many employees who actually left. This results in lower recall and makes it less suitable for identifying at-risk employees.\n",
    "> - Overall, the ensemble models (Random Forest and XGBoost) provide the best balance between correctly identifying leavers and minimizing incorrect predictions, while Logistic Regression struggles with this non-linear problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show confusion matrix exemplar\n",
    "display(Image(filename=\"../resources/images/confusion_matrix_exemplar.png\", width=400))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Check feature importance**\n",
    "\n",
    "> After fitting baseline models, I reviewed the decision tree and feature importances. This step is not to guide feature selection yet, but rather to **cross-check with the EDA** and ensure the models are **learning meaningful patterns**.\n",
    ">\n",
    "> I’m mindful not to overinterpret these plots—they can be intuitive and visually appealing, but heavy reliance risks overfitting and misleading conclusions. This is a **calibration check**, not a signal to optimize prematurely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load decision tree model and plot the tree\n",
    "\n",
    "dt_model = joblib.load(\"../results/saved_models/decision_tree_(base).joblib\")\n",
    "estimator = dt_model.named_steps[\"model\"]\n",
    "\n",
    "# ensure feature_names matches columns used for training\n",
    "# sklearn requires a list of strings, not a pandas index or series\n",
    "feature_names = list(X_train.columns)\n",
    "\n",
    "plt.figure(figsize=(20, 12))\n",
    "plot_tree(\n",
    "    estimator,\n",
    "    feature_names=feature_names,\n",
    "    class_names=[\"Stayed\", \"Left\"],\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    max_depth=3,\n",
    ")\n",
    "plt.title(\"Baseline Decision Tree\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"../results/images/decision_tree_(base)_visualization.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot feature importance for each model\n",
    "\n",
    "# list of model files and names\n",
    "model_files = [\n",
    "    (\"decision_tree_(base).joblib\", \"Decision Tree\", X_train.columns),\n",
    "    (\"random_forest_(base).joblib\", \"Random Forest\", X_train.columns),\n",
    "    (\"xgboost_(base).joblib\", \"XGBoost\", X_train.columns),\n",
    "    (\"logistic_regression_(base).joblib\", \"Logistic Regression\", X_train_lr.columns),\n",
    "]\n",
    "\n",
    "# load each model and plot feature importance\n",
    "for file_name, model_name, feature_names in model_files:\n",
    "    load_and_plot_feature_importance(file_name, model_name, feature_names, top_n=10, save_png=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> All models consistently identify **low satisfaction** and **extreme workload** (either very high or very low) as the most important predictors of employee attrition. This finding aligns with the exploratory data analysis (EDA). **Tenure** also emerges as a significant factor, matching a pattern around the 4-5 year mark observed in the EDA. In contrast, **salary**, **department**, and **recent promotions** have minimal predictive value in this dataset. These key features are especially prominent in the ensemble models (Random Forest and XGBoost), which are likely the most robust. While all models highlight these variables, it is important to note that decision trees are prone to overfitting, and logistic regression underperforms due to its inability to capture non-linear relationships present in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"feature-engineering-round-one\"></a>\n",
    "\n",
    "## **Feature Engineering (Round One)**\n",
    "\n",
    "[Back to top](#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Based on EDA and feature importance, focus on:\n",
    ">\n",
    "> - Satisfaction level (especially low values)\n",
    "> - Extreme workload (very high or very low monthly hours, number of projects)\n",
    "> - Tenure (especially the 4–5 year window)\n",
    ">\n",
    "> Feature engineering steps to experiment with:\n",
    ">\n",
    "> **Binning:**\n",
    ">\n",
    "> - Bin satisfaction_level (e.g., low/medium/high)\n",
    "> - Bin average_monthly_hours (e.g., <160, 160–240, >240)\n",
    "> - Bin number_project (e.g., ≤2, 3–5, ≥6)\n",
    "> - Bin tenure (e.g., ≤3, 4–5, >5 years)\n",
    ">\n",
    "> **Interactions:**\n",
    ">\n",
    "> - satisfaction_level * number_project\n",
    ">    - low: possibly disengaged or underperforming\n",
    ">    - high: possibly engaged top performer or healthy productivity\n",
    ">    - mid: potential burnout\n",
    "> - satisfaction_level * average_monthly_hours\n",
    ">    - satisfaction **given workload**\n",
    ">    - low: burnout risk\n",
    ">    - high: engaged\n",
    "> - evaluation * satisfaction\n",
    ">    - performace and morale\n",
    ">    - both low: possibly disengaged firing risk\n",
    ">    - both high: ideal employee\n",
    ">    - high eval, low satisfaction: attrition risk\n",
    "> - monthly_hours / number_project\n",
    ">    - overwork / underwork index\n",
    ">\n",
    "> **Categorical Flags:**\n",
    ">\n",
    "> - burnout: (projects ≥ 6 or hours ≥ 240) & satisfaction ≤ 0.3\n",
    "> - disengaged: (projects ≤ 2 and hours < 160 and satisfaction ≤ 0.5)\n",
    "> - no_promo_4yr: (promotion_last_5years == 0) & (tenure >= 4)\n",
    ">\n",
    "> **Feature Selection:**\n",
    ">\n",
    "> - Drop weak predictors (e.g., department, salary, work_accident) for logistic regression, as they add noise and multicollinearity.\n",
    ">\n",
    "> ---\n",
    ">\n",
    "> **Note:**  \n",
    "> I used simple hyperparameters for quick testing of new features and combinations. I used a wide set of hyperparameters and walked away from the computer to enjoy life while it crunched data. I eventually settled on a strategy of exhaustively grid searching quick models and randomly searching heavy tree models. Once the best feature set was identified, I did a final round of model training with a more extensive hyperparameter grid for optimal performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Feature engineering functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to add new features to the X_train / X_train_lr dataframe\n",
    "\n",
    "\n",
    "# add binning features\n",
    "def add_binning_features(df):\n",
    "    df = df.copy()\n",
    "    df[\"satisfaction_bin\"] = pd.cut(\n",
    "        df[\"satisfaction_level\"],\n",
    "        bins=[-0.01, 0.4, 0.7, 1.0],\n",
    "        labels=[\"low\", \"medium\", \"high\"],\n",
    "    )\n",
    "    df[\"hours_bin\"] = pd.cut(\n",
    "        df[\"average_monthly_hours\"],\n",
    "        bins=[0, 160, 240, np.inf],\n",
    "        labels=[\"low\", \"medium\", \"high\"],\n",
    "    )\n",
    "    df[\"projects_bin\"] = pd.cut(\n",
    "        df[\"number_project\"], bins=[0, 2, 5, np.inf], labels=[\"low\", \"medium\", \"high\"]\n",
    "    )\n",
    "    df[\"tenure_bin\"] = pd.cut(\n",
    "        df[\"tenure\"], bins=[0, 3, 5, np.inf], labels=[\"short\", \"mid\", \"long\"]\n",
    "    )\n",
    "    # encode the binned features as dummies\n",
    "    df = pd.get_dummies(\n",
    "        df,\n",
    "        columns=[\"satisfaction_bin\", \"hours_bin\", \"projects_bin\", \"tenure_bin\"],\n",
    "        drop_first=True,\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "# add interaction features\n",
    "def add_interaction_features(df):\n",
    "    df = df.copy()\n",
    "    df[\"satisfaction_x_projects\"] = df[\"satisfaction_level\"] * df[\"number_project\"]\n",
    "    df[\"satisfaction_x_hours\"] = df[\"satisfaction_level\"] * df[\"average_monthly_hours\"]\n",
    "    df[\"evaluation_x_satisfaction\"] = df[\"last_evaluation\"] * df[\"satisfaction_level\"]\n",
    "    df[\"hours_per_project\"] = df[\"average_monthly_hours\"] / df[\"number_project\"]\n",
    "    return df\n",
    "\n",
    "\n",
    "# add flag features\n",
    "def add_flag_features(df):\n",
    "    df = df.copy()\n",
    "    df[\"burnout\"] = (\n",
    "        (df[\"number_project\"] >= 6) | (df[\"average_monthly_hours\"] >= 240)\n",
    "    ) & (df[\"satisfaction_level\"] <= 0.3)\n",
    "    df[\"disengaged\"] = (\n",
    "        (df[\"number_project\"] <= 2)\n",
    "        & (df[\"average_monthly_hours\"] < 160)\n",
    "        & (df[\"satisfaction_level\"] <= 0.5)\n",
    "    )\n",
    "    df[\"no_promo_4yr\"] = (df[\"promotion_last_5years\"] == 0) & (df[\"tenure\"] >= 4)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Feature selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection for logistic regression\n",
    "drop_cols = [col for col in X_train_lr.columns if col.startswith(\"department_\")]\n",
    "drop_cols += [\"salary\", \"work_accident\"]\n",
    "X_train_lr_fs = X_train_lr.drop(columns=drop_cols)\n",
    "\n",
    "# feature selection for tree-based models\n",
    "drop_cols = [col for col in X_train.columns if col.startswith(\"department_\")]\n",
    "drop_cols += [\"salary\", \"work_accident\"]\n",
    "X_train_fs = X_train.drop(columns=drop_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Define logistic regression models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression feature engineering parameters\n",
    "lr_fe_params = {\n",
    "    \"model__C\": [0.1, 1.0, 10.0],  # regularization strength (inverse)\n",
    "    \"model__penalty\": [\"l1\", \"l2\"],  # regularization type (L1 = Lasso, L2 = Ridge)\n",
    "    \"model__solver\": [\"liblinear\"],  # optimization algorithm (liblinear supports L1/L2)\n",
    "    \"model__class_weight\": [None, \"balanced\"],  # None or balanced for class imbalance\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define feature engineered logistic regression models, their feature functions, and parameter grids\n",
    "lr_fe_models = {\n",
    "    \"Logistic Regression with Binning\": LogisticRegression(\n",
    "        max_iter=1000, random_state=42\n",
    "    ),\n",
    "    \"Logistic Regression with Interaction\": LogisticRegression(\n",
    "        max_iter=1000, random_state=42\n",
    "    ),\n",
    "    \"Logistic Regression with Flags\": LogisticRegression(\n",
    "        max_iter=1000, random_state=42\n",
    "    ),\n",
    "}\n",
    "lr_fe_feature_funcs = {\n",
    "    \"Logistic Regression with Binning\": add_binning_features,\n",
    "    \"Logistic Regression with Interaction\": add_interaction_features,\n",
    "    \"Logistic Regression with Flags\": add_flag_features,\n",
    "}\n",
    "lr_fe_param_grids = {\n",
    "    \"Logistic Regression with Binning\": lr_fe_params,\n",
    "    \"Logistic Regression with Interaction\": lr_fe_params,\n",
    "    \"Logistic Regression with Flags\": lr_fe_params,\n",
    "}\n",
    "\n",
    "# create models_config for logistic regression with feature engineering\n",
    "lr_fe_configs = make_models_config(\n",
    "    lr_fe_models,\n",
    "    X_train_lr,\n",
    "    y_train_lr,\n",
    "    feature_func=lr_fe_feature_funcs,\n",
    "    scaler=StandardScaler(),\n",
    "    param_grids=lr_fe_param_grids,\n",
    ")\n",
    "\n",
    "# create models_config for logistic regression with feature engineering and feature selection\n",
    "lr_fe_fs_configs = make_models_config(\n",
    "    lr_fe_models,\n",
    "    X_train_lr_fs,\n",
    "    y_train_lr,\n",
    "    feature_func=lr_fe_feature_funcs,\n",
    "    scaler=StandardScaler(),\n",
    "    param_grids=lr_fe_param_grids,\n",
    "    name_suffix=\" (feature selection)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Run logistic regression models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run feature engineered logistic regression model evaluation\n",
    "results_lr_fe_df = run_model_evaluation(\n",
    "    lr_fe_configs + lr_fe_fs_configs, scoring=scoring\n",
    ")\n",
    "# print feature engineered model results, order by recall\n",
    "print(\"Feature Engineered Model Evaluation Results:\")\n",
    "results_lr_fe_df.sort_values(by=\"recall\", ascending=False, inplace=True)\n",
    "results_lr_fe_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### **Observations of Feature-Engineered Logistic Regression Results**\n",
    ">\n",
    "> - **Logistic Regression with Flags (feature selection)** had the highest recall and strong metrics, using only 6 features—making it highly interpretable and efficient.\n",
    ">\n",
    "> - **Feature selection** (removing department, salary, accident, etc.) simplified the model without hurting accuracy.\n",
    ">\n",
    "> - **Interaction and binning features** improved recall and F1 over the baseline, but not as much as the flag-based models.\n",
    ">\n",
    "> - **Interpretability:** These models are transparent and easy to explain—ideal for HR use.\n",
    ">\n",
    "> - **Summary:** With targeted feature engineering, logistic regression can approach the accuracy of complex models while staying simple and explainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot confuision matrices for feature engineered models\n",
    "plot_confusion_grid_from_results(results_lr_fe_df)\n",
    "# plot_confusion_from_results(results_lr_fe_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Define tree-based feature engineering models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tree-based feature engineering parameters\n",
    "tree_fe_params = {\n",
    "    \"Random Forest\": {\n",
    "        \"model__n_estimators\": [100, 300],  # 300 was best, but 100 is faster for FE\n",
    "        \"model__max_depth\": [\n",
    "            3,\n",
    "            4,\n",
    "            5,\n",
    "            8,\n",
    "        ],  # 5 was best, trying for regularization, deeper trees can overfit, take longer to train\n",
    "        \"model__max_features\": [\"sqrt\", 1.0],  # 1.0 was best, but sqrt is common\n",
    "        \"model__max_samples\": [0.7, 1.0],  # 1.0 was best\n",
    "        \"model__min_samples_leaf\": [1, 2],  # 1 or 2\n",
    "        \"model__min_samples_split\": [2, 3],  # 2 or 3\n",
    "        \"model__class_weight\": [\n",
    "            None,\n",
    "            \"balanced\",\n",
    "        ],  # None or balanced for class imbalance\n",
    "    },\n",
    "    \"XGBoost\": {\n",
    "        \"model__n_estimators\": [100, 300],  # 300 was best\n",
    "        \"model__max_depth\": [\n",
    "            3,\n",
    "            4,\n",
    "            5,\n",
    "            8,\n",
    "        ],  # 3 was best (moderate increase in training time)\n",
    "        \"model__learning_rate\": [\n",
    "            0.1,\n",
    "            0.2,\n",
    "        ],  # 0.1 is standard, 0.2 for speed, step size shrinkage\n",
    "        \"model__subsample\": [\n",
    "            0.6,\n",
    "            0.8,\n",
    "            1.0,\n",
    "        ],  # 1.0 was best, row subsampling (adds randomness, helps generalization)\n",
    "        \"model__colsample_bytree\": [\n",
    "            0.6,\n",
    "            0.8,\n",
    "            1.0,\n",
    "        ],  # 1.0 was best, column subsampling (adds randomness, helps generalization)\n",
    "        \"model__min_child_weight\": [\n",
    "            1,\n",
    "            5,\n",
    "        ],  # 1 is default, 5 for regularization, minimum sum of instance weight in a child\n",
    "        \"model__gamma\": [\n",
    "            0,\n",
    "            0.1,\n",
    "            0.2,\n",
    "        ],  # 0.2 was best, try 0 for comparison, minimum loss reduction required to make a split\n",
    "        \"model__scale_pos_weight\": [\n",
    "            1,\n",
    "            scale_pos_weight_value,\n",
    "        ],  # 1 or calculated value for class imbalance\n",
    "        \"model__reg_alpha\": [\n",
    "            0,\n",
    "            0.1,\n",
    "            1,\n",
    "        ],  # L1 regularization (helps control overfitting)\n",
    "        \"model__reg_lambda\": [1, 2, 5],  # L2 regularization (helps control overfitting)\n",
    "    },\n",
    "    \"Decision Tree\": {\n",
    "        \"model__max_depth\": [3, 4, 5, 6, 8],  # best was 8\n",
    "        \"model__min_samples_leaf\": [1, 2, 3],  # 1 was best\n",
    "        \"model__min_samples_split\": [2, 3, 4],  # 2 was best\n",
    "        \"model__class_weight\": [\n",
    "            None,\n",
    "            \"balanced\",\n",
    "        ],  # None or balanced for class imbalance\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tree-based feature engineering configs with full model names\n",
    "dt_fe_models = {\n",
    "    \"Decision Tree with Binning\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Decision Tree with Interaction\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Decision Tree with Flags\": DecisionTreeClassifier(random_state=42),\n",
    "}\n",
    "rf_xgb_fe_models = {\n",
    "    \"Random Forest with Binning\": RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "    \"Random Forest with Interaction\": RandomForestClassifier(\n",
    "        random_state=42, n_jobs=-1\n",
    "    ),\n",
    "    \"Random Forest with Flags\": RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "    \"XGBoost with Binning\": XGBClassifier(\n",
    "        eval_metric=get_xgb_eval_metric(scoring), random_state=42, n_jobs=-1\n",
    "    ),\n",
    "    \"XGBoost with Interaction\": XGBClassifier(\n",
    "        eval_metric=get_xgb_eval_metric(scoring), random_state=42, n_jobs=-1\n",
    "    ),\n",
    "    \"XGBoost with Flags\": XGBClassifier(\n",
    "        eval_metric=get_xgb_eval_metric(scoring), random_state=42, n_jobs=-1\n",
    "    ),\n",
    "}\n",
    "\n",
    "tree_fe_feature_funcs = {\n",
    "    \"Random Forest with Binning\": add_binning_features,\n",
    "    \"Random Forest with Interaction\": add_interaction_features,\n",
    "    \"Random Forest with Flags\": add_flag_features,\n",
    "    \"XGBoost with Binning\": add_binning_features,\n",
    "    \"XGBoost with Interaction\": add_interaction_features,\n",
    "    \"XGBoost with Flags\": add_flag_features,\n",
    "    \"Decision Tree with Binning\": add_binning_features,\n",
    "    \"Decision Tree with Interaction\": add_interaction_features,\n",
    "    \"Decision Tree with Flags\": add_flag_features,\n",
    "}\n",
    "\n",
    "tree_fe_param_grids = {\n",
    "    \"Random Forest with Binning\": tree_fe_params[\"Random Forest\"],\n",
    "    \"Random Forest with Interaction\": tree_fe_params[\"Random Forest\"],\n",
    "    \"Random Forest with Flags\": tree_fe_params[\"Random Forest\"],\n",
    "    \"XGBoost with Binning\": tree_fe_params[\"XGBoost\"],\n",
    "    \"XGBoost with Interaction\": tree_fe_params[\"XGBoost\"],\n",
    "    \"XGBoost with Flags\": tree_fe_params[\"XGBoost\"],\n",
    "    \"Decision Tree with Binning\": tree_fe_params[\"Decision Tree\"],\n",
    "    \"Decision Tree with Interaction\": tree_fe_params[\"Decision Tree\"],\n",
    "    \"Decision Tree with Flags\": tree_fe_params[\"Decision Tree\"],\n",
    "}\n",
    "\n",
    "# with all feature engineering functions applied\n",
    "dt_fe_configs = make_models_config(\n",
    "    dt_fe_models,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    feature_func=tree_fe_feature_funcs,\n",
    "    param_grids=tree_fe_param_grids,\n",
    ")\n",
    "rf_xgb_fe_configs = make_models_config(\n",
    "    rf_xgb_fe_models,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    feature_func=tree_fe_feature_funcs,\n",
    "    param_grids=tree_fe_param_grids,\n",
    ")\n",
    "\n",
    "# with feature engineering and feature selection (drop_cols)\n",
    "dt_fe_fs_configs = make_models_config(\n",
    "    dt_fe_models,\n",
    "    X_train_fs,\n",
    "    y_train,\n",
    "    feature_func=tree_fe_feature_funcs,\n",
    "    param_grids=tree_fe_param_grids,\n",
    "    name_suffix=\" (feature selection)\",\n",
    ")\n",
    "rf_xgb_fe_fs_configs = make_models_config(\n",
    "    rf_xgb_fe_models,\n",
    "    X_train_fs,\n",
    "    y_train,\n",
    "    feature_func=tree_fe_feature_funcs,\n",
    "    param_grids=tree_fe_param_grids,\n",
    "    name_suffix=\" (feature selection)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Run tree-based feature engineering models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run tree-based feature engineered model evaluation\n",
    "results_tree_fe_df = run_model_evaluation(\n",
    "    dt_fe_configs + dt_fe_fs_configs, scoring=scoring, search_type=\"grid\"\n",
    ")\n",
    "results_tree_fe_df = run_model_evaluation(\n",
    "    rf_xgb_fe_configs + rf_xgb_fe_fs_configs,\n",
    "    results_df=results_tree_fe_df,\n",
    "    scoring=scoring,\n",
    "    search_type=\"random\",\n",
    "    n_iter=50,\n",
    ")\n",
    "# print feature engineered tree-based model results, order by recall\n",
    "print(\"Feature Engineered Tree-Based Model Evaluation Results:\")\n",
    "results_tree_fe_df.sort_values(by=\"recall\", ascending=False, inplace=True)\n",
    "results_tree_fe_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### **Patterns in Results thus far**\n",
    ">\n",
    "> - **Recall is consistently high** across all models, especially for Logistic Regression and Decision Tree (base), indicating strong sensitivity to identifying leavers.\n",
    "> - **F1 and Precision are much lower for Logistic Regression (base),** suggesting many false positives. Tree-based and XGBoost models have much better balance between recall and precision.\n",
    "> - **ROC AUC is highest for XGBoost and Random Forest,** showing strong overall discrimination.\n",
    "> - **Feature selection and engineering** (binning, interaction, flags) generally improves F1, precision, and accuracy, sometimes at a small cost to recall.\n",
    "> - **Reducing features (feature selection)** often maintains or even improves performance, especially for XGBoost and Decision Tree, and greatly reduces model complexity and training time.\n",
    "> - **Confusion matrices show that most errors are false positives** (predicting leave when they stay), which is expected with `class_weight='balanced'` and high recall focus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"feature-engineering-round-two\"></a>\n",
    "\n",
    "## **Feature Engineering (Round Two)**\n",
    "\n",
    "[Back to top](#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### **What? Why?**\n",
    ">\n",
    "> Really, this is a feature shrinking round. Some feature engineering paired with a lot of feature selection. Feature-rich models have barely improved or even reduced performance, and feature selection has performed well.\n",
    ">\n",
    "> Simpler models are easier to explain to stakeholders, and it'll hopefully reduce noise and potential multicollinearity.\n",
    ">\n",
    "> **Selected features + burnout flag:**\n",
    "> This set isolates the core predictors of attrition (satisfaction, workload, tenure, promotion) and adds a “burnout” flag to capture the high-risk group of overworked, dissatisfied employees\n",
    ">\n",
    "> **Selected features + interactions:**\n",
    "> This set focuses on the main drivers (satisfaction, workload, tenure) and adds interaction terms (satisfaction × projects, hours per project) to capture non-linear effects and workload intensity, which EDA showed are important for distinguishing between underworked, overworked, and healthy employees.\n",
    ">\n",
    "> **Selected features + interactions + burnout flag:**\n",
    "> This feature set combines the core predictors of attrition (satisfaction, workload, tenure) with a “burnout” flag to capture high-risk, overworked employees. It also includes a key interaction term, \"satisfaction × projects\", to distinguish between groups identified in EDA\n",
    ">\n",
    "> `satisfaction_x_projects` separates healthy, burned-out, and underperforming employees:\n",
    "> - Employees who are satisfied and productive (high satisfaction, moderate projects)\n",
    "> - Those who are overworked and dissatisfied (low satisfaction, high projects)\n",
    "> - Those who are disengaged (low satisfaction, low projects)\n",
    ">\n",
    "> `hours per project` captures nuanced patterns of overwork and underwork:\n",
    "> - Employees with many projects but reasonable hours (healthy workload)\n",
    "> - Employees with few projects but high hours (potentially inefficient or struggling)\n",
    "> - Employees with many projects and high hours (burnout risk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected features + burnout flag\n",
    "def select_core_features_with_burnout(df):\n",
    "    df = df.copy()\n",
    "    # burnout flag: (projects >= 6 or hours >= 240) & satisfaction <= 0.3\n",
    "    df[\"burnout\"] = (\n",
    "        (df[\"number_project\"] >= 6) | (df[\"average_monthly_hours\"] >= 240)\n",
    "    ) & (df[\"satisfaction_level\"] <= 0.3)\n",
    "    return df[\n",
    "        [\n",
    "            \"satisfaction_level\",\n",
    "            \"last_evaluation\",\n",
    "            \"number_project\",\n",
    "            \"average_monthly_hours\",\n",
    "            \"tenure\",\n",
    "            \"promotion_last_5years\",\n",
    "            \"burnout\",\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "\n",
    "# selected features + interactions\n",
    "def select_core_features_with_interactions(df):\n",
    "    df = df.copy()\n",
    "    # interactions\n",
    "    df[\"satisfaction_x_projects\"] = df[\"satisfaction_level\"] * df[\"number_project\"]\n",
    "    df[\"hours_per_project\"] = df[\"average_monthly_hours\"] / df[\"number_project\"]\n",
    "    return df[\n",
    "        [\n",
    "            \"satisfaction_level\",\n",
    "            \"number_project\",\n",
    "            \"average_monthly_hours\",\n",
    "            \"tenure\",\n",
    "            \"satisfaction_x_projects\",\n",
    "            \"hours_per_project\",\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "\n",
    "# selected features + interactions + burnout flag\n",
    "def select_core_features_with_interactions_and_burnout(df):\n",
    "    df = df.copy()\n",
    "    # burnout flag: (projects >= 6 or hours >= 240) & satisfaction <= 0.3\n",
    "    df[\"burnout\"] = (\n",
    "        (df[\"number_project\"] >= 6) | (df[\"average_monthly_hours\"] >= 240)\n",
    "    ) & (df[\"satisfaction_level\"] <= 0.3)\n",
    "    # interaction\n",
    "    df[\"satisfaction_x_projects\"] = df[\"satisfaction_level\"] * df[\"number_project\"]\n",
    "    return df[\n",
    "        [\n",
    "            \"satisfaction_level\",\n",
    "            \"number_project\",\n",
    "            \"average_monthly_hours\",\n",
    "            \"tenure\",\n",
    "            \"burnout\",\n",
    "            \"satisfaction_x_projects\",\n",
    "        ]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Define feature engineering round 2 models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Feature engineering round 2 model dicts ---\n",
    "\n",
    "# logistic regression FE2 models, feature funcs, param grids\n",
    "lr_fe2_models = {\n",
    "    \"Logistic Regression (Core + Burnout)\": LogisticRegression(\n",
    "        max_iter=1000, random_state=42\n",
    "    ),\n",
    "    \"Logistic Regression (Core + Interactions)\": LogisticRegression(\n",
    "        max_iter=1000, random_state=42\n",
    "    ),\n",
    "    \"Logistic Regression (Core + Interactions + Burnout)\": LogisticRegression(\n",
    "        max_iter=1000, random_state=42\n",
    "    ),\n",
    "}\n",
    "lr_fe2_feature_funcs = {\n",
    "    \"Logistic Regression (Core + Burnout)\": select_core_features_with_burnout,\n",
    "    \"Logistic Regression (Core + Interactions)\": select_core_features_with_interactions,\n",
    "    \"Logistic Regression (Core + Interactions + Burnout)\": select_core_features_with_interactions_and_burnout,\n",
    "}\n",
    "lr_fe2_param_grids = {\n",
    "    \"Logistic Regression (Core + Burnout)\": lr_fe_params,\n",
    "    \"Logistic Regression (Core + Interactions)\": lr_fe_params,\n",
    "    \"Logistic Regression (Core + Interactions + Burnout)\": lr_fe_params,\n",
    "}\n",
    "\n",
    "# tree-based FE2 models, feature funcs, param grids\n",
    "dt_fe2_models = {\n",
    "    \"Decision Tree (Core + Burnout)\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Decision Tree (Core + Interactions)\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Decision Tree (Core + Interactions + Burnout)\": DecisionTreeClassifier(\n",
    "        random_state=42\n",
    "    ),\n",
    "}\n",
    "rf_xgb_fe2_models = {\n",
    "    \"Random Forest (Core + Burnout)\": RandomForestClassifier(\n",
    "        random_state=42, n_jobs=-1\n",
    "    ),\n",
    "    \"Random Forest (Core + Interactions)\": RandomForestClassifier(\n",
    "        random_state=42, n_jobs=-1\n",
    "    ),\n",
    "    \"Random Forest (Core + Interactions + Burnout)\": RandomForestClassifier(\n",
    "        random_state=42, n_jobs=-1\n",
    "    ),\n",
    "    \"XGBoost (Core + Burnout)\": XGBClassifier(\n",
    "        eval_metric=get_xgb_eval_metric(scoring), random_state=42, n_jobs=-1\n",
    "    ),\n",
    "    \"XGBoost (Core + Interactions)\": XGBClassifier(\n",
    "        eval_metric=get_xgb_eval_metric(scoring), random_state=42, n_jobs=-1\n",
    "    ),\n",
    "    \"XGBoost (Core + Interactions + Burnout)\": XGBClassifier(\n",
    "        eval_metric=get_xgb_eval_metric(scoring), random_state=42, n_jobs=-1\n",
    "    ),\n",
    "}\n",
    "tree_fe2_feature_funcs = {\n",
    "    \"Decision Tree (Core + Burnout)\": select_core_features_with_burnout,\n",
    "    \"Decision Tree (Core + Interactions)\": select_core_features_with_interactions,\n",
    "    \"Decision Tree (Core + Interactions + Burnout)\": select_core_features_with_interactions_and_burnout,\n",
    "    \"Random Forest (Core + Burnout)\": select_core_features_with_burnout,\n",
    "    \"Random Forest (Core + Interactions)\": select_core_features_with_interactions,\n",
    "    \"Random Forest (Core + Interactions + Burnout)\": select_core_features_with_interactions_and_burnout,\n",
    "    \"XGBoost (Core + Burnout)\": select_core_features_with_burnout,\n",
    "    \"XGBoost (Core + Interactions)\": select_core_features_with_interactions,\n",
    "    \"XGBoost (Core + Interactions + Burnout)\": select_core_features_with_interactions_and_burnout,\n",
    "}\n",
    "tree_fe2_param_grids = {\n",
    "    \"Decision Tree (Core + Burnout)\": tree_fe_params[\"Decision Tree\"],\n",
    "    \"Decision Tree (Core + Interactions)\": tree_fe_params[\"Decision Tree\"],\n",
    "    \"Decision Tree (Core + Interactions + Burnout)\": tree_fe_params[\"Decision Tree\"],\n",
    "    \"Random Forest (Core + Burnout)\": tree_fe_params[\"Random Forest\"],\n",
    "    \"Random Forest (Core + Interactions)\": tree_fe_params[\"Random Forest\"],\n",
    "    \"Random Forest (Core + Interactions + Burnout)\": tree_fe_params[\"Random Forest\"],\n",
    "    \"XGBoost (Core + Burnout)\": tree_fe_params[\"XGBoost\"],\n",
    "    \"XGBoost (Core + Interactions)\": tree_fe_params[\"XGBoost\"],\n",
    "    \"XGBoost (Core + Interactions + Burnout)\": tree_fe_params[\"XGBoost\"],\n",
    "}\n",
    "\n",
    "# create models_config for FE2 models\n",
    "lr_fe2_configs = make_models_config(\n",
    "    lr_fe2_models,\n",
    "    X_train_lr,\n",
    "    y_train_lr,\n",
    "    feature_func=lr_fe2_feature_funcs,\n",
    "    scaler=StandardScaler(),\n",
    "    param_grids=lr_fe2_param_grids,\n",
    ")\n",
    "dt_fe2_configs = make_models_config(\n",
    "    dt_fe2_models,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    feature_func=tree_fe2_feature_funcs,\n",
    "    param_grids=tree_fe2_param_grids,\n",
    ")\n",
    "rf_xgb_fe2_configs = make_models_config(\n",
    "    rf_xgb_fe2_models,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    feature_func=tree_fe2_feature_funcs,\n",
    "    param_grids=tree_fe2_param_grids,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Run feature engineering round 2 models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run feature engineered round 2 model evaluation\n",
    "results_fe2_df = run_model_evaluation(\n",
    "    lr_fe2_configs, scoring=scoring, search_type=\"grid\"\n",
    ")\n",
    "results_fe2_df = run_model_evaluation(\n",
    "    dt_fe2_configs, results_df=results_fe2_df, scoring=scoring, search_type=\"grid\"\n",
    ")\n",
    "results_fe2_df = run_model_evaluation(\n",
    "    rf_xgb_fe2_configs,\n",
    "    results_df=results_fe2_df,\n",
    "    scoring=scoring,\n",
    "    search_type=\"random\",\n",
    "    n_iter=50,\n",
    ")\n",
    "# print feature engineered round 2 model results, order by recall\n",
    "print(\"Feature Engineered Round 2 Model Evaluation Results:\")\n",
    "results_fe2_df.sort_values(by=\"recall\", ascending=False, inplace=True)\n",
    "results_fe2_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot confusion matrices for feature engineered round 2 models\n",
    "plot_confusion_grid_from_results(results_fe2_df)\n",
    "# plot_confusion_from_results(results_fe2_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model-evaluation-results\"></a>\n",
    "\n",
    "## **Model Evaluation Results**\n",
    "\n",
    "[Back to top](#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge all results dataframes into a single dataframe for comparison\n",
    "all_results_df = pd.concat(\n",
    "    [results_df, results_lr_fe_df, results_tree_fe_df, results_fe2_df],\n",
    "    ignore_index=True,\n",
    ")\n",
    "all_results_df.sort_values(by=\"recall\", ascending=False, inplace=True)\n",
    "print(\"All Model Evaluation Results:\")\n",
    "all_results_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results to CSV\n",
    "results_df.to_csv(\"../results/base_model_evaluation_results.csv\", index=False)\n",
    "results_lr_fe_df.to_csv(\n",
    "    \"../results/logistic_regression_feature_engineered_results.csv\", index=False\n",
    ")\n",
    "results_tree_fe_df.to_csv(\n",
    "    \"../results/tree_based_feature_engineered_results.csv\", index=False\n",
    ")\n",
    "results_fe2_df.to_csv(\"../results/feature_engineered_round_2_results.csv\", index=False)\n",
    "all_results_df.to_csv(\"../results/all_model_evaluation_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot confusion matrices for all models in a grid\n",
    "plot_confusion_grid_from_results(all_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_grid_from_results(\n",
    "    all_results_df.iloc[:9], \n",
    "    png_title=\"Top Model Confusion Matrices\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation Summary\n",
    "\n",
    "#### 1. **Logistic Regression**\n",
    "- **Best Recall:**  \n",
    "  - *Logistic Regression (Core + Interactions)* achieves the highest recall (0.962), with only 6 features and a simple, interpretable model.\n",
    "  - Other logistic regression variants with feature selection or binning also maintain high recall (0.94–0.96) with fewer features.\n",
    "- **F1 & Precision:**  \n",
    "  - F1 scores for logistic regression are generally lower (0.64–0.75), reflecting lower precision (0.51–0.63).\n",
    "  - Feature selection and engineering (e.g., interactions, binning) slightly improve F1 and precision while keeping models simple.\n",
    "\n",
    "#### 2. **Tree-Based Models (Decision Tree, Random Forest, XGBoost)**\n",
    "- **Top F1 & Precision:**  \n",
    "  - XGBoost and Random Forest models consistently achieve the highest F1 (up to 0.91) and precision (up to 0.89), with strong recall (0.93–0.94).\n",
    "  - Decision Trees also perform well, especially with feature engineering (F1 up to 0.89, precision up to 0.87).\n",
    "- **Feature Efficiency:**  \n",
    "  - Tree-based models with feature selection or engineered features (e.g., \"Core + Burnout\", \"feature selection\") often match or outperform base models with fewer features.\n",
    "\n",
    "#### 3. **Feature Selection & Engineering**\n",
    "- **Effectiveness:**  \n",
    "  - Models using feature selection or engineered features (interactions, binning, flags) often achieve similar or better performance with fewer features.\n",
    "  - This reduces model complexity and improves interpretability without sacrificing accuracy, recall, or F1.\n",
    "\n",
    "#### 4. **Interpretability vs. Performance**\n",
    "- **Trade-off:**  \n",
    "  - Logistic regression models are more interpretable and, with feature engineering, are now much more competitive in recall and accuracy.\n",
    "  - Tree-based models remain top performers for F1 and precision, but at the cost of increased complexity.\n",
    "\n",
    "---\n",
    "\n",
    "**Conclusion:**  \n",
    "- Feature selection and engineering are highly effective, enabling simpler models (especially logistic regression) to achieve strong recall and competitive accuracy.\n",
    "- Tree-based models (especially XGBoost) remain the best for F1 and precision, but logistic regression is now a viable, interpretable alternative for high-recall use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print all_results_df, ordered by alternate metrics\n",
    "metrics = [\"f1\", \"accuracy\", \"roc_auc\", \"precision\"]\n",
    "\n",
    "for metric in metrics:\n",
    "    print(f\"\\n--- Sorted by {metric} (descending) ---\")\n",
    "    display(all_results_df.sort_values(by=metric, ascending=False).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression\n",
    "Top Pick:\n",
    "\n",
    "Logistic Regression (Core + Interactions)\n",
    "Recall: 0.962 (highest among all models)\n",
    "F1: 0.667 (moderate)\n",
    "Precision: 0.51 (lower, but expected with high recall)\n",
    "Features: 6 (very simple, highly interpretable)\n",
    "Why:\n",
    "Achieves the highest recall, which is critical for identifying as many at-risk employees as possible.\n",
    "Uses only 6 features, making it easy to explain to HR and stakeholders.\n",
    "Slightly lower F1 and precision, but this is a common trade-off when maximizing recall.\n",
    "Good for organizations prioritizing interpretability and proactive retention.\n",
    "Alternative:\n",
    "\n",
    "Logistic Regression with Interaction (feature selection)\n",
    "Recall: 0.960 (very close to top)\n",
    "F1: 0.671 (slightly higher)\n",
    "Precision: 0.52 (slightly higher)\n",
    "Features: 10 (still simple)\n",
    "Why:\n",
    "Slightly better F1 and precision, with a small drop in recall.\n",
    "Still interpretable and efficient.\n",
    "\n",
    "----\n",
    "\n",
    "Decision Tree\n",
    "Top Pick:\n",
    "\n",
    "Decision Tree (Core + Burnout)\n",
    "Recall: 0.944 (highest among DTs)\n",
    "F1: 0.814\n",
    "Precision: 0.72\n",
    "Features: 7\n",
    "Depth: 5 → interpretable\n",
    "Why:\n",
    "Best recall for DTs, with strong F1 and precision.\n",
    "Simple model, easy to visualize and explain.\n",
    "Relatively shallow depth helps interpretability.\n",
    "Captures key non-linear relationships (e.g., burnout).\n",
    "\n",
    "Alternative:\n",
    "\n",
    "Decision Tree (base)\n",
    "Recall: 0.942\n",
    "F1: 0.803\n",
    "Precision: 0.70\n",
    "Features: 18\n",
    "Why:\n",
    "Slightly lower recall, more features, but still interpretable.\n",
    "Deeper and less parsimonious.\n",
    "Useful if you want to see the effect of all variables.\n",
    "\n",
    "----\n",
    "\n",
    "Random Forest\n",
    "Top Pick:\n",
    "\n",
    "Random Forest (Core + Burnout)\n",
    "Recall: 0.941 (highest among RFs)\n",
    "F1: 0.837 (best among all models except XGB)\n",
    "Precision: 0.75\n",
    "Features: 7\n",
    "Max Depth: 5 (controlled complexity)\n",
    "Why:\n",
    "Best recall and F1 for RFs, with a compact feature set.\n",
    "Limited feature set and shallow trees improve generalizability.\n",
    "Balances predictive power and interpretability.\n",
    "Efficient for deployment.\n",
    "\n",
    "Alternative:\n",
    "\n",
    "Random Forest (base)\n",
    "Recall: 0.940\n",
    "F1: 0.770\n",
    "Precision: 0.65\n",
    "Features: 18\n",
    "Why:\n",
    "Slightly lower recall and F1, but includes all features.\n",
    "Useful for feature importance analysis.\n",
    "\n",
    "----\n",
    "\n",
    "XGBoost\n",
    "Top Pick:\n",
    "\n",
    "XGBoost (base)\n",
    "Recall: 0.937 (highest among XGBs)\n",
    "F1: 0.911 (highest overall)\n",
    "Precision: 0.89 (highest overall)\n",
    "ROC AUC: 0.986 (highest overall)\n",
    "Features: 18\n",
    "Why:\n",
    "Best balance of recall, F1, precision, and ROC AUC.\n",
    "Best overall performer in general metrics.\n",
    "Excellent for minimizing both false negatives and false positives.\n",
    "Slightly more complex, but worth it for performance.\n",
    "\n",
    "Alternative:\n",
    "\n",
    "XGBoost (Core + Burnout)\n",
    "Recall: 0.937 (same as base)\n",
    "F1: 0.906\n",
    "Precision: 0.88\n",
    "Features: 7\n",
    "Why:\n",
    "Nearly identical recall, slightly lower F1/precision, but much simpler.\n",
    "Good if you want a more interpretable XGBoost model.\n",
    "\n",
    "Alternates:\n",
    "XGB with Binning (recall tied, more compact features)\n",
    "XGB with Flags (feature selection) (best interpretability: only 9 features, F1=0.914, recall=0.935)\n",
    "\n",
    "If interpretability or runtime matters more than a slight edge in F1, pick the XGB with Flags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print total execution time, for measuring performance\n",
    "nb_end_time = time.time()\n",
    "print(f\"Total execution time: {nb_end_time - nb_start_time:.2f} seconds\")\n",
    "print(\n",
    "    f\"Total execution time: {time.strftime('%H:%M:%S', time.gmtime(nb_end_time - nb_start_time))}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "401PgchTPr4E"
   },
   "source": [
    "<a id=\"pace-execute-stage\"></a>\n",
    "\n",
    "# pacE: Execute Stage\n",
    "[Back to top](#)\n",
    "- Interpret model performance and results\n",
    "- Share actionable steps with stakeholders\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> I passed the point of diminishing returns long ago.\n",
    ">\n",
    "> But, I learned a lot of foundational stuff about the model construction process (Pipelines, cross-validation, random search vs. grid serach, checking misclassification errors, feature selection and engineering, etc), and i did get the logistic regression model a bit better, so I'll call it a win. The time may have been wasted now, but I'll be a lot quicker next time. Nothing like mastering the basics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ex8pgn5iNzau"
   },
   "source": [
    "✏\n",
    "## Recall evaluation metrics\n",
    "\n",
    "- **AUC** is the area under the ROC curve; it's also considered the probability that the model ranks a random positive example more highly than a random negative example.\n",
    "- **Precision** measures the proportion of data points predicted as True that are actually True, in other words, the proportion of positive predictions that are true positives.\n",
    "- **Recall** measures the proportion of data points that are predicted as True, out of all the data points that are actually True. In other words, it measures the proportion of positives that are correctly classified.\n",
    "- **Accuracy** measures the proportion of data points that are correctly classified.\n",
    "- **F1-score** is an aggregation of precision and recall.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJ7Sn8--C45C"
   },
   "source": [
    "💭\n",
    "### Reflect on these questions as you complete the executing stage.\n",
    "\n",
    "- What key insights emerged from your model(s)?\n",
    "- What business recommendations do you propose based on the models built?\n",
    "- What potential recommendations would you make to your manager/company?\n",
    "- Do you think your model could be improved? Why or why not? How?\n",
    "- Given what you know about the data and the models you were using, what other questions could you address for the team?\n",
    "- What resources do you find yourself using as you complete this stage? (Make sure to include the links.)\n",
    "- Do you have any ethical considerations in this stage?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aF1C4CBhLXbN"
   },
   "source": [
    "> ### Execute Stage Reflection\n",
    "> \n",
    "> #### What key insights emerged from your model(s)?\n",
    "> - **Satisfaction level** and **workload** (number of projects, monthly hours) are the strongest predictors of attrition.\n",
    "> - Two main at-risk groups: **overworked/burned-out employees** (many projects, long hours, low satisfaction) and **underworked/disengaged employees** (few projects, low satisfaction).\n",
    "> - **Tenure** is important: attrition peaks at 4–5 years, then drops sharply.\n",
    "> - **Salary, department, and recent promotions** have minimal predictive value.\n",
    "> - **Tree-based models (Random Forest, XGBoost)** achieved the best balance of recall, precision, and F1. With feature engineering, **logistic regression** became competitive and highly interpretable.\n",
    "> \n",
    "> #### What business recommendations do you propose based on the models built?\n",
    "> - **Monitor satisfaction and workload:** Regularly survey employees and track workload to identify those at risk of burnout or disengagement.\n",
    "> - **Targeted retention efforts:** Focus on employees with low satisfaction and extreme workloads, especially those at the 4–5 year tenure mark.\n",
    "> - **Promotions and recognition:** Consider more frequent recognition or advancement opportunities.\n",
    "> - **Work-life balance:** Encourage reasonable project loads and monthly hours to reduce burnout risk.\n",
    "> \n",
    "> #### What potential recommendations would you make to your manager/company?\n",
    "> - **Implement early warning systems** using the model to flag at-risk employees for supportive HR outreach.\n",
    "> - **Review workload distribution** and ensure fair, manageable assignments.\n",
    "> - **Conduct stay interviews** with employees approaching 4–5 years of tenure.\n",
    "> - **Communicate transparently** about how predictive models are used, emphasizing support rather than punitive action.\n",
    "> \n",
    "> #### Do you think your model could be improved? Why or why not? How?\n",
    "> - **Feature engineering:** Further refine interaction terms or add time-based features if available.\n",
    "> - **External data:** Incorporate additional data (e.g., engagement surveys, manager ratings, exit interview themes).\n",
    "> - **Model calibration:** Regularly retrain and calibrate the model as new data becomes available.\n",
    "> - **Bias audits:** Routinely check for bias across demographic groups.\n",
    "> \n",
    "> #### Given what you know about the data and the models you were using, what other questions could you address for the team?\n",
    "> - What are the specific reasons for attrition in different departments or roles?\n",
    "> - Are there seasonal or cyclical patterns in attrition?\n",
    "> - How do external factors (e.g., economic conditions, industry trends) affect turnover?\n",
    "> - What interventions are most effective for retaining at-risk employees?\n",
    "> \n",
    "> #### What resources do you find yourself using as you complete this stage? (Make sure to include the links.)\n",
    "> - [pandas documentation](https://pandas.pydata.org/docs/)\n",
    "> - [matplotlib documentation](https://matplotlib.org/stable/users/index.html)\n",
    "> - [seaborn documentation](https://seaborn.pydata.org/)\n",
    "> - [scikit-learn documentation](https://scikit-learn.org/stable/user_guide.html)\n",
    "> - [XGBoost documentation](https://xgboost.readthedocs.io/en/stable/)\n",
    "> - [Kaggle HR Analytics Dataset](https://www.kaggle.com/datasets/mfaisalqureshi/hr-analytics-and-job-prediction?select=HR_comma_sep.csv)\n",
    "> \n",
    "> #### Do you have any ethical considerations in this stage?\n",
    "> - **Data privacy:** Ensure employee data is kept confidential and secure.\n",
    "> - **Fairness:** Avoid using the model to unfairly target or penalize specific groups.\n",
    "> - **Transparency:** Clearly communicate how predictions are generated and used.\n",
    "> - **Supportive use:** Use predictions to offer support and resources, not for punitive measures.\n",
    "> - **Ongoing monitoring:** Regularly audit the model for bias and unintended consequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6pBUk35yTDaL"
   },
   "source": [
    "<a id=\"results-and-evaluation\"></a>\n",
    "\n",
    "## Results and Evaluation\n",
    "[Back to top](#)\n",
    "- Interpret model\n",
    "- Evaluate model performance using metrics\n",
    "- Prepare results, visualizations, and actionable steps to share with stakeholders\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GXrsxT498Z7h"
   },
   "source": [
    "### Summary of model results\n",
    "\n",
    "> Will do, after running X_test through model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9MOMqelNLn2v"
   },
   "source": [
    "### Conclusion, Recommendations, Next Steps\n",
    "\n",
    "> ### Conclusion\n",
    "> - **Satisfaction level** and **workload** (number of projects, monthly hours) are the strongest predictors of employee attrition.\n",
    "> - Two main at-risk groups emerged: **overworked/burned-out employees** (many projects, long hours, low satisfaction) and **underworked/disengaged employees** (few projects, low satisfaction).\n",
    "> - **Tenure** is important: attrition peaks at 4–5 years, then drops sharply.\n",
    "> - **Salary, department, and recent promotions** have minimal predictive value.\n",
    "> - **Tree-based models (Random Forest, XGBoost)** achieved the best balance of recall, precision, and F1. With feature engineering, **logistic regression** became competitive and highly interpretable.\n",
    "> \n",
    "> ### Recommendations\n",
    "> - **Monitor satisfaction and workload:** Regularly survey employees and track workload to identify those at risk of burnout or disengagement.\n",
    "> - **Targeted retention efforts:** Focus on employees with low satisfaction and extreme workloads, especially those at the 4–5 year tenure mark.\n",
    "> - **Promotions and recognition:** Consider more frequent recognition or advancement opportunities.\n",
    "> - **Work-life balance:** Encourage reasonable project loads and monthly hours to reduce burnout risk.\n",
    "> - **Implement early warning systems:** Use the model to flag at-risk employees for supportive HR outreach.\n",
    "> - **Review workload distribution:** Ensure fair, manageable assignments.\n",
    "> - **Conduct stay interviews:** Engage employees approaching 4–5 years of tenure.\n",
    "> - **Communicate transparently:** Clearly explain how predictive models are used, emphasizing support rather than punitive action.\n",
    "> \n",
    "> ### Next Steps\n",
    "> - **Model deployment:** Integrate the predictive model into HR processes for early identification of at-risk employees.\n",
    "> - **Continuous improvement:** Regularly retrain and calibrate the model as new data becomes available.\n",
    "> - **Expand data sources:** Incorporate additional data (e.g., engagement surveys, manager ratings, exit interview themes) to improve model accuracy.\n",
    "> - **Bias and fairness audits:** Routinely check for bias across demographic groups and monitor for unintended consequences.\n",
    "> - **Ethical safeguards:** Ensure employee data privacy, fairness, and transparency in all predictive analytics initiatives.\n",
    "> \n",
    "> ---\n",
    "> **Resources Used:**\n",
    "> - [pandas documentation](https://pandas.pydata.org/docs/)\n",
    "> - [matplotlib documentation](https://matplotlib.org/stable/users/index.html)\n",
    "> - [seaborn documentation](https://seaborn.pydata.org/)\n",
    "> - [scikit-learn documentation](https://scikit-learn.org/stable/user_guide.html)\n",
    "> - [XGBoost documentation](https://xgboost.readthedocs.io/en/stable/)\n",
    "> - [Kaggle HR Analytics Dataset](https://www.kaggle.com/datasets/mfaisalqureshi/hr-analytics-and-job-prediction?select=HR_comma_sep.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congratulations!** You've completed this lab. However, you may not notice a green check mark next to this item on Coursera's platform. Please continue your progress regardless of the check mark. Just click on the \"save\" icon at the top of this notebook to ensure your work has been logged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<script src=\"https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js\"></script>\n",
    "<script src=\"../static/js/scripts.js\"></script>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
